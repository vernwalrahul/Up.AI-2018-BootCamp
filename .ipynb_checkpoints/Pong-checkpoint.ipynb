{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 : Simple model\n",
    "# model = Sequential()\n",
    "# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "# model.add(Dense(nb_actions))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# Option 2: deep network\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 100000 steps ...\n",
      "    17/100000: episode: 1, duration: 0.068s, episode steps: 17, steps per second: 251, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.089 [-0.956, 1.606], mean_best_reward: --\n",
      "    44/100000: episode: 2, duration: 0.023s, episode steps: 27, steps per second: 1184, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.055 [-1.396, 0.834], mean_best_reward: --\n",
      "    57/100000: episode: 3, duration: 0.013s, episode steps: 13, steps per second: 1035, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.077 [-1.802, 2.710], mean_best_reward: --\n",
      "    74/100000: episode: 4, duration: 0.017s, episode steps: 17, steps per second: 1027, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.079 [-1.376, 2.322], mean_best_reward: --\n",
      "   101/100000: episode: 5, duration: 0.035s, episode steps: 27, steps per second: 779, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.007 [-1.781, 2.511], mean_best_reward: --\n",
      "   113/100000: episode: 6, duration: 0.020s, episode steps: 12, steps per second: 587, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.089 [-1.418, 2.131], mean_best_reward: --\n",
      "   130/100000: episode: 7, duration: 0.020s, episode steps: 17, steps per second: 843, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.091 [-0.770, 1.283], mean_best_reward: --\n",
      "   143/100000: episode: 8, duration: 0.013s, episode steps: 13, steps per second: 1036, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.117 [-2.778, 1.727], mean_best_reward: --\n",
      "   171/100000: episode: 9, duration: 0.024s, episode steps: 28, steps per second: 1183, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.083 [-1.861, 0.965], mean_best_reward: --\n",
      "   180/100000: episode: 10, duration: 0.009s, episode steps: 9, steps per second: 963, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.147 [-1.130, 1.934], mean_best_reward: --\n",
      "   189/100000: episode: 11, duration: 0.010s, episode steps: 9, steps per second: 916, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [-1.810, 2.798], mean_best_reward: --\n",
      "   200/100000: episode: 12, duration: 0.011s, episode steps: 11, steps per second: 999, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.125 [-2.239, 1.383], mean_best_reward: --\n",
      "   215/100000: episode: 13, duration: 0.014s, episode steps: 15, steps per second: 1069, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.093 [-2.300, 1.374], mean_best_reward: --\n",
      "   234/100000: episode: 14, duration: 0.017s, episode steps: 19, steps per second: 1139, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.117 [-1.193, 0.748], mean_best_reward: --\n",
      "   256/100000: episode: 15, duration: 0.019s, episode steps: 22, steps per second: 1188, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.044 [-2.130, 1.396], mean_best_reward: --\n",
      "   268/100000: episode: 16, duration: 0.011s, episode steps: 12, steps per second: 1054, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.103 [-2.568, 1.614], mean_best_reward: --\n",
      "   283/100000: episode: 17, duration: 0.014s, episode steps: 15, steps per second: 1096, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.106 [-2.275, 1.342], mean_best_reward: --\n",
      "   299/100000: episode: 18, duration: 0.016s, episode steps: 16, steps per second: 1025, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.079 [-2.482, 1.571], mean_best_reward: --\n",
      "   310/100000: episode: 19, duration: 0.014s, episode steps: 11, steps per second: 813, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.110 [-1.773, 1.175], mean_best_reward: --\n",
      "   328/100000: episode: 20, duration: 0.023s, episode steps: 18, steps per second: 782, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.066 [-1.371, 2.199], mean_best_reward: --\n",
      "   373/100000: episode: 21, duration: 0.036s, episode steps: 45, steps per second: 1238, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.095 [-1.106, 0.800], mean_best_reward: --\n",
      "   411/100000: episode: 22, duration: 0.035s, episode steps: 38, steps per second: 1100, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.029 [-1.015, 1.407], mean_best_reward: --\n",
      "   435/100000: episode: 23, duration: 0.022s, episode steps: 24, steps per second: 1069, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.097 [-1.382, 0.428], mean_best_reward: --\n",
      "   463/100000: episode: 24, duration: 0.025s, episode steps: 28, steps per second: 1108, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.143 [-1.048, 0.546], mean_best_reward: --\n",
      "   480/100000: episode: 25, duration: 0.016s, episode steps: 17, steps per second: 1037, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.106 [-1.088, 0.561], mean_best_reward: --\n",
      "   500/100000: episode: 26, duration: 0.019s, episode steps: 20, steps per second: 1041, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.438, 1.095], mean_best_reward: --\n",
      "   519/100000: episode: 27, duration: 0.024s, episode steps: 19, steps per second: 801, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.069 [-2.172, 1.360], mean_best_reward: --\n",
      "   528/100000: episode: 28, duration: 0.016s, episode steps: 9, steps per second: 569, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.144 [-1.358, 2.232], mean_best_reward: --\n",
      "   540/100000: episode: 29, duration: 0.013s, episode steps: 12, steps per second: 904, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.124 [-1.763, 0.971], mean_best_reward: --\n",
      "   552/100000: episode: 30, duration: 0.012s, episode steps: 12, steps per second: 996, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.126 [-1.749, 2.728], mean_best_reward: --\n",
      "   565/100000: episode: 31, duration: 0.013s, episode steps: 13, steps per second: 1031, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.091 [-0.993, 1.719], mean_best_reward: --\n",
      "   586/100000: episode: 32, duration: 0.018s, episode steps: 21, steps per second: 1155, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.064 [-1.360, 2.210], mean_best_reward: --\n",
      "   597/100000: episode: 33, duration: 0.011s, episode steps: 11, steps per second: 1010, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.127 [-1.405, 2.269], mean_best_reward: --\n",
      "   629/100000: episode: 34, duration: 0.026s, episode steps: 32, steps per second: 1208, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.098 [-1.405, 0.575], mean_best_reward: --\n",
      "   690/100000: episode: 35, duration: 0.049s, episode steps: 61, steps per second: 1243, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.009 [-1.074, 0.957], mean_best_reward: --\n",
      "   702/100000: episode: 36, duration: 0.012s, episode steps: 12, steps per second: 990, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.093 [-1.216, 2.050], mean_best_reward: --\n",
      "   717/100000: episode: 37, duration: 0.014s, episode steps: 15, steps per second: 1073, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.085 [-0.804, 1.327], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   740/100000: episode: 38, duration: 0.024s, episode steps: 23, steps per second: 967, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.089 [-2.459, 1.345], mean_best_reward: --\n",
      "   752/100000: episode: 39, duration: 0.019s, episode steps: 12, steps per second: 630, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.117 [-2.292, 1.337], mean_best_reward: --\n",
      "   772/100000: episode: 40, duration: 0.018s, episode steps: 20, steps per second: 1113, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.079 [-0.820, 1.647], mean_best_reward: --\n",
      "   789/100000: episode: 41, duration: 0.016s, episode steps: 17, steps per second: 1082, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.071 [-0.935, 1.450], mean_best_reward: --\n",
      "   810/100000: episode: 42, duration: 0.019s, episode steps: 21, steps per second: 1122, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.112 [-2.016, 0.959], mean_best_reward: --\n",
      "   820/100000: episode: 43, duration: 0.010s, episode steps: 10, steps per second: 961, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.107 [-1.208, 1.979], mean_best_reward: --\n",
      "   841/100000: episode: 44, duration: 0.019s, episode steps: 21, steps per second: 1134, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.061 [-1.111, 0.465], mean_best_reward: --\n",
      "   857/100000: episode: 45, duration: 0.015s, episode steps: 16, steps per second: 1094, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.093 [-2.133, 1.342], mean_best_reward: --\n",
      "   870/100000: episode: 46, duration: 0.013s, episode steps: 13, steps per second: 998, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.081 [-2.689, 1.739], mean_best_reward: --\n",
      "   888/100000: episode: 47, duration: 0.017s, episode steps: 18, steps per second: 1065, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.074 [-1.011, 1.589], mean_best_reward: --\n",
      "   903/100000: episode: 48, duration: 0.015s, episode steps: 15, steps per second: 973, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.091 [-1.782, 1.004], mean_best_reward: --\n",
      "   913/100000: episode: 49, duration: 0.011s, episode steps: 10, steps per second: 949, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.117 [-1.603, 0.975], mean_best_reward: --\n",
      "   926/100000: episode: 50, duration: 0.013s, episode steps: 13, steps per second: 1028, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.094 [-1.375, 2.172], mean_best_reward: --\n",
      "   945/100000: episode: 51, duration: 0.019s, episode steps: 19, steps per second: 1004, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.086 [-1.887, 0.991], mean_best_reward: --\n",
      "   965/100000: episode: 52, duration: 0.025s, episode steps: 20, steps per second: 793, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.064 [-1.333, 0.833], mean_best_reward: --\n",
      "   993/100000: episode: 53, duration: 0.024s, episode steps: 28, steps per second: 1162, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.075 [-0.602, 1.210], mean_best_reward: --\n",
      "  1014/100000: episode: 54, duration: 0.026s, episode steps: 21, steps per second: 811, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.081 [-1.508, 0.824], mean_best_reward: --\n",
      "  1025/100000: episode: 55, duration: 0.019s, episode steps: 11, steps per second: 589, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.109 [-1.758, 2.737], mean_best_reward: --\n",
      "  1035/100000: episode: 56, duration: 0.014s, episode steps: 10, steps per second: 729, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.136 [-1.200, 2.053], mean_best_reward: --\n",
      "  1046/100000: episode: 57, duration: 0.015s, episode steps: 11, steps per second: 737, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.143 [-1.328, 2.242], mean_best_reward: --\n",
      "  1056/100000: episode: 58, duration: 0.013s, episode steps: 10, steps per second: 780, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.133 [-2.551, 1.585], mean_best_reward: --\n",
      "  1066/100000: episode: 59, duration: 0.011s, episode steps: 10, steps per second: 926, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.144 [-1.919, 1.132], mean_best_reward: --\n",
      "  1078/100000: episode: 60, duration: 0.013s, episode steps: 12, steps per second: 956, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.113 [-1.399, 2.209], mean_best_reward: --\n",
      "  1096/100000: episode: 61, duration: 0.016s, episode steps: 18, steps per second: 1099, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.117 [-1.147, 2.296], mean_best_reward: --\n",
      "  1106/100000: episode: 62, duration: 0.010s, episode steps: 10, steps per second: 957, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.118 [-1.420, 2.224], mean_best_reward: --\n",
      "  1121/100000: episode: 63, duration: 0.015s, episode steps: 15, steps per second: 983, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.092 [-1.400, 2.347], mean_best_reward: --\n",
      "  1139/100000: episode: 64, duration: 0.019s, episode steps: 18, steps per second: 972, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.094 [-0.976, 1.588], mean_best_reward: --\n",
      "  1151/100000: episode: 65, duration: 0.014s, episode steps: 12, steps per second: 835, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.097 [-1.014, 1.644], mean_best_reward: --\n",
      "  1168/100000: episode: 66, duration: 0.016s, episode steps: 17, steps per second: 1085, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.077 [-1.544, 0.840], mean_best_reward: --\n",
      "  1211/100000: episode: 67, duration: 0.037s, episode steps: 43, steps per second: 1163, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.174 [-0.346, 1.022], mean_best_reward: --\n",
      "  1223/100000: episode: 68, duration: 0.014s, episode steps: 12, steps per second: 876, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.118 [-1.544, 0.818], mean_best_reward: --\n",
      "  1233/100000: episode: 69, duration: 0.012s, episode steps: 10, steps per second: 861, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.126 [-2.052, 1.201], mean_best_reward: --\n",
      "  1252/100000: episode: 70, duration: 0.020s, episode steps: 19, steps per second: 974, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.095 [-0.767, 1.497], mean_best_reward: --\n",
      "  1276/100000: episode: 71, duration: 0.023s, episode steps: 24, steps per second: 1053, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.049 [-2.408, 1.553], mean_best_reward: --\n",
      "  1292/100000: episode: 72, duration: 0.016s, episode steps: 16, steps per second: 992, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.088 [-1.470, 0.784], mean_best_reward: --\n",
      "  1310/100000: episode: 73, duration: 0.018s, episode steps: 18, steps per second: 987, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.060 [-2.002, 1.218], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1330/100000: episode: 74, duration: 0.022s, episode steps: 20, steps per second: 899, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.069 [-1.166, 1.985], mean_best_reward: --\n",
      "  1362/100000: episode: 75, duration: 0.037s, episode steps: 32, steps per second: 867, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.062 [-0.847, 1.719], mean_best_reward: --\n",
      "  1397/100000: episode: 76, duration: 0.032s, episode steps: 35, steps per second: 1099, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.018 [-0.650, 1.262], mean_best_reward: --\n",
      "  1409/100000: episode: 77, duration: 0.013s, episode steps: 12, steps per second: 934, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.118 [-0.816, 1.376], mean_best_reward: --\n",
      "  1423/100000: episode: 78, duration: 0.014s, episode steps: 14, steps per second: 996, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.096 [-1.979, 1.182], mean_best_reward: --\n",
      "  1446/100000: episode: 79, duration: 0.022s, episode steps: 23, steps per second: 1052, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.027 [-1.211, 1.775], mean_best_reward: --\n",
      "  1458/100000: episode: 80, duration: 0.012s, episode steps: 12, steps per second: 962, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.087 [-2.030, 1.378], mean_best_reward: --\n",
      "  1469/100000: episode: 81, duration: 0.012s, episode steps: 11, steps per second: 923, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.149 [-2.371, 1.384], mean_best_reward: --\n",
      "  1480/100000: episode: 82, duration: 0.012s, episode steps: 11, steps per second: 908, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.130 [-1.733, 2.798], mean_best_reward: --\n",
      "  1503/100000: episode: 83, duration: 0.022s, episode steps: 23, steps per second: 1059, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.304 [0.000, 1.000], mean observation: 0.036 [-1.790, 2.762], mean_best_reward: --\n",
      "  1516/100000: episode: 84, duration: 0.014s, episode steps: 13, steps per second: 944, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.106 [-1.336, 2.079], mean_best_reward: --\n",
      "  1540/100000: episode: 85, duration: 0.032s, episode steps: 24, steps per second: 752, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.076 [-1.327, 2.223], mean_best_reward: --\n",
      "  1552/100000: episode: 86, duration: 0.014s, episode steps: 12, steps per second: 868, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.094 [-2.070, 1.375], mean_best_reward: --\n",
      "  1562/100000: episode: 87, duration: 0.011s, episode steps: 10, steps per second: 935, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.149 [-1.770, 2.752], mean_best_reward: --\n",
      "  1578/100000: episode: 88, duration: 0.016s, episode steps: 16, steps per second: 999, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.080 [-0.815, 1.472], mean_best_reward: --\n",
      "  1597/100000: episode: 89, duration: 0.017s, episode steps: 19, steps per second: 1128, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.081 [-0.805, 1.509], mean_best_reward: --\n",
      "  1615/100000: episode: 90, duration: 0.016s, episode steps: 18, steps per second: 1097, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.096 [-0.617, 1.335], mean_best_reward: --\n",
      "  1629/100000: episode: 91, duration: 0.013s, episode steps: 14, steps per second: 1068, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.082 [-1.449, 1.018], mean_best_reward: --\n",
      "  1649/100000: episode: 92, duration: 0.018s, episode steps: 20, steps per second: 1101, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.070 [-0.834, 1.618], mean_best_reward: --\n",
      "  1666/100000: episode: 93, duration: 0.018s, episode steps: 17, steps per second: 969, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.056 [-2.694, 1.792], mean_best_reward: --\n",
      "  1704/100000: episode: 94, duration: 0.031s, episode steps: 38, steps per second: 1229, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.008 [-2.044, 1.503], mean_best_reward: --\n",
      "  1716/100000: episode: 95, duration: 0.012s, episode steps: 12, steps per second: 1008, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.122 [-1.970, 1.148], mean_best_reward: --\n",
      "  1762/100000: episode: 96, duration: 0.042s, episode steps: 46, steps per second: 1102, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.045 [-1.165, 0.831], mean_best_reward: --\n",
      "  1776/100000: episode: 97, duration: 0.017s, episode steps: 14, steps per second: 812, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.114 [-1.211, 2.193], mean_best_reward: --\n",
      "  1788/100000: episode: 98, duration: 0.014s, episode steps: 12, steps per second: 873, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.117 [-0.817, 1.569], mean_best_reward: --\n",
      "  1806/100000: episode: 99, duration: 0.016s, episode steps: 18, steps per second: 1114, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.079 [-1.554, 2.491], mean_best_reward: --\n",
      "  1818/100000: episode: 100, duration: 0.012s, episode steps: 12, steps per second: 996, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.114 [-1.552, 2.388], mean_best_reward: --\n",
      "  1829/100000: episode: 101, duration: 0.011s, episode steps: 11, steps per second: 1000, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.123 [-1.133, 1.926], mean_best_reward: --\n",
      "  1842/100000: episode: 102, duration: 0.013s, episode steps: 13, steps per second: 1000, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.088 [-1.607, 0.985], mean_best_reward: --\n",
      "  1858/100000: episode: 103, duration: 0.015s, episode steps: 16, steps per second: 1100, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.131 [-0.578, 1.413], mean_best_reward: --\n",
      "  1931/100000: episode: 104, duration: 0.061s, episode steps: 73, steps per second: 1192, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.231 [-1.346, 2.079], mean_best_reward: --\n",
      "  1944/100000: episode: 105, duration: 0.013s, episode steps: 13, steps per second: 969, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.116 [-2.839, 1.749], mean_best_reward: --\n",
      "  1955/100000: episode: 106, duration: 0.012s, episode steps: 11, steps per second: 913, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.137 [-1.328, 2.260], mean_best_reward: --\n",
      "  1975/100000: episode: 107, duration: 0.024s, episode steps: 20, steps per second: 831, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.065 [-1.668, 1.014], mean_best_reward: --\n",
      "  1988/100000: episode: 108, duration: 0.022s, episode steps: 13, steps per second: 594, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.096 [-1.886, 1.192], mean_best_reward: --\n",
      "  1999/100000: episode: 109, duration: 0.016s, episode steps: 11, steps per second: 684, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-2.136, 3.288], mean_best_reward: --\n",
      "  2009/100000: episode: 110, duration: 0.011s, episode steps: 10, steps per second: 886, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.143 [-1.792, 2.731], mean_best_reward: --\n",
      "  2022/100000: episode: 111, duration: 0.015s, episode steps: 13, steps per second: 877, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.115 [-2.197, 1.355], mean_best_reward: --\n",
      "  2033/100000: episode: 112, duration: 0.012s, episode steps: 11, steps per second: 948, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.110 [-0.990, 1.772], mean_best_reward: --\n",
      "  2083/100000: episode: 113, duration: 0.042s, episode steps: 50, steps per second: 1190, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.026 [-0.830, 0.605], mean_best_reward: --\n",
      "  2097/100000: episode: 114, duration: 0.014s, episode steps: 14, steps per second: 980, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.099 [-2.648, 1.725], mean_best_reward: --\n",
      "  2108/100000: episode: 115, duration: 0.011s, episode steps: 11, steps per second: 999, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.118 [-1.370, 2.191], mean_best_reward: --\n",
      "  2117/100000: episode: 116, duration: 0.010s, episode steps: 9, steps per second: 939, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.746, 2.787], mean_best_reward: --\n",
      "  2153/100000: episode: 117, duration: 0.029s, episode steps: 36, steps per second: 1227, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.001 [-0.769, 1.313], mean_best_reward: --\n",
      "  2163/100000: episode: 118, duration: 0.010s, episode steps: 10, steps per second: 979, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.138 [-1.200, 2.036], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2198/100000: episode: 119, duration: 0.038s, episode steps: 35, steps per second: 919, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.047 [-1.502, 2.089], mean_best_reward: --\n",
      "  2210/100000: episode: 120, duration: 0.015s, episode steps: 12, steps per second: 820, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.113 [-2.055, 1.183], mean_best_reward: --\n",
      "  2220/100000: episode: 121, duration: 0.013s, episode steps: 10, steps per second: 759, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.147 [-1.351, 2.228], mean_best_reward: --\n",
      "  2232/100000: episode: 122, duration: 0.013s, episode steps: 12, steps per second: 900, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.081 [-2.966, 1.977], mean_best_reward: --\n",
      "  2248/100000: episode: 123, duration: 0.023s, episode steps: 16, steps per second: 700, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.073 [-1.348, 2.176], mean_best_reward: --\n",
      "  2283/100000: episode: 124, duration: 0.051s, episode steps: 35, steps per second: 684, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.077 [-1.196, 0.410], mean_best_reward: --\n",
      "  2310/100000: episode: 125, duration: 0.032s, episode steps: 27, steps per second: 850, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.006 [-2.473, 1.782], mean_best_reward: --\n",
      "  2332/100000: episode: 126, duration: 0.026s, episode steps: 22, steps per second: 855, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.098 [-1.713, 0.798], mean_best_reward: --\n",
      "  2346/100000: episode: 127, duration: 0.020s, episode steps: 14, steps per second: 690, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.097 [-2.203, 1.402], mean_best_reward: --\n",
      "  2363/100000: episode: 128, duration: 0.034s, episode steps: 17, steps per second: 495, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.076 [-2.351, 1.403], mean_best_reward: --\n",
      "  2379/100000: episode: 129, duration: 0.030s, episode steps: 16, steps per second: 542, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.812 [0.000, 1.000], mean observation: -0.109 [-3.112, 1.954], mean_best_reward: --\n",
      "  2392/100000: episode: 130, duration: 0.023s, episode steps: 13, steps per second: 557, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.113 [-2.392, 1.392], mean_best_reward: --\n",
      "  2405/100000: episode: 131, duration: 0.023s, episode steps: 13, steps per second: 559, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.095 [-2.128, 1.376], mean_best_reward: --\n",
      "  2417/100000: episode: 132, duration: 0.019s, episode steps: 12, steps per second: 646, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.127 [-1.144, 1.985], mean_best_reward: --\n",
      "  2431/100000: episode: 133, duration: 0.019s, episode steps: 14, steps per second: 735, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.084 [-1.378, 2.153], mean_best_reward: --\n",
      "  2450/100000: episode: 134, duration: 0.022s, episode steps: 19, steps per second: 870, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.080 [-2.906, 1.811], mean_best_reward: --\n",
      "  2461/100000: episode: 135, duration: 0.012s, episode steps: 11, steps per second: 942, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.114 [-1.801, 2.782], mean_best_reward: --\n",
      "  2470/100000: episode: 136, duration: 0.010s, episode steps: 9, steps per second: 888, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.142 [-2.278, 1.377], mean_best_reward: --\n",
      "  2491/100000: episode: 137, duration: 0.020s, episode steps: 21, steps per second: 1046, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.030 [-1.782, 2.696], mean_best_reward: --\n",
      "  2507/100000: episode: 138, duration: 0.017s, episode steps: 16, steps per second: 947, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.092 [-1.536, 2.521], mean_best_reward: --\n",
      "  2517/100000: episode: 139, duration: 0.013s, episode steps: 10, steps per second: 789, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.140 [-2.510, 1.560], mean_best_reward: --\n",
      "  2526/100000: episode: 140, duration: 0.013s, episode steps: 9, steps per second: 676, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.763, 2.836], mean_best_reward: --\n",
      "  2546/100000: episode: 141, duration: 0.021s, episode steps: 20, steps per second: 945, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.031 [-1.401, 1.830], mean_best_reward: --\n",
      "  2555/100000: episode: 142, duration: 0.009s, episode steps: 9, steps per second: 951, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.140 [-2.318, 1.415], mean_best_reward: --\n",
      "  2572/100000: episode: 143, duration: 0.015s, episode steps: 17, steps per second: 1131, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.059 [-2.152, 1.383], mean_best_reward: --\n",
      "  2595/100000: episode: 144, duration: 0.019s, episode steps: 23, steps per second: 1214, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.072 [-1.226, 0.776], mean_best_reward: --\n",
      "  2611/100000: episode: 145, duration: 0.015s, episode steps: 16, steps per second: 1089, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.072 [-0.651, 1.265], mean_best_reward: --\n",
      "  2620/100000: episode: 146, duration: 0.009s, episode steps: 9, steps per second: 985, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-2.870, 1.732], mean_best_reward: --\n",
      "  2638/100000: episode: 147, duration: 0.016s, episode steps: 18, steps per second: 1127, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.079 [-1.729, 0.979], mean_best_reward: --\n",
      "  2657/100000: episode: 148, duration: 0.017s, episode steps: 19, steps per second: 1112, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.065 [-1.569, 0.988], mean_best_reward: --\n",
      "  2667/100000: episode: 149, duration: 0.010s, episode steps: 10, steps per second: 990, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.150 [-1.583, 2.622], mean_best_reward: --\n",
      "  2676/100000: episode: 150, duration: 0.011s, episode steps: 9, steps per second: 833, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.811, 1.763], mean_best_reward: --\n",
      "  2687/100000: episode: 151, duration: 0.012s, episode steps: 11, steps per second: 930, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.120 [-1.749, 2.781], mean_best_reward: 53.000000\n",
      "  2700/100000: episode: 152, duration: 0.012s, episode steps: 13, steps per second: 1043, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.120 [-1.789, 2.867], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2712/100000: episode: 153, duration: 0.017s, episode steps: 12, steps per second: 702, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.113 [-1.159, 1.989], mean_best_reward: --\n",
      "  2742/100000: episode: 154, duration: 0.031s, episode steps: 30, steps per second: 983, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.054 [-1.652, 0.799], mean_best_reward: --\n",
      "  2752/100000: episode: 155, duration: 0.010s, episode steps: 10, steps per second: 966, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.127 [-1.414, 2.253], mean_best_reward: --\n",
      "  2785/100000: episode: 156, duration: 0.027s, episode steps: 33, steps per second: 1213, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.051 [-2.825, 1.767], mean_best_reward: --\n",
      "  2802/100000: episode: 157, duration: 0.017s, episode steps: 17, steps per second: 1015, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.084 [-1.315, 1.988], mean_best_reward: --\n",
      "  2821/100000: episode: 158, duration: 0.018s, episode steps: 19, steps per second: 1060, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.117 [-1.135, 0.573], mean_best_reward: --\n",
      "  2837/100000: episode: 159, duration: 0.017s, episode steps: 16, steps per second: 918, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.634, 0.999], mean_best_reward: --\n",
      "  2852/100000: episode: 160, duration: 0.018s, episode steps: 15, steps per second: 827, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.100 [-0.645, 1.228], mean_best_reward: --\n",
      "  2863/100000: episode: 161, duration: 0.014s, episode steps: 11, steps per second: 806, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.100 [-0.798, 1.438], mean_best_reward: --\n",
      "  2895/100000: episode: 162, duration: 0.034s, episode steps: 32, steps per second: 955, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.594 [0.000, 1.000], mean observation: 0.012 [-1.834, 1.387], mean_best_reward: --\n",
      "  2916/100000: episode: 163, duration: 0.023s, episode steps: 21, steps per second: 933, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.080 [-1.788, 0.970], mean_best_reward: --\n",
      "  2931/100000: episode: 164, duration: 0.018s, episode steps: 15, steps per second: 850, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.116 [-2.851, 1.723], mean_best_reward: --\n",
      "  2951/100000: episode: 165, duration: 0.020s, episode steps: 20, steps per second: 976, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.089 [-1.891, 1.136], mean_best_reward: --\n",
      "  2971/100000: episode: 166, duration: 0.018s, episode steps: 20, steps per second: 1137, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.072 [-1.365, 0.779], mean_best_reward: --\n",
      "  2985/100000: episode: 167, duration: 0.013s, episode steps: 14, steps per second: 1105, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.111 [-0.954, 1.719], mean_best_reward: --\n",
      "  3006/100000: episode: 168, duration: 0.018s, episode steps: 21, steps per second: 1155, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.077 [-1.129, 2.010], mean_best_reward: --\n",
      "  3016/100000: episode: 169, duration: 0.010s, episode steps: 10, steps per second: 986, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.135 [-2.208, 1.346], mean_best_reward: --\n",
      "  3029/100000: episode: 170, duration: 0.012s, episode steps: 13, steps per second: 1049, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.112 [-1.354, 2.247], mean_best_reward: --\n",
      "  3042/100000: episode: 171, duration: 0.012s, episode steps: 13, steps per second: 1056, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.113 [-1.716, 0.993], mean_best_reward: --\n",
      "  3061/100000: episode: 172, duration: 0.017s, episode steps: 19, steps per second: 1094, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.113 [-1.950, 0.948], mean_best_reward: --\n",
      "  3074/100000: episode: 173, duration: 0.012s, episode steps: 13, steps per second: 1050, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.103 [-0.966, 1.821], mean_best_reward: --\n",
      "  3090/100000: episode: 174, duration: 0.015s, episode steps: 16, steps per second: 1043, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.080 [-1.753, 2.653], mean_best_reward: --\n",
      "  3124/100000: episode: 175, duration: 0.028s, episode steps: 34, steps per second: 1210, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.528, 1.259], mean_best_reward: --\n",
      "  3148/100000: episode: 176, duration: 0.027s, episode steps: 24, steps per second: 894, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.039 [-3.413, 2.333], mean_best_reward: --\n",
      "  3203/100000: episode: 177, duration: 0.045s, episode steps: 55, steps per second: 1225, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.079 [-1.342, 0.520], mean_best_reward: --\n",
      "  3213/100000: episode: 178, duration: 0.010s, episode steps: 10, steps per second: 955, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.526, 1.543], mean_best_reward: --\n",
      "  3222/100000: episode: 179, duration: 0.009s, episode steps: 9, steps per second: 962, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.156 [-1.330, 2.275], mean_best_reward: --\n",
      "  3233/100000: episode: 180, duration: 0.011s, episode steps: 11, steps per second: 997, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.142 [-1.184, 2.002], mean_best_reward: --\n",
      "  3253/100000: episode: 181, duration: 0.019s, episode steps: 20, steps per second: 1045, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.045 [-1.198, 1.945], mean_best_reward: --\n",
      "  3264/100000: episode: 182, duration: 0.012s, episode steps: 11, steps per second: 913, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.101 [-1.184, 1.917], mean_best_reward: --\n",
      "  3274/100000: episode: 183, duration: 0.012s, episode steps: 10, steps per second: 833, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.947, 3.036], mean_best_reward: --\n",
      "  3286/100000: episode: 184, duration: 0.013s, episode steps: 12, steps per second: 906, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.110 [-1.990, 1.174], mean_best_reward: --\n",
      "  3294/100000: episode: 185, duration: 0.009s, episode steps: 8, steps per second: 848, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.587, 1.617], mean_best_reward: --\n",
      "  3302/100000: episode: 186, duration: 0.010s, episode steps: 8, steps per second: 840, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.535, 1.527], mean_best_reward: --\n",
      "  3311/100000: episode: 187, duration: 0.011s, episode steps: 9, steps per second: 808, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.765, 2.783], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3322/100000: episode: 188, duration: 0.015s, episode steps: 11, steps per second: 736, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.118 [-1.880, 1.136], mean_best_reward: --\n",
      "  3338/100000: episode: 189, duration: 0.025s, episode steps: 16, steps per second: 633, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.076 [-2.553, 1.555], mean_best_reward: --\n",
      "  3358/100000: episode: 190, duration: 0.019s, episode steps: 20, steps per second: 1037, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.040 [-1.511, 2.347], mean_best_reward: --\n",
      "  3377/100000: episode: 191, duration: 0.018s, episode steps: 19, steps per second: 1035, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.056 [-2.208, 1.392], mean_best_reward: --\n",
      "  3394/100000: episode: 192, duration: 0.016s, episode steps: 17, steps per second: 1085, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.096 [-1.498, 0.814], mean_best_reward: --\n",
      "  3421/100000: episode: 193, duration: 0.023s, episode steps: 27, steps per second: 1169, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.296 [0.000, 1.000], mean observation: -0.002 [-2.087, 2.829], mean_best_reward: --\n",
      "  3434/100000: episode: 194, duration: 0.013s, episode steps: 13, steps per second: 1014, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.088 [-1.779, 2.728], mean_best_reward: --\n",
      "  3463/100000: episode: 195, duration: 0.029s, episode steps: 29, steps per second: 1004, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.019 [-0.975, 1.239], mean_best_reward: --\n",
      "  3476/100000: episode: 196, duration: 0.015s, episode steps: 13, steps per second: 845, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.095 [-1.371, 2.180], mean_best_reward: --\n",
      "  3488/100000: episode: 197, duration: 0.012s, episode steps: 12, steps per second: 1036, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.122 [-0.949, 1.691], mean_best_reward: --\n",
      "  3513/100000: episode: 198, duration: 0.021s, episode steps: 25, steps per second: 1198, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.059 [-1.408, 2.427], mean_best_reward: --\n",
      "  3530/100000: episode: 199, duration: 0.019s, episode steps: 17, steps per second: 917, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.091 [-1.862, 1.141], mean_best_reward: --\n",
      "  3544/100000: episode: 200, duration: 0.016s, episode steps: 14, steps per second: 885, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.109 [-0.997, 1.827], mean_best_reward: --\n",
      "  3583/100000: episode: 201, duration: 0.034s, episode steps: 39, steps per second: 1141, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.043 [-0.849, 1.681], mean_best_reward: 44.000000\n",
      "  3603/100000: episode: 202, duration: 0.020s, episode steps: 20, steps per second: 995, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.041 [-1.203, 1.886], mean_best_reward: --\n",
      "  3618/100000: episode: 203, duration: 0.015s, episode steps: 15, steps per second: 1004, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.091 [-2.802, 1.716], mean_best_reward: --\n",
      "  3632/100000: episode: 204, duration: 0.015s, episode steps: 14, steps per second: 920, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.107 [-1.574, 0.766], mean_best_reward: --\n",
      "  3645/100000: episode: 205, duration: 0.014s, episode steps: 13, steps per second: 961, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.090 [-2.395, 1.565], mean_best_reward: --\n",
      "  3657/100000: episode: 206, duration: 0.013s, episode steps: 12, steps per second: 933, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.098 [-0.980, 1.691], mean_best_reward: --\n",
      "  3667/100000: episode: 207, duration: 0.012s, episode steps: 10, steps per second: 869, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.126 [-2.007, 1.222], mean_best_reward: --\n",
      "  3680/100000: episode: 208, duration: 0.014s, episode steps: 13, steps per second: 899, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.111 [-0.954, 1.450], mean_best_reward: --\n",
      "  3691/100000: episode: 209, duration: 0.012s, episode steps: 11, steps per second: 955, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.130 [-1.914, 1.129], mean_best_reward: --\n",
      "  3707/100000: episode: 210, duration: 0.016s, episode steps: 16, steps per second: 1014, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.103 [-1.489, 0.755], mean_best_reward: --\n",
      "  3718/100000: episode: 211, duration: 0.011s, episode steps: 11, steps per second: 965, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.111 [-1.619, 0.972], mean_best_reward: --\n",
      "  3732/100000: episode: 212, duration: 0.018s, episode steps: 14, steps per second: 789, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.092 [-2.504, 1.573], mean_best_reward: --\n",
      "  3742/100000: episode: 213, duration: 0.017s, episode steps: 10, steps per second: 604, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.136 [-1.911, 1.146], mean_best_reward: --\n",
      "  3755/100000: episode: 214, duration: 0.014s, episode steps: 13, steps per second: 929, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.124 [-2.465, 1.550], mean_best_reward: --\n",
      "  3781/100000: episode: 215, duration: 0.024s, episode steps: 26, steps per second: 1102, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.001 [-1.988, 2.752], mean_best_reward: --\n",
      "  3795/100000: episode: 216, duration: 0.013s, episode steps: 14, steps per second: 1060, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.093 [-2.036, 1.336], mean_best_reward: --\n",
      "  3810/100000: episode: 217, duration: 0.014s, episode steps: 15, steps per second: 1055, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.087 [-2.475, 1.571], mean_best_reward: --\n",
      "  3841/100000: episode: 218, duration: 0.028s, episode steps: 31, steps per second: 1115, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.387 [0.000, 1.000], mean observation: 0.039 [-1.337, 2.218], mean_best_reward: --\n",
      "  3851/100000: episode: 219, duration: 0.012s, episode steps: 10, steps per second: 840, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-3.081, 1.944], mean_best_reward: --\n",
      "  3860/100000: episode: 220, duration: 0.011s, episode steps: 9, steps per second: 839, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.139 [-1.335, 2.246], mean_best_reward: --\n",
      "  3886/100000: episode: 221, duration: 0.024s, episode steps: 26, steps per second: 1076, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.092 [-1.319, 0.593], mean_best_reward: --\n",
      "  3895/100000: episode: 222, duration: 0.010s, episode steps: 9, steps per second: 868, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [-1.713, 2.873], mean_best_reward: --\n",
      "  3912/100000: episode: 223, duration: 0.017s, episode steps: 17, steps per second: 1019, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.085 [-2.263, 1.327], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3924/100000: episode: 224, duration: 0.015s, episode steps: 12, steps per second: 824, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.119 [-1.200, 2.080], mean_best_reward: --\n",
      "  3947/100000: episode: 225, duration: 0.026s, episode steps: 23, steps per second: 879, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.077 [-1.052, 0.403], mean_best_reward: --\n",
      "  3957/100000: episode: 226, duration: 0.011s, episode steps: 10, steps per second: 908, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.965, 3.058], mean_best_reward: --\n",
      "  3982/100000: episode: 227, duration: 0.023s, episode steps: 25, steps per second: 1077, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.085 [-1.036, 0.560], mean_best_reward: --\n",
      "  3994/100000: episode: 228, duration: 0.013s, episode steps: 12, steps per second: 951, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.104 [-1.950, 1.176], mean_best_reward: --\n",
      "  4004/100000: episode: 229, duration: 0.011s, episode steps: 10, steps per second: 943, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.145 [-2.042, 1.141], mean_best_reward: --\n",
      "  4030/100000: episode: 230, duration: 0.027s, episode steps: 26, steps per second: 946, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-0.999, 0.638], mean_best_reward: --\n",
      "  4044/100000: episode: 231, duration: 0.014s, episode steps: 14, steps per second: 974, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.074 [-0.986, 1.538], mean_best_reward: --\n",
      "  4060/100000: episode: 232, duration: 0.015s, episode steps: 16, steps per second: 1059, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-1.311, 0.798], mean_best_reward: --\n",
      "  4079/100000: episode: 233, duration: 0.018s, episode steps: 19, steps per second: 1059, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.056 [-1.692, 1.142], mean_best_reward: --\n",
      "  4100/100000: episode: 234, duration: 0.020s, episode steps: 21, steps per second: 1068, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.064 [-1.097, 0.617], mean_best_reward: --\n",
      "  4134/100000: episode: 235, duration: 0.034s, episode steps: 34, steps per second: 990, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.067 [-0.598, 1.216], mean_best_reward: --\n",
      "  4146/100000: episode: 236, duration: 0.016s, episode steps: 12, steps per second: 769, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.129 [-1.521, 2.524], mean_best_reward: --\n",
      "  4173/100000: episode: 237, duration: 0.025s, episode steps: 27, steps per second: 1060, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.010 [-1.794, 2.715], mean_best_reward: --\n",
      "  4210/100000: episode: 238, duration: 0.032s, episode steps: 37, steps per second: 1169, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.082 [-0.748, 1.113], mean_best_reward: --\n",
      "  4220/100000: episode: 239, duration: 0.011s, episode steps: 10, steps per second: 919, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.541, 2.542], mean_best_reward: --\n",
      "  4236/100000: episode: 240, duration: 0.015s, episode steps: 16, steps per second: 1072, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.094 [-1.136, 1.959], mean_best_reward: --\n",
      "  4249/100000: episode: 241, duration: 0.013s, episode steps: 13, steps per second: 1023, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.104 [-0.636, 1.098], mean_best_reward: --\n",
      "  4265/100000: episode: 242, duration: 0.016s, episode steps: 16, steps per second: 1000, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.070 [-1.582, 2.511], mean_best_reward: --\n",
      "  4283/100000: episode: 243, duration: 0.017s, episode steps: 18, steps per second: 1038, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.080 [-1.666, 0.827], mean_best_reward: --\n",
      "  4344/100000: episode: 244, duration: 0.051s, episode steps: 61, steps per second: 1186, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.100 [-0.732, 0.991], mean_best_reward: --\n",
      "  4352/100000: episode: 245, duration: 0.013s, episode steps: 8, steps per second: 622, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.160 [-2.268, 1.371], mean_best_reward: --\n",
      "  4365/100000: episode: 246, duration: 0.013s, episode steps: 13, steps per second: 969, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.077 [-1.576, 2.384], mean_best_reward: --\n",
      "  4385/100000: episode: 247, duration: 0.017s, episode steps: 20, steps per second: 1153, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.076 [-1.311, 0.648], mean_best_reward: --\n",
      "  4398/100000: episode: 248, duration: 0.012s, episode steps: 13, steps per second: 1047, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.097 [-2.204, 1.356], mean_best_reward: --\n",
      "  4433/100000: episode: 249, duration: 0.029s, episode steps: 35, steps per second: 1196, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.072 [-0.783, 1.078], mean_best_reward: --\n",
      "  4443/100000: episode: 250, duration: 0.010s, episode steps: 10, steps per second: 1001, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.112 [-2.170, 1.398], mean_best_reward: --\n",
      "  4465/100000: episode: 251, duration: 0.020s, episode steps: 22, steps per second: 1128, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.069 [-2.101, 1.189], mean_best_reward: 50.500000\n",
      "  4476/100000: episode: 252, duration: 0.011s, episode steps: 11, steps per second: 991, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.091 [-1.411, 2.232], mean_best_reward: --\n",
      "  4497/100000: episode: 253, duration: 0.020s, episode steps: 21, steps per second: 1067, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.057 [-1.004, 1.681], mean_best_reward: --\n",
      "  4530/100000: episode: 254, duration: 0.027s, episode steps: 33, steps per second: 1225, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.042 [-1.159, 0.611], mean_best_reward: --\n",
      "  4571/100000: episode: 255, duration: 0.039s, episode steps: 41, steps per second: 1058, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.191 [-0.992, 0.613], mean_best_reward: --\n",
      "  4586/100000: episode: 256, duration: 0.016s, episode steps: 15, steps per second: 925, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.109 [-1.237, 0.761], mean_best_reward: --\n",
      "  4601/100000: episode: 257, duration: 0.014s, episode steps: 15, steps per second: 1078, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.102 [-1.427, 2.463], mean_best_reward: --\n",
      "  4639/100000: episode: 258, duration: 0.031s, episode steps: 38, steps per second: 1208, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.605 [0.000, 1.000], mean observation: 0.005 [-2.165, 1.533], mean_best_reward: --\n",
      "  4651/100000: episode: 259, duration: 0.012s, episode steps: 12, steps per second: 1025, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.091 [-1.393, 2.159], mean_best_reward: --\n",
      "  4663/100000: episode: 260, duration: 0.012s, episode steps: 12, steps per second: 1030, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.091 [-2.041, 1.192], mean_best_reward: --\n",
      "  4680/100000: episode: 261, duration: 0.015s, episode steps: 17, steps per second: 1101, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.107 [-2.870, 1.726], mean_best_reward: --\n",
      "  4709/100000: episode: 262, duration: 0.026s, episode steps: 29, steps per second: 1127, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.086 [-1.589, 0.608], mean_best_reward: --\n",
      "  4729/100000: episode: 263, duration: 0.019s, episode steps: 20, steps per second: 1036, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.073 [-1.398, 0.739], mean_best_reward: --\n",
      "  4740/100000: episode: 264, duration: 0.012s, episode steps: 11, steps per second: 931, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.120 [-1.390, 2.385], mean_best_reward: --\n",
      "  4757/100000: episode: 265, duration: 0.017s, episode steps: 17, steps per second: 993, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.084 [-0.740, 1.143], mean_best_reward: --\n",
      "  4769/100000: episode: 266, duration: 0.013s, episode steps: 12, steps per second: 938, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.136 [-2.541, 1.538], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4786/100000: episode: 267, duration: 0.023s, episode steps: 17, steps per second: 743, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.083 [-2.014, 1.168], mean_best_reward: --\n",
      "  4806/100000: episode: 268, duration: 0.021s, episode steps: 20, steps per second: 949, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.085 [-2.300, 1.356], mean_best_reward: --\n",
      "  4819/100000: episode: 269, duration: 0.014s, episode steps: 13, steps per second: 953, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.109 [-1.786, 0.951], mean_best_reward: --\n",
      "  4830/100000: episode: 270, duration: 0.012s, episode steps: 11, steps per second: 899, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.110 [-2.246, 1.353], mean_best_reward: --\n",
      "  4854/100000: episode: 271, duration: 0.022s, episode steps: 24, steps per second: 1097, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.109 [-0.379, 1.098], mean_best_reward: --\n",
      "  4869/100000: episode: 272, duration: 0.014s, episode steps: 15, steps per second: 1111, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.078 [-2.427, 1.550], mean_best_reward: --\n",
      "  4879/100000: episode: 273, duration: 0.010s, episode steps: 10, steps per second: 993, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-2.003, 3.066], mean_best_reward: --\n",
      "  4913/100000: episode: 274, duration: 0.028s, episode steps: 34, steps per second: 1210, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.029 [-0.849, 1.755], mean_best_reward: --\n",
      "  4940/100000: episode: 275, duration: 0.023s, episode steps: 27, steps per second: 1187, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.009 [-2.391, 1.746], mean_best_reward: --\n",
      "  4958/100000: episode: 276, duration: 0.016s, episode steps: 18, steps per second: 1115, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.057 [-1.613, 2.482], mean_best_reward: --\n",
      "  4976/100000: episode: 277, duration: 0.016s, episode steps: 18, steps per second: 1127, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.069 [-0.742, 1.189], mean_best_reward: --\n",
      "  4988/100000: episode: 278, duration: 0.012s, episode steps: 12, steps per second: 1023, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.106 [-1.945, 3.007], mean_best_reward: --\n",
      "  4999/100000: episode: 279, duration: 0.014s, episode steps: 11, steps per second: 810, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.131 [-1.373, 2.215], mean_best_reward: --\n",
      "  5038/100000: episode: 280, duration: 0.037s, episode steps: 39, steps per second: 1066, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.079 [-0.385, 1.205], mean_best_reward: --\n",
      "  5081/100000: episode: 281, duration: 0.034s, episode steps: 43, steps per second: 1263, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.080 [-0.801, 1.516], mean_best_reward: --\n",
      "  5092/100000: episode: 282, duration: 0.011s, episode steps: 11, steps per second: 1010, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.128 [-2.418, 1.538], mean_best_reward: --\n",
      "  5106/100000: episode: 283, duration: 0.013s, episode steps: 14, steps per second: 1072, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.099 [-2.441, 1.538], mean_best_reward: --\n",
      "  5123/100000: episode: 284, duration: 0.015s, episode steps: 17, steps per second: 1113, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.067 [-2.182, 1.397], mean_best_reward: --\n",
      "  5136/100000: episode: 285, duration: 0.013s, episode steps: 13, steps per second: 1032, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.102 [-1.814, 1.159], mean_best_reward: --\n",
      "  5147/100000: episode: 286, duration: 0.011s, episode steps: 11, steps per second: 1007, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.105 [-2.716, 1.727], mean_best_reward: --\n",
      "  5166/100000: episode: 287, duration: 0.017s, episode steps: 19, steps per second: 1137, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.029 [-2.617, 1.787], mean_best_reward: --\n",
      "  5180/100000: episode: 288, duration: 0.019s, episode steps: 14, steps per second: 720, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.086 [-3.021, 1.955], mean_best_reward: --\n",
      "  5195/100000: episode: 289, duration: 0.028s, episode steps: 15, steps per second: 538, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.079 [-1.571, 0.988], mean_best_reward: --\n",
      "  5212/100000: episode: 290, duration: 0.022s, episode steps: 17, steps per second: 776, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.082 [-1.730, 2.730], mean_best_reward: --\n",
      "  5222/100000: episode: 291, duration: 0.013s, episode steps: 10, steps per second: 779, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-3.076, 1.935], mean_best_reward: --\n",
      "  5235/100000: episode: 292, duration: 0.013s, episode steps: 13, steps per second: 1030, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.115 [-1.382, 2.285], mean_best_reward: --\n",
      "  5341/100000: episode: 293, duration: 0.127s, episode steps: 106, steps per second: 832, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.023 [-0.741, 1.047], mean_best_reward: --\n",
      "  5381/100000: episode: 294, duration: 0.039s, episode steps: 40, steps per second: 1017, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.099 [-0.821, 0.377], mean_best_reward: --\n",
      "  5398/100000: episode: 295, duration: 0.020s, episode steps: 17, steps per second: 843, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.092 [-1.131, 1.859], mean_best_reward: --\n",
      "  5409/100000: episode: 296, duration: 0.013s, episode steps: 11, steps per second: 855, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.105 [-1.961, 1.180], mean_best_reward: --\n",
      "  5422/100000: episode: 297, duration: 0.012s, episode steps: 13, steps per second: 1063, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.084 [-0.980, 1.451], mean_best_reward: --\n",
      "  5434/100000: episode: 298, duration: 0.012s, episode steps: 12, steps per second: 1036, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.104 [-1.710, 0.954], mean_best_reward: --\n",
      "  5445/100000: episode: 299, duration: 0.011s, episode steps: 11, steps per second: 992, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.098 [-2.751, 1.800], mean_best_reward: --\n",
      "  5462/100000: episode: 300, duration: 0.015s, episode steps: 17, steps per second: 1123, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.082 [-1.334, 2.171], mean_best_reward: --\n",
      "  5478/100000: episode: 301, duration: 0.017s, episode steps: 16, steps per second: 951, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-0.985, 0.617], mean_best_reward: 67.000000\n",
      "  5494/100000: episode: 302, duration: 0.024s, episode steps: 16, steps per second: 673, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.100 [-1.263, 0.807], mean_best_reward: --\n",
      "  5516/100000: episode: 303, duration: 0.054s, episode steps: 22, steps per second: 411, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.064 [-1.214, 2.053], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5536/100000: episode: 304, duration: 0.030s, episode steps: 20, steps per second: 658, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.067 [-1.689, 0.994], mean_best_reward: --\n",
      "  5560/100000: episode: 305, duration: 0.039s, episode steps: 24, steps per second: 620, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.056 [-0.860, 0.579], mean_best_reward: --\n",
      "  5617/100000: episode: 306, duration: 0.066s, episode steps: 57, steps per second: 867, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.092 [-1.673, 0.857], mean_best_reward: --\n",
      "  5627/100000: episode: 307, duration: 0.010s, episode steps: 10, steps per second: 1026, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.120 [-1.672, 0.988], mean_best_reward: --\n",
      "  5650/100000: episode: 308, duration: 0.020s, episode steps: 23, steps per second: 1149, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.040 [-2.264, 1.400], mean_best_reward: --\n",
      "  5662/100000: episode: 309, duration: 0.012s, episode steps: 12, steps per second: 1039, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.128 [-0.751, 1.262], mean_best_reward: --\n",
      "  5682/100000: episode: 310, duration: 0.017s, episode steps: 20, steps per second: 1170, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.116 [-0.902, 0.574], mean_best_reward: --\n",
      "  5716/100000: episode: 311, duration: 0.034s, episode steps: 34, steps per second: 1002, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.047 [-1.405, 0.774], mean_best_reward: --\n",
      "  5729/100000: episode: 312, duration: 0.019s, episode steps: 13, steps per second: 669, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.106 [-1.520, 1.009], mean_best_reward: --\n",
      "  5741/100000: episode: 313, duration: 0.017s, episode steps: 12, steps per second: 725, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.104 [-1.926, 2.935], mean_best_reward: --\n",
      "  5760/100000: episode: 314, duration: 0.021s, episode steps: 19, steps per second: 919, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.110 [-0.789, 1.254], mean_best_reward: --\n",
      "  5776/100000: episode: 315, duration: 0.017s, episode steps: 16, steps per second: 965, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.812 [0.000, 1.000], mean observation: -0.080 [-3.015, 1.932], mean_best_reward: --\n",
      "  5792/100000: episode: 316, duration: 0.015s, episode steps: 16, steps per second: 1074, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.096 [-0.830, 1.605], mean_best_reward: --\n",
      "  5812/100000: episode: 317, duration: 0.018s, episode steps: 20, steps per second: 1125, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.058 [-1.584, 2.494], mean_best_reward: --\n",
      "  5846/100000: episode: 318, duration: 0.028s, episode steps: 34, steps per second: 1210, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.382 [0.000, 1.000], mean observation: 0.058 [-1.913, 2.796], mean_best_reward: --\n",
      "  5865/100000: episode: 319, duration: 0.017s, episode steps: 19, steps per second: 1129, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.092 [-0.775, 1.217], mean_best_reward: --\n",
      "  5878/100000: episode: 320, duration: 0.013s, episode steps: 13, steps per second: 1027, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.099 [-2.904, 1.914], mean_best_reward: --\n",
      "  5906/100000: episode: 321, duration: 0.024s, episode steps: 28, steps per second: 1170, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.048, 0.635], mean_best_reward: --\n",
      "  5917/100000: episode: 322, duration: 0.014s, episode steps: 11, steps per second: 785, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.120 [-1.338, 2.095], mean_best_reward: --\n",
      "  5928/100000: episode: 323, duration: 0.013s, episode steps: 11, steps per second: 832, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.138 [-1.740, 2.861], mean_best_reward: --\n",
      "  5948/100000: episode: 324, duration: 0.019s, episode steps: 20, steps per second: 1033, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.087 [-1.329, 2.258], mean_best_reward: --\n",
      "  5964/100000: episode: 325, duration: 0.014s, episode steps: 16, steps per second: 1139, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.061 [-1.415, 2.169], mean_best_reward: --\n",
      "  5980/100000: episode: 326, duration: 0.015s, episode steps: 16, steps per second: 1101, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.812 [0.000, 1.000], mean observation: -0.091 [-2.983, 1.908], mean_best_reward: --\n",
      "  5996/100000: episode: 327, duration: 0.015s, episode steps: 16, steps per second: 1088, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.061 [-2.480, 1.605], mean_best_reward: --\n",
      "  6009/100000: episode: 328, duration: 0.012s, episode steps: 13, steps per second: 1079, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.096 [-1.010, 1.777], mean_best_reward: --\n",
      "  6055/100000: episode: 329, duration: 0.037s, episode steps: 46, steps per second: 1248, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.125 [-0.947, 0.437], mean_best_reward: --\n",
      "  6076/100000: episode: 330, duration: 0.018s, episode steps: 21, steps per second: 1159, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.097 [-1.068, 0.453], mean_best_reward: --\n",
      "  6088/100000: episode: 331, duration: 0.012s, episode steps: 12, steps per second: 1025, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.108 [-2.406, 1.557], mean_best_reward: --\n",
      "  6097/100000: episode: 332, duration: 0.009s, episode steps: 9, steps per second: 957, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.142 [-1.004, 1.772], mean_best_reward: --\n",
      "  6123/100000: episode: 333, duration: 0.022s, episode steps: 26, steps per second: 1184, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.654 [0.000, 1.000], mean observation: -0.008 [-2.587, 1.927], mean_best_reward: --\n",
      "  6138/100000: episode: 334, duration: 0.016s, episode steps: 15, steps per second: 925, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.104 [-1.322, 2.226], mean_best_reward: --\n",
      "  6150/100000: episode: 335, duration: 0.015s, episode steps: 12, steps per second: 805, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.112 [-1.813, 1.143], mean_best_reward: --\n",
      "  6163/100000: episode: 336, duration: 0.013s, episode steps: 13, steps per second: 1039, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.117 [-2.470, 1.531], mean_best_reward: --\n",
      "  6182/100000: episode: 337, duration: 0.017s, episode steps: 19, steps per second: 1137, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.037 [-2.353, 1.571], mean_best_reward: --\n",
      "  6192/100000: episode: 338, duration: 0.010s, episode steps: 10, steps per second: 1002, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.125 [-2.031, 1.179], mean_best_reward: --\n",
      "  6214/100000: episode: 339, duration: 0.018s, episode steps: 22, steps per second: 1221, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: -0.054 [-2.426, 1.534], mean_best_reward: --\n",
      "  6230/100000: episode: 340, duration: 0.015s, episode steps: 16, steps per second: 1067, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.108 [-2.158, 1.142], mean_best_reward: --\n",
      "  6241/100000: episode: 341, duration: 0.011s, episode steps: 11, steps per second: 1012, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.136 [-2.268, 1.341], mean_best_reward: --\n",
      "  6256/100000: episode: 342, duration: 0.014s, episode steps: 15, steps per second: 1079, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.110 [-2.329, 1.339], mean_best_reward: --\n",
      "  6270/100000: episode: 343, duration: 0.013s, episode steps: 14, steps per second: 1054, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.085 [-1.292, 0.834], mean_best_reward: --\n",
      "  6305/100000: episode: 344, duration: 0.029s, episode steps: 35, steps per second: 1226, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.033 [-1.402, 1.117], mean_best_reward: --\n",
      "  6343/100000: episode: 345, duration: 0.031s, episode steps: 38, steps per second: 1234, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.079 [-0.940, 0.427], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6355/100000: episode: 346, duration: 0.014s, episode steps: 12, steps per second: 871, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.121 [-1.585, 2.521], mean_best_reward: --\n",
      "  6366/100000: episode: 347, duration: 0.013s, episode steps: 11, steps per second: 818, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.136 [-2.347, 1.418], mean_best_reward: --\n",
      "  6390/100000: episode: 348, duration: 0.021s, episode steps: 24, steps per second: 1151, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.107 [-1.544, 2.766], mean_best_reward: --\n",
      "  6401/100000: episode: 349, duration: 0.011s, episode steps: 11, steps per second: 1009, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.135 [-1.762, 2.781], mean_best_reward: --\n",
      "  6415/100000: episode: 350, duration: 0.013s, episode steps: 14, steps per second: 1074, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.114 [-0.954, 1.756], mean_best_reward: --\n",
      "  6442/100000: episode: 351, duration: 0.023s, episode steps: 27, steps per second: 1174, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.093 [-0.931, 0.544], mean_best_reward: 72.000000\n",
      "  6453/100000: episode: 352, duration: 0.011s, episode steps: 11, steps per second: 1023, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.115 [-2.793, 1.777], mean_best_reward: --\n",
      "  6476/100000: episode: 353, duration: 0.022s, episode steps: 23, steps per second: 1050, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.048 [-0.992, 1.661], mean_best_reward: --\n",
      "  6488/100000: episode: 354, duration: 0.012s, episode steps: 12, steps per second: 979, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.121 [-2.179, 1.356], mean_best_reward: --\n",
      "  6499/100000: episode: 355, duration: 0.011s, episode steps: 11, steps per second: 1016, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-3.293, 2.158], mean_best_reward: --\n",
      "  6509/100000: episode: 356, duration: 0.011s, episode steps: 10, steps per second: 952, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.514, 1.546], mean_best_reward: --\n",
      "  6531/100000: episode: 357, duration: 0.019s, episode steps: 22, steps per second: 1137, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.056 [-1.345, 0.940], mean_best_reward: --\n",
      "  6542/100000: episode: 358, duration: 0.011s, episode steps: 11, steps per second: 1015, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.151 [-2.908, 1.738], mean_best_reward: --\n",
      "  6556/100000: episode: 359, duration: 0.013s, episode steps: 14, steps per second: 1081, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.110 [-1.602, 0.784], mean_best_reward: --\n",
      "  6568/100000: episode: 360, duration: 0.019s, episode steps: 12, steps per second: 642, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.114 [-2.208, 1.325], mean_best_reward: --\n",
      "  6590/100000: episode: 361, duration: 0.023s, episode steps: 22, steps per second: 960, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.072 [-0.999, 1.863], mean_best_reward: --\n",
      "  6603/100000: episode: 362, duration: 0.012s, episode steps: 13, steps per second: 1046, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.090 [-2.294, 1.395], mean_best_reward: --\n",
      "  6631/100000: episode: 363, duration: 0.024s, episode steps: 28, steps per second: 1175, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.055 [-1.173, 0.553], mean_best_reward: --\n",
      "  6641/100000: episode: 364, duration: 0.010s, episode steps: 10, steps per second: 984, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-3.009, 1.999], mean_best_reward: --\n",
      "  6651/100000: episode: 365, duration: 0.010s, episode steps: 10, steps per second: 974, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.104 [-2.028, 1.230], mean_best_reward: --\n",
      "  6741/100000: episode: 366, duration: 0.071s, episode steps: 90, steps per second: 1269, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.142 [-2.679, 2.166], mean_best_reward: --\n",
      "  6754/100000: episode: 367, duration: 0.013s, episode steps: 13, steps per second: 1021, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.102 [-2.235, 1.377], mean_best_reward: --\n",
      "  6765/100000: episode: 368, duration: 0.011s, episode steps: 11, steps per second: 984, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.135 [-2.785, 1.738], mean_best_reward: --\n",
      "  6799/100000: episode: 369, duration: 0.031s, episode steps: 34, steps per second: 1115, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.618 [0.000, 1.000], mean observation: -0.028 [-2.467, 1.738], mean_best_reward: --\n",
      "  6810/100000: episode: 370, duration: 0.015s, episode steps: 11, steps per second: 719, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.110 [-1.978, 1.162], mean_best_reward: --\n",
      "  6820/100000: episode: 371, duration: 0.010s, episode steps: 10, steps per second: 1005, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.139 [-2.470, 1.517], mean_best_reward: --\n",
      "  6832/100000: episode: 372, duration: 0.012s, episode steps: 12, steps per second: 1039, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.108 [-2.554, 1.575], mean_best_reward: --\n",
      "  6841/100000: episode: 373, duration: 0.010s, episode steps: 9, steps per second: 946, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.830, 1.752], mean_best_reward: --\n",
      "  6891/100000: episode: 374, duration: 0.040s, episode steps: 50, steps per second: 1258, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.116 [-0.422, 0.853], mean_best_reward: --\n",
      "  6909/100000: episode: 375, duration: 0.016s, episode steps: 18, steps per second: 1098, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.055 [-1.566, 2.435], mean_best_reward: --\n",
      "  6921/100000: episode: 376, duration: 0.012s, episode steps: 12, steps per second: 1011, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.120 [-0.781, 1.538], mean_best_reward: --\n",
      "  6933/100000: episode: 377, duration: 0.012s, episode steps: 12, steps per second: 1026, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.096 [-1.538, 1.001], mean_best_reward: --\n",
      "  6942/100000: episode: 378, duration: 0.009s, episode steps: 9, steps per second: 1005, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.136 [-2.447, 1.542], mean_best_reward: --\n",
      "  6952/100000: episode: 379, duration: 0.010s, episode steps: 10, steps per second: 968, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.128 [-0.979, 1.694], mean_best_reward: --\n",
      "  6968/100000: episode: 380, duration: 0.015s, episode steps: 16, steps per second: 1091, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.073 [-1.538, 2.329], mean_best_reward: --\n",
      "  6997/100000: episode: 381, duration: 0.024s, episode steps: 29, steps per second: 1204, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.008 [-1.154, 0.818], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7013/100000: episode: 382, duration: 0.017s, episode steps: 16, steps per second: 944, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.085 [-1.561, 2.470], mean_best_reward: --\n",
      "  7035/100000: episode: 383, duration: 0.022s, episode steps: 22, steps per second: 989, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.069 [-1.143, 1.782], mean_best_reward: --\n",
      "  7061/100000: episode: 384, duration: 0.022s, episode steps: 26, steps per second: 1161, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.041 [-1.023, 1.564], mean_best_reward: --\n",
      "  7084/100000: episode: 385, duration: 0.020s, episode steps: 23, steps per second: 1138, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.085 [-1.146, 0.584], mean_best_reward: --\n",
      "  7096/100000: episode: 386, duration: 0.012s, episode steps: 12, steps per second: 1041, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.125 [-1.711, 0.962], mean_best_reward: --\n",
      "  7107/100000: episode: 387, duration: 0.011s, episode steps: 11, steps per second: 987, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.100 [-2.159, 1.385], mean_best_reward: --\n",
      "  7118/100000: episode: 388, duration: 0.011s, episode steps: 11, steps per second: 978, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.104 [-2.423, 1.545], mean_best_reward: --\n",
      "  7134/100000: episode: 389, duration: 0.018s, episode steps: 16, steps per second: 891, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.211, 0.783], mean_best_reward: --\n",
      "  7145/100000: episode: 390, duration: 0.012s, episode steps: 11, steps per second: 888, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.119 [-2.212, 1.369], mean_best_reward: --\n",
      "  7174/100000: episode: 391, duration: 0.031s, episode steps: 29, steps per second: 926, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.065 [-1.200, 0.425], mean_best_reward: --\n",
      "  7183/100000: episode: 392, duration: 0.012s, episode steps: 9, steps per second: 767, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.134 [-2.294, 1.350], mean_best_reward: --\n",
      "  7205/100000: episode: 393, duration: 0.032s, episode steps: 22, steps per second: 683, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.082 [-1.181, 0.730], mean_best_reward: --\n",
      "  7223/100000: episode: 394, duration: 0.030s, episode steps: 18, steps per second: 605, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.086 [-1.222, 2.132], mean_best_reward: --\n",
      "  7235/100000: episode: 395, duration: 0.018s, episode steps: 12, steps per second: 677, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.103 [-1.206, 1.882], mean_best_reward: --\n",
      "  7244/100000: episode: 396, duration: 0.013s, episode steps: 9, steps per second: 700, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.169 [-2.819, 1.717], mean_best_reward: --\n",
      "  7255/100000: episode: 397, duration: 0.015s, episode steps: 11, steps per second: 735, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.117 [-2.314, 1.418], mean_best_reward: --\n",
      "  7266/100000: episode: 398, duration: 0.014s, episode steps: 11, steps per second: 775, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.103 [-2.045, 1.384], mean_best_reward: --\n",
      "  7279/100000: episode: 399, duration: 0.016s, episode steps: 13, steps per second: 805, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.084 [-2.116, 1.393], mean_best_reward: --\n",
      "  7290/100000: episode: 400, duration: 0.013s, episode steps: 11, steps per second: 849, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.113 [-2.833, 1.782], mean_best_reward: --\n",
      "  7307/100000: episode: 401, duration: 0.022s, episode steps: 17, steps per second: 776, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.055 [-2.734, 1.778], mean_best_reward: 48.000000\n",
      "  7320/100000: episode: 402, duration: 0.016s, episode steps: 13, steps per second: 792, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.068 [-1.861, 1.224], mean_best_reward: --\n",
      "  7343/100000: episode: 403, duration: 0.025s, episode steps: 23, steps per second: 913, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.027 [-1.666, 1.014], mean_best_reward: --\n",
      "  7368/100000: episode: 404, duration: 0.033s, episode steps: 25, steps per second: 762, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.068 [-1.150, 0.595], mean_best_reward: --\n",
      "  7421/100000: episode: 405, duration: 0.048s, episode steps: 53, steps per second: 1107, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.083 [-1.373, 0.559], mean_best_reward: --\n",
      "  7445/100000: episode: 406, duration: 0.021s, episode steps: 24, steps per second: 1122, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.058 [-1.562, 2.552], mean_best_reward: --\n",
      "  7466/100000: episode: 407, duration: 0.020s, episode steps: 21, steps per second: 1055, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.034 [-1.032, 1.633], mean_best_reward: --\n",
      "  7507/100000: episode: 408, duration: 0.033s, episode steps: 41, steps per second: 1252, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.095 [-0.473, 1.003], mean_best_reward: --\n",
      "  7523/100000: episode: 409, duration: 0.014s, episode steps: 16, steps per second: 1106, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.071 [-1.972, 1.209], mean_best_reward: --\n",
      "  7533/100000: episode: 410, duration: 0.010s, episode steps: 10, steps per second: 1007, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.129 [-2.607, 1.577], mean_best_reward: --\n",
      "  7563/100000: episode: 411, duration: 0.025s, episode steps: 30, steps per second: 1220, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.068 [-0.923, 0.583], mean_best_reward: --\n",
      "  7578/100000: episode: 412, duration: 0.014s, episode steps: 15, steps per second: 1076, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.096 [-0.979, 1.606], mean_best_reward: --\n",
      "  7588/100000: episode: 413, duration: 0.011s, episode steps: 10, steps per second: 937, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.112 [-1.301, 0.801], mean_best_reward: --\n",
      "  7657/100000: episode: 414, duration: 0.059s, episode steps: 69, steps per second: 1162, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.116 [-1.820, 0.606], mean_best_reward: --\n",
      "  7669/100000: episode: 415, duration: 0.016s, episode steps: 12, steps per second: 749, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.144 [-1.528, 2.619], mean_best_reward: --\n",
      "  7693/100000: episode: 416, duration: 0.030s, episode steps: 24, steps per second: 802, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.078 [-0.787, 1.738], mean_best_reward: --\n",
      "  7719/100000: episode: 417, duration: 0.033s, episode steps: 26, steps per second: 788, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.048 [-0.967, 1.441], mean_best_reward: --\n",
      "  7731/100000: episode: 418, duration: 0.019s, episode steps: 12, steps per second: 639, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.107 [-2.441, 1.581], mean_best_reward: --\n",
      "  7744/100000: episode: 419, duration: 0.019s, episode steps: 13, steps per second: 691, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.106 [-1.543, 0.956], mean_best_reward: --\n",
      "  7755/100000: episode: 420, duration: 0.014s, episode steps: 11, steps per second: 775, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.142 [-1.341, 2.310], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7772/100000: episode: 421, duration: 0.029s, episode steps: 17, steps per second: 587, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.062 [-1.416, 0.833], mean_best_reward: --\n",
      "  7784/100000: episode: 422, duration: 0.020s, episode steps: 12, steps per second: 603, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.135 [-2.596, 1.538], mean_best_reward: --\n",
      "  7804/100000: episode: 423, duration: 0.025s, episode steps: 20, steps per second: 794, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.050 [-2.352, 1.514], mean_best_reward: --\n",
      "  7822/100000: episode: 424, duration: 0.020s, episode steps: 18, steps per second: 896, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.110, 0.616], mean_best_reward: --\n",
      "  7841/100000: episode: 425, duration: 0.022s, episode steps: 19, steps per second: 856, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.082 [-1.575, 2.607], mean_best_reward: --\n",
      "  7878/100000: episode: 426, duration: 0.040s, episode steps: 37, steps per second: 930, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.013 [-1.079, 0.942], mean_best_reward: --\n",
      "  7892/100000: episode: 427, duration: 0.013s, episode steps: 14, steps per second: 1052, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.101 [-1.573, 0.964], mean_best_reward: --\n",
      "  7918/100000: episode: 428, duration: 0.022s, episode steps: 26, steps per second: 1186, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.047 [-0.822, 1.330], mean_best_reward: --\n",
      "  7945/100000: episode: 429, duration: 0.023s, episode steps: 27, steps per second: 1151, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.110 [-0.928, 0.625], mean_best_reward: --\n",
      "  7957/100000: episode: 430, duration: 0.013s, episode steps: 12, steps per second: 947, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.115 [-2.008, 1.185], mean_best_reward: --\n",
      "  7967/100000: episode: 431, duration: 0.015s, episode steps: 10, steps per second: 647, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.146 [-1.740, 2.723], mean_best_reward: --\n",
      "  7980/100000: episode: 432, duration: 0.013s, episode steps: 13, steps per second: 1021, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.107 [-1.415, 2.333], mean_best_reward: --\n",
      "  8030/100000: episode: 433, duration: 0.038s, episode steps: 50, steps per second: 1300, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.072 [-0.347, 0.951], mean_best_reward: --\n",
      "  8042/100000: episode: 434, duration: 0.013s, episode steps: 12, steps per second: 952, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.100 [-1.610, 2.492], mean_best_reward: --\n",
      "  8061/100000: episode: 435, duration: 0.017s, episode steps: 19, steps per second: 1113, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.095 [-0.753, 1.254], mean_best_reward: --\n",
      "  8076/100000: episode: 436, duration: 0.014s, episode steps: 15, steps per second: 1036, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.081 [-1.866, 1.172], mean_best_reward: --\n",
      "  8089/100000: episode: 437, duration: 0.013s, episode steps: 13, steps per second: 978, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.124 [-2.291, 1.353], mean_best_reward: --\n",
      "  8103/100000: episode: 438, duration: 0.013s, episode steps: 14, steps per second: 1071, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.049, 0.628], mean_best_reward: --\n",
      "  8123/100000: episode: 439, duration: 0.018s, episode steps: 20, steps per second: 1115, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.077 [-0.996, 1.761], mean_best_reward: --\n",
      "  8145/100000: episode: 440, duration: 0.020s, episode steps: 22, steps per second: 1128, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.078 [-0.577, 1.114], mean_best_reward: --\n",
      "  8161/100000: episode: 441, duration: 0.015s, episode steps: 16, steps per second: 1080, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.070 [-0.986, 1.650], mean_best_reward: --\n",
      "  8171/100000: episode: 442, duration: 0.013s, episode steps: 10, steps per second: 761, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.132 [-2.053, 1.212], mean_best_reward: --\n",
      "  8180/100000: episode: 443, duration: 0.014s, episode steps: 9, steps per second: 641, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.132 [-2.277, 1.389], mean_best_reward: --\n",
      "  8280/100000: episode: 444, duration: 0.078s, episode steps: 100, steps per second: 1278, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.004 [-0.831, 1.468], mean_best_reward: --\n",
      "  8369/100000: episode: 445, duration: 0.072s, episode steps: 89, steps per second: 1230, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.104 [-1.322, 1.629], mean_best_reward: --\n",
      "  8384/100000: episode: 446, duration: 0.014s, episode steps: 15, steps per second: 1086, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.074 [-1.091, 0.611], mean_best_reward: --\n",
      "  8397/100000: episode: 447, duration: 0.013s, episode steps: 13, steps per second: 1034, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.100 [-1.288, 0.797], mean_best_reward: --\n",
      "  8408/100000: episode: 448, duration: 0.014s, episode steps: 11, steps per second: 773, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.107 [-2.738, 1.783], mean_best_reward: --\n",
      "  8433/100000: episode: 449, duration: 0.027s, episode steps: 25, steps per second: 930, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.017 [-2.099, 1.583], mean_best_reward: --\n",
      "  8450/100000: episode: 450, duration: 0.016s, episode steps: 17, steps per second: 1059, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.095 [-1.289, 0.781], mean_best_reward: --\n",
      "  8465/100000: episode: 451, duration: 0.014s, episode steps: 15, steps per second: 1044, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.117 [-1.330, 0.587], mean_best_reward: 30.000000\n",
      "  8488/100000: episode: 452, duration: 0.020s, episode steps: 23, steps per second: 1170, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.067 [-2.184, 1.351], mean_best_reward: --\n",
      "  8506/100000: episode: 453, duration: 0.016s, episode steps: 18, steps per second: 1108, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.087 [-2.032, 1.135], mean_best_reward: --\n",
      "  8519/100000: episode: 454, duration: 0.012s, episode steps: 13, steps per second: 1063, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.098 [-1.348, 2.216], mean_best_reward: --\n",
      "  8544/100000: episode: 455, duration: 0.021s, episode steps: 25, steps per second: 1189, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.045 [-1.182, 0.783], mean_best_reward: --\n",
      "  8556/100000: episode: 456, duration: 0.011s, episode steps: 12, steps per second: 1056, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.102 [-0.779, 1.377], mean_best_reward: --\n",
      "  8584/100000: episode: 457, duration: 0.023s, episode steps: 28, steps per second: 1205, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: -0.016 [-1.555, 2.192], mean_best_reward: --\n",
      "  8596/100000: episode: 458, duration: 0.012s, episode steps: 12, steps per second: 1023, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.103 [-2.707, 1.796], mean_best_reward: --\n",
      "  8613/100000: episode: 459, duration: 0.015s, episode steps: 17, steps per second: 1129, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.089 [-1.574, 2.518], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8626/100000: episode: 460, duration: 0.014s, episode steps: 13, steps per second: 898, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.109 [-1.351, 2.304], mean_best_reward: --\n",
      "  8642/100000: episode: 461, duration: 0.017s, episode steps: 16, steps per second: 933, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.090 [-0.770, 1.282], mean_best_reward: --\n",
      "  8681/100000: episode: 462, duration: 0.032s, episode steps: 39, steps per second: 1233, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.075 [-0.755, 1.234], mean_best_reward: --\n",
      "  8695/100000: episode: 463, duration: 0.013s, episode steps: 14, steps per second: 1091, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.078 [-1.794, 1.198], mean_best_reward: --\n",
      "  8728/100000: episode: 464, duration: 0.027s, episode steps: 33, steps per second: 1216, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.089 [-1.198, 0.736], mean_best_reward: --\n",
      "  8742/100000: episode: 465, duration: 0.013s, episode steps: 14, steps per second: 1067, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.083 [-1.591, 1.030], mean_best_reward: --\n",
      "  8766/100000: episode: 466, duration: 0.020s, episode steps: 24, steps per second: 1177, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.045 [-1.283, 0.788], mean_best_reward: --\n",
      "  8781/100000: episode: 467, duration: 0.014s, episode steps: 15, steps per second: 1092, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.080 [-1.165, 1.930], mean_best_reward: --\n",
      "  8799/100000: episode: 468, duration: 0.017s, episode steps: 18, steps per second: 1086, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.043 [-1.964, 2.800], mean_best_reward: --\n",
      "  8813/100000: episode: 469, duration: 0.013s, episode steps: 14, steps per second: 1080, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.107 [-2.113, 1.166], mean_best_reward: --\n",
      "  8835/100000: episode: 470, duration: 0.019s, episode steps: 22, steps per second: 1156, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.227 [0.000, 1.000], mean observation: 0.034 [-2.362, 3.383], mean_best_reward: --\n",
      "  8847/100000: episode: 471, duration: 0.016s, episode steps: 12, steps per second: 736, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.103 [-2.174, 1.387], mean_best_reward: --\n",
      "  8869/100000: episode: 472, duration: 0.024s, episode steps: 22, steps per second: 914, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.050 [-1.362, 0.942], mean_best_reward: --\n",
      "  8879/100000: episode: 473, duration: 0.010s, episode steps: 10, steps per second: 976, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.128 [-2.557, 1.586], mean_best_reward: --\n",
      "  8893/100000: episode: 474, duration: 0.013s, episode steps: 14, steps per second: 1109, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.112 [-2.491, 1.542], mean_best_reward: --\n",
      "  8905/100000: episode: 475, duration: 0.012s, episode steps: 12, steps per second: 1027, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.110 [-2.082, 1.330], mean_best_reward: --\n",
      "  8917/100000: episode: 476, duration: 0.012s, episode steps: 12, steps per second: 1032, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.103 [-1.199, 1.904], mean_best_reward: --\n",
      "  8940/100000: episode: 477, duration: 0.019s, episode steps: 23, steps per second: 1180, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.084 [-1.243, 0.601], mean_best_reward: --\n",
      "  9018/100000: episode: 478, duration: 0.059s, episode steps: 78, steps per second: 1314, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.159 [-1.846, 1.681], mean_best_reward: --\n",
      "  9034/100000: episode: 479, duration: 0.015s, episode steps: 16, steps per second: 1064, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.092 [-1.136, 2.035], mean_best_reward: --\n",
      "  9045/100000: episode: 480, duration: 0.011s, episode steps: 11, steps per second: 1015, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.104 [-2.201, 1.415], mean_best_reward: --\n",
      "  9056/100000: episode: 481, duration: 0.011s, episode steps: 11, steps per second: 998, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.123 [-2.740, 1.779], mean_best_reward: --\n",
      "  9079/100000: episode: 482, duration: 0.024s, episode steps: 23, steps per second: 965, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.106 [-2.138, 0.990], mean_best_reward: --\n",
      "  9099/100000: episode: 483, duration: 0.021s, episode steps: 20, steps per second: 955, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.062 [-1.527, 2.399], mean_best_reward: --\n",
      "  9110/100000: episode: 484, duration: 0.011s, episode steps: 11, steps per second: 1046, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.114 [-1.402, 2.351], mean_best_reward: --\n",
      "  9122/100000: episode: 485, duration: 0.012s, episode steps: 12, steps per second: 1024, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.103 [-2.188, 1.348], mean_best_reward: --\n",
      "  9134/100000: episode: 486, duration: 0.012s, episode steps: 12, steps per second: 1030, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.106 [-1.797, 2.694], mean_best_reward: --\n",
      "  9253/100000: episode: 487, duration: 0.090s, episode steps: 119, steps per second: 1325, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.358 [-0.827, 2.068], mean_best_reward: --\n",
      "  9263/100000: episode: 488, duration: 0.010s, episode steps: 10, steps per second: 972, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.546, 1.605], mean_best_reward: --\n",
      "  9277/100000: episode: 489, duration: 0.013s, episode steps: 14, steps per second: 1038, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.085 [-1.576, 2.467], mean_best_reward: --\n",
      "  9289/100000: episode: 490, duration: 0.012s, episode steps: 12, steps per second: 1013, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.122 [-1.597, 2.591], mean_best_reward: --\n",
      "  9303/100000: episode: 491, duration: 0.013s, episode steps: 14, steps per second: 1051, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.115 [-1.720, 0.940], mean_best_reward: --\n",
      "  9320/100000: episode: 492, duration: 0.024s, episode steps: 17, steps per second: 699, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.084 [-0.788, 1.421], mean_best_reward: --\n",
      "  9336/100000: episode: 493, duration: 0.015s, episode steps: 16, steps per second: 1079, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.122 [-2.135, 1.155], mean_best_reward: --\n",
      "  9358/100000: episode: 494, duration: 0.019s, episode steps: 22, steps per second: 1138, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.058 [-1.353, 2.223], mean_best_reward: --\n",
      "  9370/100000: episode: 495, duration: 0.012s, episode steps: 12, steps per second: 1022, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.111 [-1.989, 1.213], mean_best_reward: --\n",
      "  9381/100000: episode: 496, duration: 0.011s, episode steps: 11, steps per second: 1019, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.128 [-1.563, 2.544], mean_best_reward: --\n",
      "  9404/100000: episode: 497, duration: 0.020s, episode steps: 23, steps per second: 1132, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.040 [-1.375, 2.169], mean_best_reward: --\n",
      "  9419/100000: episode: 498, duration: 0.014s, episode steps: 15, steps per second: 1062, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.084 [-1.325, 2.206], mean_best_reward: --\n",
      "  9433/100000: episode: 499, duration: 0.014s, episode steps: 14, steps per second: 1031, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.100 [-1.369, 2.178], mean_best_reward: --\n",
      "  9445/100000: episode: 500, duration: 0.012s, episode steps: 12, steps per second: 1043, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.118 [-2.583, 1.574], mean_best_reward: --\n",
      "  9453/100000: episode: 501, duration: 0.009s, episode steps: 8, steps per second: 861, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.139 [-1.178, 2.022], mean_best_reward: 47.500000\n",
      "  9471/100000: episode: 502, duration: 0.017s, episode steps: 18, steps per second: 1085, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.744, 1.144], mean_best_reward: --\n",
      "  9510/100000: episode: 503, duration: 0.033s, episode steps: 39, steps per second: 1193, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.048 [-1.393, 0.817], mean_best_reward: --\n",
      "  9524/100000: episode: 504, duration: 0.013s, episode steps: 14, steps per second: 1064, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.097 [-1.675, 0.986], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9546/100000: episode: 505, duration: 0.025s, episode steps: 22, steps per second: 883, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.076 [-0.583, 1.114], mean_best_reward: --\n",
      "  9576/100000: episode: 506, duration: 0.029s, episode steps: 30, steps per second: 1042, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.104 [-0.787, 1.573], mean_best_reward: --\n",
      "  9584/100000: episode: 507, duration: 0.009s, episode steps: 8, steps per second: 910, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.611, 2.547], mean_best_reward: --\n",
      "  9595/100000: episode: 508, duration: 0.011s, episode steps: 11, steps per second: 1005, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.141 [-1.749, 2.894], mean_best_reward: --\n",
      "  9635/100000: episode: 509, duration: 0.031s, episode steps: 40, steps per second: 1272, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.121 [-1.162, 0.520], mean_best_reward: --\n",
      "  9645/100000: episode: 510, duration: 0.010s, episode steps: 10, steps per second: 961, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.145 [-2.026, 1.176], mean_best_reward: --\n",
      "  9662/100000: episode: 511, duration: 0.015s, episode steps: 17, steps per second: 1112, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.100 [-0.805, 1.239], mean_best_reward: --\n",
      "  9674/100000: episode: 512, duration: 0.012s, episode steps: 12, steps per second: 1036, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.094 [-1.559, 2.488], mean_best_reward: --\n",
      "  9698/100000: episode: 513, duration: 0.020s, episode steps: 24, steps per second: 1177, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.106 [-0.562, 1.366], mean_best_reward: --\n",
      "  9776/100000: episode: 514, duration: 0.061s, episode steps: 78, steps per second: 1277, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: -0.319 [-2.463, 1.666], mean_best_reward: --\n",
      "  9809/100000: episode: 515, duration: 0.034s, episode steps: 33, steps per second: 974, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.056 [-0.637, 1.346], mean_best_reward: --\n",
      "  9825/100000: episode: 516, duration: 0.014s, episode steps: 16, steps per second: 1112, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.094 [-1.147, 1.938], mean_best_reward: --\n",
      "  9870/100000: episode: 517, duration: 0.036s, episode steps: 45, steps per second: 1253, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.078 [-0.396, 0.955], mean_best_reward: --\n",
      "  9894/100000: episode: 518, duration: 0.020s, episode steps: 24, steps per second: 1191, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.005 [-1.785, 2.503], mean_best_reward: --\n",
      "  9910/100000: episode: 519, duration: 0.014s, episode steps: 16, steps per second: 1106, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.114 [-2.198, 1.154], mean_best_reward: --\n",
      "  9936/100000: episode: 520, duration: 0.022s, episode steps: 26, steps per second: 1188, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.777, 1.161], mean_best_reward: --\n",
      "  9946/100000: episode: 521, duration: 0.010s, episode steps: 10, steps per second: 995, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.137 [-1.347, 2.259], mean_best_reward: --\n",
      "  9960/100000: episode: 522, duration: 0.013s, episode steps: 14, steps per second: 1067, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.069 [-1.640, 1.002], mean_best_reward: --\n",
      "  9976/100000: episode: 523, duration: 0.014s, episode steps: 16, steps per second: 1121, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.099 [-1.627, 0.948], mean_best_reward: --\n",
      "  9987/100000: episode: 524, duration: 0.011s, episode steps: 11, steps per second: 1025, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.117 [-1.976, 1.151], mean_best_reward: --\n",
      "  9998/100000: episode: 525, duration: 0.013s, episode steps: 11, steps per second: 877, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.091 [-1.804, 2.768], mean_best_reward: --\n",
      " 10047/100000: episode: 526, duration: 0.043s, episode steps: 49, steps per second: 1144, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.105 [-0.948, 0.575], mean_best_reward: --\n",
      " 10062/100000: episode: 527, duration: 0.014s, episode steps: 15, steps per second: 1074, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.092 [-1.720, 2.769], mean_best_reward: --\n",
      " 10073/100000: episode: 528, duration: 0.011s, episode steps: 11, steps per second: 1013, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.127 [-1.338, 2.225], mean_best_reward: --\n",
      " 10087/100000: episode: 529, duration: 0.013s, episode steps: 14, steps per second: 1075, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.077 [-1.225, 1.974], mean_best_reward: --\n",
      " 10124/100000: episode: 530, duration: 0.030s, episode steps: 37, steps per second: 1238, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.062 [-1.013, 0.580], mean_best_reward: --\n",
      " 10142/100000: episode: 531, duration: 0.016s, episode steps: 18, steps per second: 1128, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.105 [-2.201, 1.148], mean_best_reward: --\n",
      " 10152/100000: episode: 532, duration: 0.010s, episode steps: 10, steps per second: 979, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.946, 3.038], mean_best_reward: --\n",
      " 10171/100000: episode: 533, duration: 0.017s, episode steps: 19, steps per second: 1132, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.090 [-1.418, 0.653], mean_best_reward: --\n",
      " 10197/100000: episode: 534, duration: 0.022s, episode steps: 26, steps per second: 1203, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.079 [-1.344, 0.587], mean_best_reward: --\n",
      " 10207/100000: episode: 535, duration: 0.010s, episode steps: 10, steps per second: 995, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.110 [-1.685, 0.983], mean_best_reward: --\n",
      " 10223/100000: episode: 536, duration: 0.016s, episode steps: 16, steps per second: 991, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.097 [-1.161, 2.116], mean_best_reward: --\n",
      " 10256/100000: episode: 537, duration: 0.034s, episode steps: 33, steps per second: 979, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.026 [-1.203, 0.974], mean_best_reward: --\n",
      " 10287/100000: episode: 538, duration: 0.025s, episode steps: 31, steps per second: 1228, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.099 [-1.214, 0.411], mean_best_reward: --\n",
      " 10312/100000: episode: 539, duration: 0.021s, episode steps: 25, steps per second: 1192, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.056 [-0.824, 1.563], mean_best_reward: --\n",
      " 10328/100000: episode: 540, duration: 0.014s, episode steps: 16, steps per second: 1105, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.124 [-1.145, 2.148], mean_best_reward: --\n",
      " 10338/100000: episode: 541, duration: 0.010s, episode steps: 10, steps per second: 968, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.121 [-1.529, 2.538], mean_best_reward: --\n",
      " 10367/100000: episode: 542, duration: 0.024s, episode steps: 29, steps per second: 1208, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.106 [-0.950, 0.370], mean_best_reward: --\n",
      " 10376/100000: episode: 543, duration: 0.009s, episode steps: 9, steps per second: 955, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.155 [-1.526, 2.509], mean_best_reward: --\n",
      " 10396/100000: episode: 544, duration: 0.018s, episode steps: 20, steps per second: 1129, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.079 [-1.424, 0.776], mean_best_reward: --\n",
      " 10416/100000: episode: 545, duration: 0.018s, episode steps: 20, steps per second: 1133, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.031 [-1.173, 1.837], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10466/100000: episode: 546, duration: 0.049s, episode steps: 50, steps per second: 1014, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.023 [-1.378, 0.601], mean_best_reward: --\n",
      " 10479/100000: episode: 547, duration: 0.012s, episode steps: 13, steps per second: 1092, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.089 [-2.673, 1.776], mean_best_reward: --\n",
      " 10489/100000: episode: 548, duration: 0.011s, episode steps: 10, steps per second: 926, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.154 [-1.720, 2.716], mean_best_reward: --\n",
      " 10507/100000: episode: 549, duration: 0.016s, episode steps: 18, steps per second: 1108, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.066 [-0.991, 0.630], mean_best_reward: --\n",
      " 10526/100000: episode: 550, duration: 0.017s, episode steps: 19, steps per second: 1147, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.061 [-0.990, 1.629], mean_best_reward: --\n",
      " 10548/100000: episode: 551, duration: 0.020s, episode steps: 22, steps per second: 1114, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-0.929, 0.410], mean_best_reward: 39.500000\n",
      " 10562/100000: episode: 552, duration: 0.014s, episode steps: 14, steps per second: 1015, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.088 [-1.612, 2.593], mean_best_reward: --\n",
      " 10575/100000: episode: 553, duration: 0.014s, episode steps: 13, steps per second: 941, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.086 [-0.978, 1.702], mean_best_reward: --\n",
      " 10585/100000: episode: 554, duration: 0.010s, episode steps: 10, steps per second: 963, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.127 [-1.975, 1.172], mean_best_reward: --\n",
      " 10595/100000: episode: 555, duration: 0.010s, episode steps: 10, steps per second: 978, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.108 [-2.419, 1.602], mean_best_reward: --\n",
      " 10610/100000: episode: 556, duration: 0.014s, episode steps: 15, steps per second: 1091, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.125 [-1.343, 2.401], mean_best_reward: --\n",
      " 10632/100000: episode: 557, duration: 0.019s, episode steps: 22, steps per second: 1144, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.061 [-1.900, 1.146], mean_best_reward: --\n",
      " 10644/100000: episode: 558, duration: 0.012s, episode steps: 12, steps per second: 1021, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.126 [-1.538, 2.575], mean_best_reward: --\n",
      " 10653/100000: episode: 559, duration: 0.009s, episode steps: 9, steps per second: 973, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.159 [-2.516, 1.540], mean_best_reward: --\n",
      " 10664/100000: episode: 560, duration: 0.011s, episode steps: 11, steps per second: 1024, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.123 [-1.808, 2.786], mean_best_reward: --\n",
      " 10677/100000: episode: 561, duration: 0.015s, episode steps: 13, steps per second: 850, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.099 [-2.802, 1.751], mean_best_reward: --\n",
      " 10706/100000: episode: 562, duration: 0.028s, episode steps: 29, steps per second: 1051, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.054 [-0.805, 1.379], mean_best_reward: --\n",
      " 10719/100000: episode: 563, duration: 0.013s, episode steps: 13, steps per second: 1018, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.126 [-2.930, 1.805], mean_best_reward: --\n",
      " 10747/100000: episode: 564, duration: 0.023s, episode steps: 28, steps per second: 1205, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.607 [0.000, 1.000], mean observation: -0.029 [-1.990, 1.224], mean_best_reward: --\n",
      " 10795/100000: episode: 565, duration: 0.038s, episode steps: 48, steps per second: 1272, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.000 [-0.616, 0.882], mean_best_reward: --\n",
      " 10818/100000: episode: 566, duration: 0.020s, episode steps: 23, steps per second: 1141, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.030 [-1.410, 2.205], mean_best_reward: --\n",
      " 10833/100000: episode: 567, duration: 0.014s, episode steps: 15, steps per second: 1078, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.083 [-2.302, 1.408], mean_best_reward: --\n",
      " 10842/100000: episode: 568, duration: 0.010s, episode steps: 9, steps per second: 944, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.132 [-2.345, 1.421], mean_best_reward: --\n",
      " 10859/100000: episode: 569, duration: 0.016s, episode steps: 17, steps per second: 1090, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.101 [-1.973, 1.023], mean_best_reward: --\n",
      " 10870/100000: episode: 570, duration: 0.011s, episode steps: 11, steps per second: 1025, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.107 [-2.790, 1.800], mean_best_reward: --\n",
      " 10882/100000: episode: 571, duration: 0.011s, episode steps: 12, steps per second: 1050, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.127 [-2.282, 1.379], mean_best_reward: --\n",
      " 10896/100000: episode: 572, duration: 0.014s, episode steps: 14, steps per second: 1014, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.090 [-2.590, 1.591], mean_best_reward: --\n",
      " 10912/100000: episode: 573, duration: 0.023s, episode steps: 16, steps per second: 682, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.089 [-1.000, 1.776], mean_best_reward: --\n",
      " 10950/100000: episode: 574, duration: 0.031s, episode steps: 38, steps per second: 1229, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.037 [-0.602, 0.920], mean_best_reward: --\n",
      " 10966/100000: episode: 575, duration: 0.015s, episode steps: 16, steps per second: 1100, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.066 [-1.160, 1.920], mean_best_reward: --\n",
      " 10978/100000: episode: 576, duration: 0.011s, episode steps: 12, steps per second: 1075, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.101 [-1.415, 2.180], mean_best_reward: --\n",
      " 11001/100000: episode: 577, duration: 0.019s, episode steps: 23, steps per second: 1181, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.696 [0.000, 1.000], mean observation: -0.041 [-2.596, 1.757], mean_best_reward: --\n",
      " 11018/100000: episode: 578, duration: 0.016s, episode steps: 17, steps per second: 1082, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.109 [-1.894, 0.960], mean_best_reward: --\n",
      " 11029/100000: episode: 579, duration: 0.011s, episode steps: 11, steps per second: 1000, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.113 [-1.475, 0.793], mean_best_reward: --\n",
      " 11039/100000: episode: 580, duration: 0.010s, episode steps: 10, steps per second: 1013, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-3.058, 1.943], mean_best_reward: --\n",
      " 11051/100000: episode: 581, duration: 0.012s, episode steps: 12, steps per second: 1037, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.129 [-1.179, 2.123], mean_best_reward: --\n",
      " 11063/100000: episode: 582, duration: 0.012s, episode steps: 12, steps per second: 1037, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.098 [-2.019, 1.358], mean_best_reward: --\n",
      " 11074/100000: episode: 583, duration: 0.011s, episode steps: 11, steps per second: 1018, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.126 [-1.332, 2.135], mean_best_reward: --\n",
      " 11091/100000: episode: 584, duration: 0.015s, episode steps: 17, steps per second: 1119, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.079 [-1.762, 2.764], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11102/100000: episode: 585, duration: 0.012s, episode steps: 11, steps per second: 922, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.121 [-1.923, 1.184], mean_best_reward: --\n",
      " 11117/100000: episode: 586, duration: 0.021s, episode steps: 15, steps per second: 707, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.078 [-1.416, 2.310], mean_best_reward: --\n",
      " 11129/100000: episode: 587, duration: 0.011s, episode steps: 12, steps per second: 1070, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.097 [-0.813, 1.221], mean_best_reward: --\n",
      " 11144/100000: episode: 588, duration: 0.014s, episode steps: 15, steps per second: 1088, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.082 [-1.388, 2.139], mean_best_reward: --\n",
      " 11162/100000: episode: 589, duration: 0.016s, episode steps: 18, steps per second: 1127, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.098 [-1.042, 0.574], mean_best_reward: --\n",
      " 11175/100000: episode: 590, duration: 0.012s, episode steps: 13, steps per second: 1042, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.126 [-2.837, 1.737], mean_best_reward: --\n",
      " 11187/100000: episode: 591, duration: 0.012s, episode steps: 12, steps per second: 1033, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.091 [-1.714, 1.027], mean_best_reward: --\n",
      " 11201/100000: episode: 592, duration: 0.013s, episode steps: 14, steps per second: 1072, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.109 [-2.023, 1.132], mean_best_reward: --\n",
      " 11215/100000: episode: 593, duration: 0.013s, episode steps: 14, steps per second: 1098, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.133 [-0.948, 1.790], mean_best_reward: --\n",
      " 11238/100000: episode: 594, duration: 0.020s, episode steps: 23, steps per second: 1164, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.013 [-2.243, 1.600], mean_best_reward: --\n",
      " 11249/100000: episode: 595, duration: 0.011s, episode steps: 11, steps per second: 1008, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.100 [-1.596, 2.292], mean_best_reward: --\n",
      " 11263/100000: episode: 596, duration: 0.013s, episode steps: 14, steps per second: 1072, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.107 [-2.613, 1.532], mean_best_reward: --\n",
      " 11275/100000: episode: 597, duration: 0.012s, episode steps: 12, steps per second: 1040, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.131 [-1.580, 0.763], mean_best_reward: --\n",
      " 11284/100000: episode: 598, duration: 0.009s, episode steps: 9, steps per second: 948, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.129 [-1.216, 1.956], mean_best_reward: --\n",
      " 11293/100000: episode: 599, duration: 0.010s, episode steps: 9, steps per second: 943, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.144 [-0.946, 1.717], mean_best_reward: --\n",
      " 11305/100000: episode: 600, duration: 0.014s, episode steps: 12, steps per second: 852, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.113 [-2.163, 1.374], mean_best_reward: --\n",
      " 11316/100000: episode: 601, duration: 0.015s, episode steps: 11, steps per second: 730, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.145 [-1.750, 2.877], mean_best_reward: 33.000000\n",
      " 11325/100000: episode: 602, duration: 0.010s, episode steps: 9, steps per second: 898, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.162 [-0.949, 1.785], mean_best_reward: --\n",
      " 11339/100000: episode: 603, duration: 0.013s, episode steps: 14, steps per second: 1039, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.116 [-0.573, 1.285], mean_best_reward: --\n",
      " 11355/100000: episode: 604, duration: 0.015s, episode steps: 16, steps per second: 1087, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.073 [-1.801, 1.192], mean_best_reward: --\n",
      " 11368/100000: episode: 605, duration: 0.012s, episode steps: 13, steps per second: 1062, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.114 [-0.754, 1.457], mean_best_reward: --\n",
      " 11383/100000: episode: 606, duration: 0.014s, episode steps: 15, steps per second: 1086, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.087 [-1.790, 2.896], mean_best_reward: --\n",
      " 11396/100000: episode: 607, duration: 0.012s, episode steps: 13, steps per second: 1057, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.113 [-1.358, 2.307], mean_best_reward: --\n",
      " 11406/100000: episode: 608, duration: 0.010s, episode steps: 10, steps per second: 1002, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.137 [-1.008, 1.701], mean_best_reward: --\n",
      " 11420/100000: episode: 609, duration: 0.013s, episode steps: 14, steps per second: 1089, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.104 [-0.981, 1.767], mean_best_reward: --\n",
      " 11435/100000: episode: 610, duration: 0.014s, episode steps: 15, steps per second: 1080, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.092 [-0.961, 1.548], mean_best_reward: --\n",
      " 11448/100000: episode: 611, duration: 0.013s, episode steps: 13, steps per second: 1035, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.097 [-2.266, 1.396], mean_best_reward: --\n",
      " 11463/100000: episode: 612, duration: 0.014s, episode steps: 15, steps per second: 1094, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.064 [-2.875, 1.993], mean_best_reward: --\n",
      " 11478/100000: episode: 613, duration: 0.014s, episode steps: 15, steps per second: 1093, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.097 [-1.514, 2.369], mean_best_reward: --\n",
      " 11488/100000: episode: 614, duration: 0.010s, episode steps: 10, steps per second: 965, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.109 [-1.959, 2.962], mean_best_reward: --\n",
      " 11507/100000: episode: 615, duration: 0.018s, episode steps: 19, steps per second: 1065, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.066 [-2.226, 1.334], mean_best_reward: --\n",
      " 11520/100000: episode: 616, duration: 0.019s, episode steps: 13, steps per second: 694, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.118 [-1.410, 2.403], mean_best_reward: --\n",
      " 11538/100000: episode: 617, duration: 0.016s, episode steps: 18, steps per second: 1117, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.044 [-1.896, 1.208], mean_best_reward: --\n",
      " 11552/100000: episode: 618, duration: 0.014s, episode steps: 14, steps per second: 1027, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.090 [-1.385, 2.239], mean_best_reward: --\n",
      " 11567/100000: episode: 619, duration: 0.015s, episode steps: 15, steps per second: 1010, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.082 [-1.729, 2.705], mean_best_reward: --\n",
      " 11578/100000: episode: 620, duration: 0.011s, episode steps: 11, steps per second: 991, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.095 [-1.021, 1.596], mean_best_reward: --\n",
      " 11587/100000: episode: 621, duration: 0.009s, episode steps: 9, steps per second: 983, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.797, 2.872], mean_best_reward: --\n",
      " 11621/100000: episode: 622, duration: 0.027s, episode steps: 34, steps per second: 1240, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.019 [-1.895, 2.710], mean_best_reward: --\n",
      " 11631/100000: episode: 623, duration: 0.010s, episode steps: 10, steps per second: 987, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.125 [-1.541, 2.473], mean_best_reward: --\n",
      " 11677/100000: episode: 624, duration: 0.037s, episode steps: 46, steps per second: 1255, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.057 [-0.955, 0.657], mean_best_reward: --\n",
      " 11694/100000: episode: 625, duration: 0.015s, episode steps: 17, steps per second: 1105, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.101 [-1.339, 2.334], mean_best_reward: --\n",
      " 11708/100000: episode: 626, duration: 0.013s, episode steps: 14, steps per second: 1072, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.113 [-2.601, 1.578], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11722/100000: episode: 627, duration: 0.016s, episode steps: 14, steps per second: 864, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.115 [-1.221, 0.552], mean_best_reward: --\n",
      " 11733/100000: episode: 628, duration: 0.015s, episode steps: 11, steps per second: 748, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.110 [-1.220, 1.765], mean_best_reward: --\n",
      " 11748/100000: episode: 629, duration: 0.014s, episode steps: 15, steps per second: 1051, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.089 [-1.414, 2.290], mean_best_reward: --\n",
      " 11764/100000: episode: 630, duration: 0.015s, episode steps: 16, steps per second: 1103, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.074 [-1.329, 2.132], mean_best_reward: --\n",
      " 11794/100000: episode: 631, duration: 0.024s, episode steps: 30, steps per second: 1243, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.747, 1.079], mean_best_reward: --\n",
      " 11809/100000: episode: 632, duration: 0.014s, episode steps: 15, steps per second: 1059, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.133 [0.000, 1.000], mean observation: 0.088 [-2.146, 3.231], mean_best_reward: --\n",
      " 11821/100000: episode: 633, duration: 0.011s, episode steps: 12, steps per second: 1058, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.120 [-0.771, 1.354], mean_best_reward: --\n",
      " 11832/100000: episode: 634, duration: 0.011s, episode steps: 11, steps per second: 1010, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.121 [-1.407, 2.253], mean_best_reward: --\n",
      " 11850/100000: episode: 635, duration: 0.016s, episode steps: 18, steps per second: 1117, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.082 [-1.008, 1.728], mean_best_reward: --\n",
      " 11863/100000: episode: 636, duration: 0.013s, episode steps: 13, steps per second: 1040, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.105 [-1.000, 1.800], mean_best_reward: --\n",
      " 11897/100000: episode: 637, duration: 0.028s, episode steps: 34, steps per second: 1197, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: -0.026 [-1.726, 1.123], mean_best_reward: --\n",
      " 11906/100000: episode: 638, duration: 0.009s, episode steps: 9, steps per second: 960, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.133 [-1.372, 2.255], mean_best_reward: --\n",
      " 11915/100000: episode: 639, duration: 0.009s, episode steps: 9, steps per second: 956, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.828, 1.766], mean_best_reward: --\n",
      " 11969/100000: episode: 640, duration: 0.051s, episode steps: 54, steps per second: 1055, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.019 [-1.809, 0.995], mean_best_reward: --\n",
      " 11990/100000: episode: 641, duration: 0.018s, episode steps: 21, steps per second: 1141, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.068 [-2.298, 1.371], mean_best_reward: --\n",
      " 12000/100000: episode: 642, duration: 0.010s, episode steps: 10, steps per second: 1010, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.933, 2.988], mean_best_reward: --\n",
      " 12021/100000: episode: 643, duration: 0.018s, episode steps: 21, steps per second: 1155, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.095 [-1.963, 0.991], mean_best_reward: --\n",
      " 12033/100000: episode: 644, duration: 0.011s, episode steps: 12, steps per second: 1051, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.113 [-1.027, 1.772], mean_best_reward: --\n",
      " 12056/100000: episode: 645, duration: 0.020s, episode steps: 23, steps per second: 1171, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.069 [-1.430, 0.778], mean_best_reward: --\n",
      " 12067/100000: episode: 646, duration: 0.011s, episode steps: 11, steps per second: 1010, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.151 [-2.911, 1.767], mean_best_reward: --\n",
      " 12084/100000: episode: 647, duration: 0.015s, episode steps: 17, steps per second: 1132, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.068 [-0.982, 1.728], mean_best_reward: --\n",
      " 12177/100000: episode: 648, duration: 0.071s, episode steps: 93, steps per second: 1307, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.204 [-1.913, 1.486], mean_best_reward: --\n",
      " 12192/100000: episode: 649, duration: 0.014s, episode steps: 15, steps per second: 1061, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.064 [-1.796, 2.573], mean_best_reward: --\n",
      " 12203/100000: episode: 650, duration: 0.013s, episode steps: 11, steps per second: 869, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.132 [-1.383, 2.252], mean_best_reward: --\n",
      " 12214/100000: episode: 651, duration: 0.015s, episode steps: 11, steps per second: 727, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.140 [-1.733, 2.822], mean_best_reward: 63.000000\n",
      " 12233/100000: episode: 652, duration: 0.017s, episode steps: 19, steps per second: 1106, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.099 [-1.128, 0.541], mean_best_reward: --\n",
      " 12254/100000: episode: 653, duration: 0.018s, episode steps: 21, steps per second: 1164, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.068 [-1.349, 2.279], mean_best_reward: --\n",
      " 12265/100000: episode: 654, duration: 0.011s, episode steps: 11, steps per second: 1014, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.107 [-1.729, 1.155], mean_best_reward: --\n",
      " 12280/100000: episode: 655, duration: 0.014s, episode steps: 15, steps per second: 1090, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.123 [-0.778, 1.618], mean_best_reward: --\n",
      " 12300/100000: episode: 656, duration: 0.017s, episode steps: 20, steps per second: 1150, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.042 [-1.406, 2.208], mean_best_reward: --\n",
      " 12311/100000: episode: 657, duration: 0.011s, episode steps: 11, steps per second: 1015, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.112 [-0.784, 1.261], mean_best_reward: --\n",
      " 12320/100000: episode: 658, duration: 0.009s, episode steps: 9, steps per second: 955, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.798, 1.751], mean_best_reward: --\n",
      " 12333/100000: episode: 659, duration: 0.012s, episode steps: 13, steps per second: 1043, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.121 [-0.970, 1.691], mean_best_reward: --\n",
      " 12345/100000: episode: 660, duration: 0.012s, episode steps: 12, steps per second: 1036, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.116 [-2.489, 1.535], mean_best_reward: --\n",
      " 12371/100000: episode: 661, duration: 0.022s, episode steps: 26, steps per second: 1196, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-0.984, 0.569], mean_best_reward: --\n",
      " 12386/100000: episode: 662, duration: 0.015s, episode steps: 15, steps per second: 1031, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.115 [-0.965, 1.841], mean_best_reward: --\n",
      " 12395/100000: episode: 663, duration: 0.010s, episode steps: 9, steps per second: 929, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-2.757, 1.745], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12416/100000: episode: 664, duration: 0.020s, episode steps: 21, steps per second: 1048, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.052 [-1.774, 0.996], mean_best_reward: --\n",
      " 12432/100000: episode: 665, duration: 0.019s, episode steps: 16, steps per second: 853, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.064 [-1.219, 2.028], mean_best_reward: --\n",
      " 12456/100000: episode: 666, duration: 0.020s, episode steps: 24, steps per second: 1200, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.357, 0.907], mean_best_reward: --\n",
      " 12478/100000: episode: 667, duration: 0.019s, episode steps: 22, steps per second: 1156, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.758, 1.453], mean_best_reward: --\n",
      " 12487/100000: episode: 668, duration: 0.009s, episode steps: 9, steps per second: 956, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.164 [-2.329, 1.328], mean_best_reward: --\n",
      " 12516/100000: episode: 669, duration: 0.024s, episode steps: 29, steps per second: 1191, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.030 [-1.180, 0.636], mean_best_reward: --\n",
      " 12542/100000: episode: 670, duration: 0.022s, episode steps: 26, steps per second: 1161, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.112 [-0.573, 1.502], mean_best_reward: --\n",
      " 12553/100000: episode: 671, duration: 0.011s, episode steps: 11, steps per second: 992, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.122 [-1.996, 1.222], mean_best_reward: --\n",
      " 12563/100000: episode: 672, duration: 0.010s, episode steps: 10, steps per second: 954, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.144 [-2.258, 1.403], mean_best_reward: --\n",
      " 12573/100000: episode: 673, duration: 0.010s, episode steps: 10, steps per second: 968, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.113 [-1.566, 0.997], mean_best_reward: --\n",
      " 12588/100000: episode: 674, duration: 0.014s, episode steps: 15, steps per second: 1099, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.098 [-1.001, 0.411], mean_best_reward: --\n",
      " 12610/100000: episode: 675, duration: 0.020s, episode steps: 22, steps per second: 1077, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.053 [-1.942, 1.171], mean_best_reward: --\n",
      " 12625/100000: episode: 676, duration: 0.015s, episode steps: 15, steps per second: 1017, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.116 [-0.599, 1.073], mean_best_reward: --\n",
      " 12634/100000: episode: 677, duration: 0.014s, episode steps: 9, steps per second: 649, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.141 [-2.376, 1.412], mean_best_reward: --\n",
      " 12650/100000: episode: 678, duration: 0.017s, episode steps: 16, steps per second: 964, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.110 [-2.159, 1.176], mean_best_reward: --\n",
      " 12680/100000: episode: 679, duration: 0.025s, episode steps: 30, steps per second: 1200, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.013 [-1.884, 1.148], mean_best_reward: --\n",
      " 12694/100000: episode: 680, duration: 0.013s, episode steps: 14, steps per second: 1063, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.115 [-1.765, 0.963], mean_best_reward: --\n",
      " 12707/100000: episode: 681, duration: 0.012s, episode steps: 13, steps per second: 1074, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.064 [-2.674, 1.804], mean_best_reward: --\n",
      " 12717/100000: episode: 682, duration: 0.010s, episode steps: 10, steps per second: 973, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-3.094, 2.003], mean_best_reward: --\n",
      " 12727/100000: episode: 683, duration: 0.010s, episode steps: 10, steps per second: 982, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.124 [-1.229, 2.037], mean_best_reward: --\n",
      " 12748/100000: episode: 684, duration: 0.018s, episode steps: 21, steps per second: 1151, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.111 [-0.573, 0.943], mean_best_reward: --\n",
      " 12768/100000: episode: 685, duration: 0.018s, episode steps: 20, steps per second: 1140, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.096 [-1.134, 0.560], mean_best_reward: --\n",
      " 12783/100000: episode: 686, duration: 0.014s, episode steps: 15, steps per second: 1090, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.099 [-1.681, 0.980], mean_best_reward: --\n",
      " 12792/100000: episode: 687, duration: 0.009s, episode steps: 9, steps per second: 992, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.822, 1.747], mean_best_reward: --\n",
      " 12800/100000: episode: 688, duration: 0.009s, episode steps: 8, steps per second: 938, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.141 [-2.176, 1.388], mean_best_reward: --\n",
      " 12831/100000: episode: 689, duration: 0.025s, episode steps: 31, steps per second: 1217, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.072 [-0.476, 0.906], mean_best_reward: --\n",
      " 12840/100000: episode: 690, duration: 0.013s, episode steps: 9, steps per second: 711, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.145 [-2.241, 1.387], mean_best_reward: --\n",
      " 12869/100000: episode: 691, duration: 0.026s, episode steps: 29, steps per second: 1123, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.046 [-1.113, 0.616], mean_best_reward: --\n",
      " 12910/100000: episode: 692, duration: 0.033s, episode steps: 41, steps per second: 1232, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.390 [0.000, 1.000], mean observation: 0.036 [-1.767, 2.857], mean_best_reward: --\n",
      " 12926/100000: episode: 693, duration: 0.015s, episode steps: 16, steps per second: 1099, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.080 [-2.196, 1.377], mean_best_reward: --\n",
      " 12937/100000: episode: 694, duration: 0.011s, episode steps: 11, steps per second: 1037, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.127 [-1.765, 0.990], mean_best_reward: --\n",
      " 12960/100000: episode: 695, duration: 0.020s, episode steps: 23, steps per second: 1153, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.048 [-0.640, 1.003], mean_best_reward: --\n",
      " 12970/100000: episode: 696, duration: 0.010s, episode steps: 10, steps per second: 972, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.146 [-1.145, 1.879], mean_best_reward: --\n",
      " 12991/100000: episode: 697, duration: 0.018s, episode steps: 21, steps per second: 1168, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.047 [-2.343, 1.544], mean_best_reward: --\n",
      " 13002/100000: episode: 698, duration: 0.011s, episode steps: 11, steps per second: 1042, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.105 [-1.789, 2.814], mean_best_reward: --\n",
      " 13015/100000: episode: 699, duration: 0.013s, episode steps: 13, steps per second: 1040, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.086 [-1.820, 1.185], mean_best_reward: --\n",
      " 13031/100000: episode: 700, duration: 0.014s, episode steps: 16, steps per second: 1108, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.112 [-2.169, 1.156], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13043/100000: episode: 701, duration: 0.013s, episode steps: 12, steps per second: 916, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.123 [-2.560, 1.582], mean_best_reward: 69.000000\n",
      " 13061/100000: episode: 702, duration: 0.021s, episode steps: 18, steps per second: 869, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.081 [-2.139, 1.164], mean_best_reward: --\n",
      " 13077/100000: episode: 703, duration: 0.015s, episode steps: 16, steps per second: 1089, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.115 [-1.567, 0.753], mean_best_reward: --\n",
      " 13090/100000: episode: 704, duration: 0.012s, episode steps: 13, steps per second: 1052, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.118 [-1.332, 2.306], mean_best_reward: --\n",
      " 13100/100000: episode: 705, duration: 0.010s, episode steps: 10, steps per second: 1010, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.941, 3.054], mean_best_reward: --\n",
      " 13115/100000: episode: 706, duration: 0.014s, episode steps: 15, steps per second: 1110, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.102 [-0.774, 1.322], mean_best_reward: --\n",
      " 13126/100000: episode: 707, duration: 0.011s, episode steps: 11, steps per second: 1024, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.114 [-1.389, 2.170], mean_best_reward: --\n",
      " 13135/100000: episode: 708, duration: 0.009s, episode steps: 9, steps per second: 966, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.116 [-1.956, 1.201], mean_best_reward: --\n",
      " 13148/100000: episode: 709, duration: 0.012s, episode steps: 13, steps per second: 1043, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.110 [-1.814, 1.029], mean_best_reward: --\n",
      " 13174/100000: episode: 710, duration: 0.022s, episode steps: 26, steps per second: 1181, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.002 [-1.979, 2.618], mean_best_reward: --\n",
      " 13184/100000: episode: 711, duration: 0.010s, episode steps: 10, steps per second: 984, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.132 [-1.714, 0.964], mean_best_reward: --\n",
      " 13196/100000: episode: 712, duration: 0.011s, episode steps: 12, steps per second: 1053, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.114 [-1.536, 2.499], mean_best_reward: --\n",
      " 13229/100000: episode: 713, duration: 0.027s, episode steps: 33, steps per second: 1234, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.576 [0.000, 1.000], mean observation: -0.059 [-1.714, 0.952], mean_best_reward: --\n",
      " 13247/100000: episode: 714, duration: 0.016s, episode steps: 18, steps per second: 1131, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.091 [-1.529, 2.565], mean_best_reward: --\n",
      " 13270/100000: episode: 715, duration: 0.027s, episode steps: 23, steps per second: 853, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.261 [0.000, 1.000], mean observation: 0.035 [-2.191, 3.206], mean_best_reward: --\n",
      " 13279/100000: episode: 716, duration: 0.010s, episode steps: 9, steps per second: 902, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.117 [-1.404, 2.259], mean_best_reward: --\n",
      " 13325/100000: episode: 717, duration: 0.036s, episode steps: 46, steps per second: 1287, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.064 [-0.588, 0.853], mean_best_reward: --\n",
      " 13335/100000: episode: 718, duration: 0.010s, episode steps: 10, steps per second: 961, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.135 [-1.550, 2.567], mean_best_reward: --\n",
      " 13345/100000: episode: 719, duration: 0.010s, episode steps: 10, steps per second: 981, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.115 [-2.592, 1.619], mean_best_reward: --\n",
      " 13356/100000: episode: 720, duration: 0.011s, episode steps: 11, steps per second: 1017, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.147 [-1.717, 2.796], mean_best_reward: --\n",
      " 13368/100000: episode: 721, duration: 0.011s, episode steps: 12, steps per second: 1053, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.079 [-2.186, 1.420], mean_best_reward: --\n",
      " 13385/100000: episode: 722, duration: 0.015s, episode steps: 17, steps per second: 1108, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.176 [0.000, 1.000], mean observation: 0.063 [-2.177, 3.295], mean_best_reward: --\n",
      " 13420/100000: episode: 723, duration: 0.029s, episode steps: 35, steps per second: 1212, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.107 [-0.392, 0.904], mean_best_reward: --\n",
      " 13432/100000: episode: 724, duration: 0.012s, episode steps: 12, steps per second: 1039, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.122 [-1.177, 2.084], mean_best_reward: --\n",
      " 13445/100000: episode: 725, duration: 0.012s, episode steps: 13, steps per second: 1084, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.099 [-1.767, 2.757], mean_best_reward: --\n",
      " 13480/100000: episode: 726, duration: 0.029s, episode steps: 35, steps per second: 1224, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.091 [-1.118, 0.602], mean_best_reward: --\n",
      " 13496/100000: episode: 727, duration: 0.017s, episode steps: 16, steps per second: 936, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.110 [-0.814, 1.648], mean_best_reward: --\n",
      " 13510/100000: episode: 728, duration: 0.016s, episode steps: 14, steps per second: 853, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.078 [-1.339, 1.947], mean_best_reward: --\n",
      " 13532/100000: episode: 729, duration: 0.019s, episode steps: 22, steps per second: 1160, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.052 [-1.990, 3.015], mean_best_reward: --\n",
      " 13542/100000: episode: 730, duration: 0.010s, episode steps: 10, steps per second: 985, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.139 [-1.526, 2.523], mean_best_reward: --\n",
      " 13552/100000: episode: 731, duration: 0.010s, episode steps: 10, steps per second: 982, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.131 [-1.586, 2.558], mean_best_reward: --\n",
      " 13588/100000: episode: 732, duration: 0.030s, episode steps: 36, steps per second: 1209, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.361 [0.000, 1.000], mean observation: -0.120 [-2.324, 2.267], mean_best_reward: --\n",
      " 13604/100000: episode: 733, duration: 0.015s, episode steps: 16, steps per second: 1092, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.082 [-0.824, 1.483], mean_best_reward: --\n",
      " 13614/100000: episode: 734, duration: 0.010s, episode steps: 10, steps per second: 999, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.150 [-1.714, 2.713], mean_best_reward: --\n",
      " 13629/100000: episode: 735, duration: 0.014s, episode steps: 15, steps per second: 1086, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.109 [-1.352, 2.279], mean_best_reward: --\n",
      " 13648/100000: episode: 736, duration: 0.017s, episode steps: 19, steps per second: 1098, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.085 [-1.262, 0.775], mean_best_reward: --\n",
      " 13661/100000: episode: 737, duration: 0.012s, episode steps: 13, steps per second: 1054, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.121 [-1.016, 1.933], mean_best_reward: --\n",
      " 13681/100000: episode: 738, duration: 0.017s, episode steps: 20, steps per second: 1151, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-1.037, 0.396], mean_best_reward: --\n",
      " 13697/100000: episode: 739, duration: 0.015s, episode steps: 16, steps per second: 1095, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.078 [-1.134, 1.936], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13716/100000: episode: 740, duration: 0.021s, episode steps: 19, steps per second: 891, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.263 [0.000, 1.000], mean observation: 0.069 [-1.712, 2.719], mean_best_reward: --\n",
      " 13729/100000: episode: 741, duration: 0.014s, episode steps: 13, steps per second: 955, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.118 [-0.768, 1.418], mean_best_reward: --\n",
      " 13748/100000: episode: 742, duration: 0.016s, episode steps: 19, steps per second: 1155, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.100 [-0.624, 1.006], mean_best_reward: --\n",
      " 13758/100000: episode: 743, duration: 0.010s, episode steps: 10, steps per second: 1003, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.127 [-1.613, 2.606], mean_best_reward: --\n",
      " 13769/100000: episode: 744, duration: 0.011s, episode steps: 11, steps per second: 1033, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.122 [-0.939, 1.792], mean_best_reward: --\n",
      " 13781/100000: episode: 745, duration: 0.012s, episode steps: 12, steps per second: 1020, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.096 [-1.601, 2.451], mean_best_reward: --\n",
      " 13797/100000: episode: 746, duration: 0.015s, episode steps: 16, steps per second: 1092, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.080 [-1.511, 0.936], mean_best_reward: --\n",
      " 13808/100000: episode: 747, duration: 0.011s, episode steps: 11, steps per second: 1009, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.118 [-1.830, 1.135], mean_best_reward: --\n",
      " 13824/100000: episode: 748, duration: 0.014s, episode steps: 16, steps per second: 1113, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.188 [0.000, 1.000], mean observation: 0.085 [-1.927, 2.956], mean_best_reward: --\n",
      " 13841/100000: episode: 749, duration: 0.015s, episode steps: 17, steps per second: 1100, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.088 [-1.716, 2.781], mean_best_reward: --\n",
      " 13859/100000: episode: 750, duration: 0.016s, episode steps: 18, steps per second: 1127, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.096 [-0.807, 1.643], mean_best_reward: --\n",
      " 13888/100000: episode: 751, duration: 0.025s, episode steps: 29, steps per second: 1149, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.109 [-0.803, 1.885], mean_best_reward: 64.000000\n",
      " 13905/100000: episode: 752, duration: 0.016s, episode steps: 17, steps per second: 1080, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.114 [-0.976, 1.904], mean_best_reward: --\n",
      " 13915/100000: episode: 753, duration: 0.010s, episode steps: 10, steps per second: 995, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.129 [-2.001, 1.139], mean_best_reward: --\n",
      " 13929/100000: episode: 754, duration: 0.016s, episode steps: 14, steps per second: 896, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.086 [-2.062, 1.216], mean_best_reward: --\n",
      " 13945/100000: episode: 755, duration: 0.019s, episode steps: 16, steps per second: 853, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.099 [-0.766, 1.258], mean_best_reward: --\n",
      " 13953/100000: episode: 756, duration: 0.009s, episode steps: 8, steps per second: 925, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.158 [-1.991, 1.134], mean_best_reward: --\n",
      " 13971/100000: episode: 757, duration: 0.015s, episode steps: 18, steps per second: 1184, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.073 [-3.017, 1.921], mean_best_reward: --\n",
      " 13985/100000: episode: 758, duration: 0.013s, episode steps: 14, steps per second: 1056, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.100 [-1.453, 0.768], mean_best_reward: --\n",
      " 13996/100000: episode: 759, duration: 0.011s, episode steps: 11, steps per second: 1022, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.152 [-2.350, 1.338], mean_best_reward: --\n",
      " 14010/100000: episode: 760, duration: 0.012s, episode steps: 14, steps per second: 1126, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.091 [-2.561, 1.592], mean_best_reward: --\n",
      " 14022/100000: episode: 761, duration: 0.012s, episode steps: 12, steps per second: 1037, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.100 [-2.163, 1.345], mean_best_reward: --\n",
      " 14033/100000: episode: 762, duration: 0.011s, episode steps: 11, steps per second: 1017, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.115 [-1.808, 2.790], mean_best_reward: --\n",
      " 14048/100000: episode: 763, duration: 0.014s, episode steps: 15, steps per second: 1079, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.096 [-1.727, 2.828], mean_best_reward: --\n",
      " 14056/100000: episode: 764, duration: 0.008s, episode steps: 8, steps per second: 949, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.169 [-1.525, 2.568], mean_best_reward: --\n",
      " 14067/100000: episode: 765, duration: 0.011s, episode steps: 11, steps per second: 1016, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.117 [-1.800, 0.973], mean_best_reward: --\n",
      " 14076/100000: episode: 766, duration: 0.009s, episode steps: 9, steps per second: 950, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.731, 2.848], mean_best_reward: --\n",
      " 14085/100000: episode: 767, duration: 0.010s, episode steps: 9, steps per second: 946, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.140 [-1.726, 0.952], mean_best_reward: --\n",
      " 14101/100000: episode: 768, duration: 0.014s, episode steps: 16, steps per second: 1109, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.054 [-1.981, 1.226], mean_best_reward: --\n",
      " 14116/100000: episode: 769, duration: 0.014s, episode steps: 15, steps per second: 1078, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.090 [-1.323, 2.100], mean_best_reward: --\n",
      " 14126/100000: episode: 770, duration: 0.010s, episode steps: 10, steps per second: 977, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.120 [-2.032, 1.171], mean_best_reward: --\n",
      " 14146/100000: episode: 771, duration: 0.023s, episode steps: 20, steps per second: 855, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-1.215, 0.764], mean_best_reward: --\n",
      " 14162/100000: episode: 772, duration: 0.014s, episode steps: 16, steps per second: 1137, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.085 [-1.325, 2.014], mean_best_reward: --\n",
      " 14171/100000: episode: 773, duration: 0.009s, episode steps: 9, steps per second: 984, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.133 [-0.803, 1.429], mean_best_reward: --\n",
      " 14192/100000: episode: 774, duration: 0.018s, episode steps: 21, steps per second: 1167, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.033 [-2.309, 1.593], mean_best_reward: --\n",
      " 14209/100000: episode: 775, duration: 0.016s, episode steps: 17, steps per second: 1094, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.104 [-1.985, 1.148], mean_best_reward: --\n",
      " 14235/100000: episode: 776, duration: 0.022s, episode steps: 26, steps per second: 1183, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.654 [0.000, 1.000], mean observation: -0.052 [-2.690, 1.620], mean_best_reward: --\n",
      " 14248/100000: episode: 777, duration: 0.013s, episode steps: 13, steps per second: 1039, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.099 [-1.770, 1.011], mean_best_reward: --\n",
      " 14259/100000: episode: 778, duration: 0.011s, episode steps: 11, steps per second: 979, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.117 [-1.954, 1.170], mean_best_reward: --\n",
      " 14272/100000: episode: 779, duration: 0.013s, episode steps: 13, steps per second: 1036, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.092 [-1.156, 0.621], mean_best_reward: --\n",
      " 14288/100000: episode: 780, duration: 0.015s, episode steps: 16, steps per second: 1098, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.079 [-2.542, 1.591], mean_best_reward: --\n",
      " 14300/100000: episode: 781, duration: 0.012s, episode steps: 12, steps per second: 1007, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.103 [-2.530, 1.592], mean_best_reward: --\n",
      " 14313/100000: episode: 782, duration: 0.013s, episode steps: 13, steps per second: 1019, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.087 [-2.201, 1.414], mean_best_reward: --\n",
      " 14322/100000: episode: 783, duration: 0.010s, episode steps: 9, steps per second: 947, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.837, 1.790], mean_best_reward: --\n",
      " 14334/100000: episode: 784, duration: 0.012s, episode steps: 12, steps per second: 1026, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.115 [-2.130, 1.343], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14364/100000: episode: 785, duration: 0.028s, episode steps: 30, steps per second: 1086, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.367 [0.000, 1.000], mean observation: 0.058 [-1.537, 2.512], mean_best_reward: --\n",
      " 14374/100000: episode: 786, duration: 0.013s, episode steps: 10, steps per second: 751, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.963, 3.089], mean_best_reward: --\n",
      " 14383/100000: episode: 787, duration: 0.010s, episode steps: 9, steps per second: 932, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.141 [-2.268, 1.362], mean_best_reward: --\n",
      " 14400/100000: episode: 788, duration: 0.015s, episode steps: 17, steps per second: 1101, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.078 [-2.219, 1.376], mean_best_reward: --\n",
      " 14417/100000: episode: 789, duration: 0.016s, episode steps: 17, steps per second: 1080, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.077 [-2.211, 1.336], mean_best_reward: --\n",
      " 14435/100000: episode: 790, duration: 0.016s, episode steps: 18, steps per second: 1109, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.107 [-1.535, 2.643], mean_best_reward: --\n",
      " 14453/100000: episode: 791, duration: 0.016s, episode steps: 18, steps per second: 1135, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.103 [-1.720, 0.849], mean_best_reward: --\n",
      " 14469/100000: episode: 792, duration: 0.015s, episode steps: 16, steps per second: 1086, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.116 [-2.194, 1.179], mean_best_reward: --\n",
      " 14482/100000: episode: 793, duration: 0.012s, episode steps: 13, steps per second: 1066, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.080 [-2.218, 1.420], mean_best_reward: --\n",
      " 14492/100000: episode: 794, duration: 0.011s, episode steps: 10, steps per second: 941, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.139 [-2.717, 1.754], mean_best_reward: --\n",
      " 14530/100000: episode: 795, duration: 0.030s, episode steps: 38, steps per second: 1248, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.063 [-1.278, 0.441], mean_best_reward: --\n",
      " 14542/100000: episode: 796, duration: 0.012s, episode steps: 12, steps per second: 1030, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.099 [-2.264, 1.408], mean_best_reward: --\n",
      " 14559/100000: episode: 797, duration: 0.015s, episode steps: 17, steps per second: 1098, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.072 [-1.847, 1.012], mean_best_reward: --\n",
      " 14584/100000: episode: 798, duration: 0.028s, episode steps: 25, steps per second: 902, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.680 [0.000, 1.000], mean observation: 0.008 [-2.475, 1.758], mean_best_reward: --\n",
      " 14609/100000: episode: 799, duration: 0.024s, episode steps: 25, steps per second: 1049, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.680 [0.000, 1.000], mean observation: -0.024 [-2.612, 1.756], mean_best_reward: --\n",
      " 14637/100000: episode: 800, duration: 0.024s, episode steps: 28, steps per second: 1172, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-0.940, 0.558], mean_best_reward: --\n",
      " 14653/100000: episode: 801, duration: 0.016s, episode steps: 16, steps per second: 984, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.081 [-1.199, 1.989], mean_best_reward: 46.000000\n",
      " 14667/100000: episode: 802, duration: 0.013s, episode steps: 14, steps per second: 1037, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.103 [-0.588, 1.221], mean_best_reward: --\n",
      " 14678/100000: episode: 803, duration: 0.011s, episode steps: 11, steps per second: 1019, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.149 [-1.344, 2.403], mean_best_reward: --\n",
      " 14696/100000: episode: 804, duration: 0.016s, episode steps: 18, steps per second: 1099, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.063 [-0.828, 1.398], mean_best_reward: --\n",
      " 14710/100000: episode: 805, duration: 0.013s, episode steps: 14, steps per second: 1049, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.087 [-1.234, 2.098], mean_best_reward: --\n",
      " 14721/100000: episode: 806, duration: 0.011s, episode steps: 11, steps per second: 983, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.110 [-2.396, 1.585], mean_best_reward: --\n",
      " 14729/100000: episode: 807, duration: 0.009s, episode steps: 8, steps per second: 907, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.591, 2.544], mean_best_reward: --\n",
      " 14741/100000: episode: 808, duration: 0.011s, episode steps: 12, steps per second: 1044, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.114 [-1.208, 2.084], mean_best_reward: --\n",
      " 14752/100000: episode: 809, duration: 0.011s, episode steps: 11, steps per second: 1003, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.118 [-1.022, 1.735], mean_best_reward: --\n",
      " 14761/100000: episode: 810, duration: 0.009s, episode steps: 9, steps per second: 964, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.143 [-2.140, 1.345], mean_best_reward: --\n",
      " 14776/100000: episode: 811, duration: 0.014s, episode steps: 15, steps per second: 1094, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.103 [-2.462, 1.575], mean_best_reward: --\n",
      " 14786/100000: episode: 812, duration: 0.012s, episode steps: 10, steps per second: 848, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.529, 2.538], mean_best_reward: --\n",
      " 14799/100000: episode: 813, duration: 0.016s, episode steps: 13, steps per second: 805, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.102 [-0.990, 1.725], mean_best_reward: --\n",
      " 14821/100000: episode: 814, duration: 0.019s, episode steps: 22, steps per second: 1142, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.040 [-1.386, 2.049], mean_best_reward: --\n",
      " 14845/100000: episode: 815, duration: 0.021s, episode steps: 24, steps per second: 1166, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.043 [-1.199, 1.937], mean_best_reward: --\n",
      " 14869/100000: episode: 816, duration: 0.021s, episode steps: 24, steps per second: 1160, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.047 [-1.044, 0.628], mean_best_reward: --\n",
      " 14896/100000: episode: 817, duration: 0.023s, episode steps: 27, steps per second: 1149, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.075 [-0.964, 1.897], mean_best_reward: --\n",
      " 14914/100000: episode: 818, duration: 0.016s, episode steps: 18, steps per second: 1107, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-1.232, 0.792], mean_best_reward: --\n",
      " 14925/100000: episode: 819, duration: 0.011s, episode steps: 11, steps per second: 965, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.130 [-1.795, 1.166], mean_best_reward: --\n",
      " 14937/100000: episode: 820, duration: 0.012s, episode steps: 12, steps per second: 1000, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.120 [-1.559, 2.588], mean_best_reward: --\n",
      " 14951/100000: episode: 821, duration: 0.013s, episode steps: 14, steps per second: 1077, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.102 [-0.797, 1.625], mean_best_reward: --\n",
      " 14966/100000: episode: 822, duration: 0.014s, episode steps: 15, steps per second: 1083, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.110 [-0.746, 1.468], mean_best_reward: --\n",
      " 14983/100000: episode: 823, duration: 0.015s, episode steps: 17, steps per second: 1098, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.067 [-1.379, 2.243], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14997/100000: episode: 824, duration: 0.014s, episode steps: 14, steps per second: 1001, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.093 [-1.941, 3.011], mean_best_reward: --\n",
      " 15013/100000: episode: 825, duration: 0.022s, episode steps: 16, steps per second: 731, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.089 [-0.951, 1.759], mean_best_reward: --\n",
      " 15023/100000: episode: 826, duration: 0.010s, episode steps: 10, steps per second: 976, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.114 [-2.485, 1.552], mean_best_reward: --\n",
      " 15038/100000: episode: 827, duration: 0.014s, episode steps: 15, steps per second: 1111, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.109 [-0.984, 1.797], mean_best_reward: --\n",
      " 15051/100000: episode: 828, duration: 0.012s, episode steps: 13, steps per second: 1055, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.120 [-1.373, 2.339], mean_best_reward: --\n",
      " 15067/100000: episode: 829, duration: 0.014s, episode steps: 16, steps per second: 1105, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.083 [-0.802, 1.517], mean_best_reward: --\n",
      " 15076/100000: episode: 830, duration: 0.010s, episode steps: 9, steps per second: 926, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.803, 2.808], mean_best_reward: --\n",
      " 15093/100000: episode: 831, duration: 0.015s, episode steps: 17, steps per second: 1104, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.069 [-1.397, 2.311], mean_best_reward: --\n",
      " 15104/100000: episode: 832, duration: 0.011s, episode steps: 11, steps per second: 1011, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.105 [-0.815, 1.292], mean_best_reward: --\n",
      " 15124/100000: episode: 833, duration: 0.018s, episode steps: 20, steps per second: 1132, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.103 [-1.518, 0.755], mean_best_reward: --\n",
      " 15137/100000: episode: 834, duration: 0.012s, episode steps: 13, steps per second: 1067, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.095 [-1.353, 2.213], mean_best_reward: --\n",
      " 15155/100000: episode: 835, duration: 0.016s, episode steps: 18, steps per second: 1094, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.072 [-1.178, 1.998], mean_best_reward: --\n",
      " 15175/100000: episode: 836, duration: 0.017s, episode steps: 20, steps per second: 1146, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.050 [-1.783, 2.662], mean_best_reward: --\n",
      " 15188/100000: episode: 837, duration: 0.012s, episode steps: 13, steps per second: 1058, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.102 [-2.744, 1.753], mean_best_reward: --\n",
      " 15201/100000: episode: 838, duration: 0.014s, episode steps: 13, steps per second: 909, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.095 [-0.994, 1.706], mean_best_reward: --\n",
      " 15220/100000: episode: 839, duration: 0.024s, episode steps: 19, steps per second: 792, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.045 [-2.288, 1.415], mean_best_reward: --\n",
      " 15232/100000: episode: 840, duration: 0.012s, episode steps: 12, steps per second: 1012, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.119 [-1.158, 1.891], mean_best_reward: --\n",
      " 15248/100000: episode: 841, duration: 0.014s, episode steps: 16, steps per second: 1111, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.075 [-0.931, 1.438], mean_best_reward: --\n",
      " 15274/100000: episode: 842, duration: 0.021s, episode steps: 26, steps per second: 1246, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.070 [-0.564, 1.177], mean_best_reward: --\n",
      " 15286/100000: episode: 843, duration: 0.012s, episode steps: 12, steps per second: 990, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.100 [-1.927, 1.179], mean_best_reward: --\n",
      " 15303/100000: episode: 844, duration: 0.015s, episode steps: 17, steps per second: 1105, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.101 [-1.142, 1.985], mean_best_reward: --\n",
      " 15322/100000: episode: 845, duration: 0.017s, episode steps: 19, steps per second: 1120, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.057 [-1.030, 1.648], mean_best_reward: --\n",
      " 15337/100000: episode: 846, duration: 0.014s, episode steps: 15, steps per second: 1087, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.068 [-2.047, 1.354], mean_best_reward: --\n",
      " 15351/100000: episode: 847, duration: 0.013s, episode steps: 14, steps per second: 1049, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.105 [-1.219, 0.806], mean_best_reward: --\n",
      " 15361/100000: episode: 848, duration: 0.010s, episode steps: 10, steps per second: 979, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.148 [-1.737, 0.979], mean_best_reward: --\n",
      " 15369/100000: episode: 849, duration: 0.009s, episode steps: 8, steps per second: 891, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.587, 2.571], mean_best_reward: --\n",
      " 15380/100000: episode: 850, duration: 0.011s, episode steps: 11, steps per second: 1003, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.109 [-1.419, 2.244], mean_best_reward: --\n",
      " 15395/100000: episode: 851, duration: 0.015s, episode steps: 15, steps per second: 1027, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.102 [-1.473, 0.787], mean_best_reward: 94.500000\n",
      " 15412/100000: episode: 852, duration: 0.021s, episode steps: 17, steps per second: 809, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.099 [-0.650, 1.452], mean_best_reward: --\n",
      " 15423/100000: episode: 853, duration: 0.013s, episode steps: 11, steps per second: 852, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.119 [-0.760, 1.270], mean_best_reward: --\n",
      " 15438/100000: episode: 854, duration: 0.016s, episode steps: 15, steps per second: 962, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.088 [-2.348, 1.421], mean_best_reward: --\n",
      " 15447/100000: episode: 855, duration: 0.010s, episode steps: 9, steps per second: 931, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.835, 1.780], mean_best_reward: --\n",
      " 15491/100000: episode: 856, duration: 0.035s, episode steps: 44, steps per second: 1249, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.112 [-0.600, 1.231], mean_best_reward: --\n",
      " 15510/100000: episode: 857, duration: 0.017s, episode steps: 19, steps per second: 1126, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.056 [-1.349, 2.130], mean_best_reward: --\n",
      " 15559/100000: episode: 858, duration: 0.039s, episode steps: 49, steps per second: 1246, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.068 [-0.570, 0.903], mean_best_reward: --\n",
      " 15573/100000: episode: 859, duration: 0.014s, episode steps: 14, steps per second: 993, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.074 [-1.211, 0.827], mean_best_reward: --\n",
      " 15596/100000: episode: 860, duration: 0.020s, episode steps: 23, steps per second: 1126, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.101 [-0.352, 0.722], mean_best_reward: --\n",
      " 15626/100000: episode: 861, duration: 0.026s, episode steps: 30, steps per second: 1176, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.088 [-0.382, 1.323], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15669/100000: episode: 862, duration: 0.047s, episode steps: 43, steps per second: 920, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.087 [-0.768, 0.944], mean_best_reward: --\n",
      " 15690/100000: episode: 863, duration: 0.019s, episode steps: 21, steps per second: 1094, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.036 [-2.724, 1.770], mean_best_reward: --\n",
      " 15700/100000: episode: 864, duration: 0.010s, episode steps: 10, steps per second: 977, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.103 [-2.255, 1.415], mean_best_reward: --\n",
      " 15751/100000: episode: 865, duration: 0.041s, episode steps: 51, steps per second: 1230, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.082 [-0.460, 0.835], mean_best_reward: --\n",
      " 15785/100000: episode: 866, duration: 0.028s, episode steps: 34, steps per second: 1195, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.086 [-0.797, 1.199], mean_best_reward: --\n",
      " 15794/100000: episode: 867, duration: 0.010s, episode steps: 9, steps per second: 926, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.125 [-1.992, 1.220], mean_best_reward: --\n",
      " 15895/100000: episode: 868, duration: 0.078s, episode steps: 101, steps per second: 1287, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.112 [-1.296, 1.614], mean_best_reward: --\n",
      " 15948/100000: episode: 869, duration: 0.049s, episode steps: 53, steps per second: 1076, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.047 [-0.554, 0.845], mean_best_reward: --\n",
      " 15966/100000: episode: 870, duration: 0.017s, episode steps: 18, steps per second: 1074, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.243, 0.762], mean_best_reward: --\n",
      " 15987/100000: episode: 871, duration: 0.019s, episode steps: 21, steps per second: 1120, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.093 [-0.965, 1.849], mean_best_reward: --\n",
      " 16012/100000: episode: 872, duration: 0.021s, episode steps: 25, steps per second: 1176, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.045 [-1.170, 0.734], mean_best_reward: --\n",
      " 16033/100000: episode: 873, duration: 0.019s, episode steps: 21, steps per second: 1129, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.060 [-0.819, 1.360], mean_best_reward: --\n",
      " 16066/100000: episode: 874, duration: 0.027s, episode steps: 33, steps per second: 1214, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.125 [-0.557, 0.975], mean_best_reward: --\n",
      " 16077/100000: episode: 875, duration: 0.011s, episode steps: 11, steps per second: 1039, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.136 [-1.755, 0.968], mean_best_reward: --\n",
      " 16086/100000: episode: 876, duration: 0.010s, episode steps: 9, steps per second: 937, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.827, 1.733], mean_best_reward: --\n",
      " 16108/100000: episode: 877, duration: 0.019s, episode steps: 22, steps per second: 1160, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.080 [-1.905, 1.030], mean_best_reward: --\n",
      " 16152/100000: episode: 878, duration: 0.035s, episode steps: 44, steps per second: 1248, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.066 [-1.330, 0.453], mean_best_reward: --\n",
      " 16164/100000: episode: 879, duration: 0.012s, episode steps: 12, steps per second: 1042, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.106 [-1.920, 1.184], mean_best_reward: --\n",
      " 16172/100000: episode: 880, duration: 0.010s, episode steps: 8, steps per second: 800, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.140 [-2.166, 1.358], mean_best_reward: --\n",
      " 16191/100000: episode: 881, duration: 0.023s, episode steps: 19, steps per second: 835, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.110 [-0.370, 1.145], mean_best_reward: --\n",
      " 16200/100000: episode: 882, duration: 0.009s, episode steps: 9, steps per second: 953, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.847, 1.801], mean_best_reward: --\n",
      " 16209/100000: episode: 883, duration: 0.010s, episode steps: 9, steps per second: 943, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.720, 2.785], mean_best_reward: --\n",
      " 16219/100000: episode: 884, duration: 0.010s, episode steps: 10, steps per second: 983, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-3.065, 1.930], mean_best_reward: --\n",
      " 16250/100000: episode: 885, duration: 0.025s, episode steps: 31, steps per second: 1234, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.088 [-0.654, 1.498], mean_best_reward: --\n",
      " 16281/100000: episode: 886, duration: 0.025s, episode steps: 31, steps per second: 1227, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.121 [-0.742, 1.178], mean_best_reward: --\n",
      " 16297/100000: episode: 887, duration: 0.015s, episode steps: 16, steps per second: 1073, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.106 [-2.138, 1.160], mean_best_reward: --\n",
      " 16318/100000: episode: 888, duration: 0.020s, episode steps: 21, steps per second: 1035, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.106 [-1.304, 0.573], mean_best_reward: --\n",
      " 16340/100000: episode: 889, duration: 0.024s, episode steps: 22, steps per second: 925, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: -0.055 [-2.526, 1.720], mean_best_reward: --\n",
      " 16351/100000: episode: 890, duration: 0.012s, episode steps: 11, steps per second: 886, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.093 [-1.515, 1.002], mean_best_reward: --\n",
      " 16366/100000: episode: 891, duration: 0.021s, episode steps: 15, steps per second: 705, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.083 [-2.306, 1.384], mean_best_reward: --\n",
      " 16382/100000: episode: 892, duration: 0.020s, episode steps: 16, steps per second: 787, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.069 [-0.996, 1.424], mean_best_reward: --\n",
      " 16394/100000: episode: 893, duration: 0.012s, episode steps: 12, steps per second: 1019, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.106 [-1.723, 0.959], mean_best_reward: --\n",
      " 16416/100000: episode: 894, duration: 0.020s, episode steps: 22, steps per second: 1107, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.221, 0.809], mean_best_reward: --\n",
      " 16429/100000: episode: 895, duration: 0.013s, episode steps: 13, steps per second: 1034, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.088 [-1.872, 1.169], mean_best_reward: --\n",
      " 16440/100000: episode: 896, duration: 0.011s, episode steps: 11, steps per second: 977, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.140 [-1.694, 0.959], mean_best_reward: --\n",
      " 16488/100000: episode: 897, duration: 0.039s, episode steps: 48, steps per second: 1241, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.025 [-1.370, 0.967], mean_best_reward: --\n",
      " 16500/100000: episode: 898, duration: 0.012s, episode steps: 12, steps per second: 1029, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.119 [-2.544, 1.550], mean_best_reward: --\n",
      " 16537/100000: episode: 899, duration: 0.030s, episode steps: 37, steps per second: 1234, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.111 [-1.221, 0.416], mean_best_reward: --\n",
      " 16545/100000: episode: 900, duration: 0.009s, episode steps: 8, steps per second: 931, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.147 [-2.187, 1.337], mean_best_reward: --\n",
      " 16571/100000: episode: 901, duration: 0.023s, episode steps: 26, steps per second: 1149, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.654 [0.000, 1.000], mean observation: -0.030 [-2.534, 1.762], mean_best_reward: 50.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16596/100000: episode: 902, duration: 0.027s, episode steps: 25, steps per second: 920, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.109 [-1.012, 0.606], mean_best_reward: --\n",
      " 16621/100000: episode: 903, duration: 0.024s, episode steps: 25, steps per second: 1045, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.084 [-0.575, 1.304], mean_best_reward: --\n",
      " 16633/100000: episode: 904, duration: 0.012s, episode steps: 12, steps per second: 1026, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.105 [-1.954, 3.029], mean_best_reward: --\n",
      " 16651/100000: episode: 905, duration: 0.017s, episode steps: 18, steps per second: 1076, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.079 [-0.749, 1.420], mean_best_reward: --\n",
      " 16662/100000: episode: 906, duration: 0.011s, episode steps: 11, steps per second: 997, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.112 [-1.191, 1.821], mean_best_reward: --\n",
      " 16671/100000: episode: 907, duration: 0.009s, episode steps: 9, steps per second: 956, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.716, 2.792], mean_best_reward: --\n",
      " 16701/100000: episode: 908, duration: 0.027s, episode steps: 30, steps per second: 1124, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.053 [-1.625, 0.932], mean_best_reward: --\n",
      " 16714/100000: episode: 909, duration: 0.012s, episode steps: 13, steps per second: 1052, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.068 [-2.206, 1.414], mean_best_reward: --\n",
      " 16732/100000: episode: 910, duration: 0.017s, episode steps: 18, steps per second: 1050, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.084 [-1.387, 0.777], mean_best_reward: --\n",
      " 16743/100000: episode: 911, duration: 0.011s, episode steps: 11, steps per second: 1015, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.094 [-1.181, 1.912], mean_best_reward: --\n",
      " 16760/100000: episode: 912, duration: 0.015s, episode steps: 17, steps per second: 1111, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.084 [-1.494, 0.829], mean_best_reward: --\n",
      " 16790/100000: episode: 913, duration: 0.026s, episode steps: 30, steps per second: 1164, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.125 [-0.796, 0.396], mean_best_reward: --\n",
      " 16812/100000: episode: 914, duration: 0.023s, episode steps: 22, steps per second: 948, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.079 [-1.138, 2.091], mean_best_reward: --\n",
      " 16842/100000: episode: 915, duration: 0.030s, episode steps: 30, steps per second: 1002, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.072 [-1.305, 0.465], mean_best_reward: --\n",
      " 16855/100000: episode: 916, duration: 0.012s, episode steps: 13, steps per second: 1041, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.114 [-1.776, 2.813], mean_best_reward: --\n",
      " 16869/100000: episode: 917, duration: 0.014s, episode steps: 14, steps per second: 1036, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.095 [-1.523, 2.504], mean_best_reward: --\n",
      " 16884/100000: episode: 918, duration: 0.015s, episode steps: 15, steps per second: 992, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.073 [-1.800, 2.699], mean_best_reward: --\n",
      " 16895/100000: episode: 919, duration: 0.016s, episode steps: 11, steps per second: 673, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.127 [-1.744, 2.832], mean_best_reward: --\n",
      " 16948/100000: episode: 920, duration: 0.042s, episode steps: 53, steps per second: 1254, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.077 [-1.205, 0.691], mean_best_reward: --\n",
      " 16963/100000: episode: 921, duration: 0.015s, episode steps: 15, steps per second: 1028, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.069 [-1.573, 2.281], mean_best_reward: --\n",
      " 16979/100000: episode: 922, duration: 0.015s, episode steps: 16, steps per second: 1067, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.087 [-1.347, 2.228], mean_best_reward: --\n",
      " 16989/100000: episode: 923, duration: 0.010s, episode steps: 10, steps per second: 988, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.135 [-2.513, 1.560], mean_best_reward: --\n",
      " 17001/100000: episode: 924, duration: 0.012s, episode steps: 12, steps per second: 1017, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.117 [-2.045, 1.161], mean_best_reward: --\n",
      " 17020/100000: episode: 925, duration: 0.019s, episode steps: 19, steps per second: 1010, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.037 [-1.414, 2.179], mean_best_reward: --\n",
      " 17031/100000: episode: 926, duration: 0.014s, episode steps: 11, steps per second: 779, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.132 [-1.376, 2.297], mean_best_reward: --\n",
      " 17041/100000: episode: 927, duration: 0.010s, episode steps: 10, steps per second: 991, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.942, 3.051], mean_best_reward: --\n",
      " 17050/100000: episode: 928, duration: 0.009s, episode steps: 9, steps per second: 952, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.136 [-1.398, 2.323], mean_best_reward: --\n",
      " 17062/100000: episode: 929, duration: 0.012s, episode steps: 12, steps per second: 1013, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.104 [-1.185, 2.055], mean_best_reward: --\n",
      " 17078/100000: episode: 930, duration: 0.015s, episode steps: 16, steps per second: 1098, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.089 [-1.611, 2.601], mean_best_reward: --\n",
      " 17108/100000: episode: 931, duration: 0.025s, episode steps: 30, steps per second: 1207, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.137 [-0.988, 2.193], mean_best_reward: --\n",
      " 17121/100000: episode: 932, duration: 0.012s, episode steps: 13, steps per second: 1045, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.115 [-1.330, 2.317], mean_best_reward: --\n",
      " 17131/100000: episode: 933, duration: 0.010s, episode steps: 10, steps per second: 976, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.122 [-1.389, 2.141], mean_best_reward: --\n",
      " 17140/100000: episode: 934, duration: 0.010s, episode steps: 9, steps per second: 929, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.135 [-1.159, 1.969], mean_best_reward: --\n",
      " 17152/100000: episode: 935, duration: 0.012s, episode steps: 12, steps per second: 997, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.119 [-1.531, 2.568], mean_best_reward: --\n",
      " 17163/100000: episode: 936, duration: 0.011s, episode steps: 11, steps per second: 1018, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.108 [-1.354, 2.222], mean_best_reward: --\n",
      " 17175/100000: episode: 937, duration: 0.012s, episode steps: 12, steps per second: 1037, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.098 [-1.213, 1.989], mean_best_reward: --\n",
      " 17185/100000: episode: 938, duration: 0.010s, episode steps: 10, steps per second: 1002, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.130 [-1.804, 2.750], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17223/100000: episode: 939, duration: 0.032s, episode steps: 38, steps per second: 1195, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.009 [-1.940, 2.889], mean_best_reward: --\n",
      " 17239/100000: episode: 940, duration: 0.021s, episode steps: 16, steps per second: 759, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.070 [-1.191, 1.829], mean_best_reward: --\n",
      " 17249/100000: episode: 941, duration: 0.010s, episode steps: 10, steps per second: 989, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.155 [-2.594, 1.526], mean_best_reward: --\n",
      " 17273/100000: episode: 942, duration: 0.020s, episode steps: 24, steps per second: 1187, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.086 [-1.179, 0.594], mean_best_reward: --\n",
      " 17285/100000: episode: 943, duration: 0.012s, episode steps: 12, steps per second: 1016, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.116 [-0.824, 1.543], mean_best_reward: --\n",
      " 17305/100000: episode: 944, duration: 0.018s, episode steps: 20, steps per second: 1117, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.057 [-0.824, 1.587], mean_best_reward: --\n",
      " 17316/100000: episode: 945, duration: 0.011s, episode steps: 11, steps per second: 1007, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.114 [-0.980, 1.585], mean_best_reward: --\n",
      " 17325/100000: episode: 946, duration: 0.009s, episode steps: 9, steps per second: 947, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.144 [-1.200, 1.898], mean_best_reward: --\n",
      " 17342/100000: episode: 947, duration: 0.015s, episode steps: 17, steps per second: 1114, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.073 [-1.281, 0.799], mean_best_reward: --\n",
      " 17360/100000: episode: 948, duration: 0.016s, episode steps: 18, steps per second: 1128, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.060 [-0.972, 1.480], mean_best_reward: --\n",
      " 17370/100000: episode: 949, duration: 0.011s, episode steps: 10, steps per second: 944, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.129 [-1.561, 2.553], mean_best_reward: --\n",
      " 17381/100000: episode: 950, duration: 0.011s, episode steps: 11, steps per second: 1023, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.111 [-1.759, 2.732], mean_best_reward: --\n",
      " 17392/100000: episode: 951, duration: 0.012s, episode steps: 11, steps per second: 942, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.115 [-1.597, 2.440], mean_best_reward: 103.500000\n",
      " 17434/100000: episode: 952, duration: 0.041s, episode steps: 42, steps per second: 1023, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.796, 0.989], mean_best_reward: --\n",
      " 17442/100000: episode: 953, duration: 0.010s, episode steps: 8, steps per second: 774, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.149 [-2.002, 1.145], mean_best_reward: --\n",
      " 17459/100000: episode: 954, duration: 0.018s, episode steps: 17, steps per second: 930, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.078 [-1.524, 2.354], mean_best_reward: --\n",
      " 17472/100000: episode: 955, duration: 0.013s, episode steps: 13, steps per second: 1023, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.101 [-1.361, 2.313], mean_best_reward: --\n",
      " 17484/100000: episode: 956, duration: 0.012s, episode steps: 12, steps per second: 1042, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.086 [-1.386, 1.955], mean_best_reward: --\n",
      " 17502/100000: episode: 957, duration: 0.016s, episode steps: 18, steps per second: 1129, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.080 [-0.817, 1.439], mean_best_reward: --\n",
      " 17571/100000: episode: 958, duration: 0.053s, episode steps: 69, steps per second: 1291, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.623 [0.000, 1.000], mean observation: 0.232 [-2.888, 3.201], mean_best_reward: --\n",
      " 17631/100000: episode: 959, duration: 0.047s, episode steps: 60, steps per second: 1282, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.199 [-1.055, 0.579], mean_best_reward: --\n",
      " 17641/100000: episode: 960, duration: 0.010s, episode steps: 10, steps per second: 972, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.594, 2.527], mean_best_reward: --\n",
      " 17656/100000: episode: 961, duration: 0.015s, episode steps: 15, steps per second: 1013, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.085 [-0.631, 1.117], mean_best_reward: --\n",
      " 17668/100000: episode: 962, duration: 0.014s, episode steps: 12, steps per second: 831, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.125 [-1.241, 0.596], mean_best_reward: --\n",
      " 17682/100000: episode: 963, duration: 0.014s, episode steps: 14, steps per second: 968, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.071 [-1.328, 0.830], mean_best_reward: --\n",
      " 17702/100000: episode: 964, duration: 0.019s, episode steps: 20, steps per second: 1062, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.078 [-1.517, 0.773], mean_best_reward: --\n",
      " 17742/100000: episode: 965, duration: 0.034s, episode steps: 40, steps per second: 1175, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.030 [-1.177, 0.812], mean_best_reward: --\n",
      " 17762/100000: episode: 966, duration: 0.018s, episode steps: 20, steps per second: 1130, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.056 [-0.829, 1.604], mean_best_reward: --\n",
      " 17773/100000: episode: 967, duration: 0.011s, episode steps: 11, steps per second: 990, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.120 [-2.730, 1.723], mean_best_reward: --\n",
      " 17801/100000: episode: 968, duration: 0.024s, episode steps: 28, steps per second: 1189, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.040 [-0.881, 1.303], mean_best_reward: --\n",
      " 17824/100000: episode: 969, duration: 0.020s, episode steps: 23, steps per second: 1162, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.075 [-0.435, 1.076], mean_best_reward: --\n",
      " 17858/100000: episode: 970, duration: 0.028s, episode steps: 34, steps per second: 1214, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.473, 0.951], mean_best_reward: --\n",
      " 17868/100000: episode: 971, duration: 0.010s, episode steps: 10, steps per second: 1006, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-3.042, 1.968], mean_best_reward: --\n",
      " 17879/100000: episode: 972, duration: 0.011s, episode steps: 11, steps per second: 993, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.114 [-2.206, 1.365], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17888/100000: episode: 973, duration: 0.013s, episode steps: 9, steps per second: 704, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.762, 2.841], mean_best_reward: --\n",
      " 17906/100000: episode: 974, duration: 0.023s, episode steps: 18, steps per second: 774, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.079 [-0.830, 1.252], mean_best_reward: --\n",
      " 17920/100000: episode: 975, duration: 0.013s, episode steps: 14, steps per second: 1043, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.105 [-0.767, 1.185], mean_best_reward: --\n",
      " 17935/100000: episode: 976, duration: 0.014s, episode steps: 15, steps per second: 1104, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.086 [-1.393, 2.304], mean_best_reward: --\n",
      " 17947/100000: episode: 977, duration: 0.011s, episode steps: 12, steps per second: 1048, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.132 [-3.070, 1.947], mean_best_reward: --\n",
      " 17960/100000: episode: 978, duration: 0.012s, episode steps: 13, steps per second: 1041, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.097 [-1.765, 2.739], mean_best_reward: --\n",
      " 17968/100000: episode: 979, duration: 0.009s, episode steps: 8, steps per second: 871, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.593, 2.544], mean_best_reward: --\n",
      " 17990/100000: episode: 980, duration: 0.019s, episode steps: 22, steps per second: 1145, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.064 [-0.767, 1.222], mean_best_reward: --\n",
      " 18009/100000: episode: 981, duration: 0.018s, episode steps: 19, steps per second: 1085, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.051 [-0.786, 1.322], mean_best_reward: --\n",
      " 18032/100000: episode: 982, duration: 0.020s, episode steps: 23, steps per second: 1142, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.109 [-0.603, 1.283], mean_best_reward: --\n",
      " 18070/100000: episode: 983, duration: 0.031s, episode steps: 38, steps per second: 1238, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.409, 1.153], mean_best_reward: --\n",
      " 18122/100000: episode: 984, duration: 0.050s, episode steps: 52, steps per second: 1039, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.066 [-1.622, 0.645], mean_best_reward: --\n",
      " 18137/100000: episode: 985, duration: 0.014s, episode steps: 15, steps per second: 1091, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.096 [-0.561, 1.017], mean_best_reward: --\n",
      " 18177/100000: episode: 986, duration: 0.031s, episode steps: 40, steps per second: 1283, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.893, 1.209], mean_best_reward: --\n",
      " 18238/100000: episode: 987, duration: 0.048s, episode steps: 61, steps per second: 1268, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.056 [-0.417, 0.892], mean_best_reward: --\n",
      " 18255/100000: episode: 988, duration: 0.015s, episode steps: 17, steps per second: 1099, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.090 [-0.786, 1.183], mean_best_reward: --\n",
      " 18273/100000: episode: 989, duration: 0.016s, episode steps: 18, steps per second: 1107, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.125 [-0.942, 1.880], mean_best_reward: --\n",
      " 18305/100000: episode: 990, duration: 0.026s, episode steps: 32, steps per second: 1213, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.033 [-0.602, 0.867], mean_best_reward: --\n",
      " 18314/100000: episode: 991, duration: 0.010s, episode steps: 9, steps per second: 924, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.788, 1.755], mean_best_reward: --\n",
      " 18325/100000: episode: 992, duration: 0.011s, episode steps: 11, steps per second: 992, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.127 [-2.258, 1.337], mean_best_reward: --\n",
      " 18361/100000: episode: 993, duration: 0.033s, episode steps: 36, steps per second: 1099, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.104 [-0.635, 1.449], mean_best_reward: --\n",
      " 18370/100000: episode: 994, duration: 0.014s, episode steps: 9, steps per second: 624, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.147 [-1.126, 1.889], mean_best_reward: --\n",
      " 18414/100000: episode: 995, duration: 0.035s, episode steps: 44, steps per second: 1271, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.614 [0.000, 1.000], mean observation: 0.023 [-2.429, 1.931], mean_best_reward: --\n",
      " 18430/100000: episode: 996, duration: 0.015s, episode steps: 16, steps per second: 1093, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.756, 1.256], mean_best_reward: --\n",
      " 18454/100000: episode: 997, duration: 0.020s, episode steps: 24, steps per second: 1179, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.072 [-0.988, 1.791], mean_best_reward: --\n",
      " 18487/100000: episode: 998, duration: 0.027s, episode steps: 33, steps per second: 1204, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.092 [-0.899, 0.404], mean_best_reward: --\n",
      " 18540/100000: episode: 999, duration: 0.043s, episode steps: 53, steps per second: 1245, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.068 [-0.510, 1.093], mean_best_reward: --\n",
      " 18559/100000: episode: 1000, duration: 0.017s, episode steps: 19, steps per second: 1129, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.081 [-1.446, 0.836], mean_best_reward: --\n",
      " 18580/100000: episode: 1001, duration: 0.020s, episode steps: 21, steps per second: 1051, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.116 [-0.397, 1.239], mean_best_reward: 75.000000\n",
      " 18592/100000: episode: 1002, duration: 0.014s, episode steps: 12, steps per second: 835, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.103 [-1.574, 2.465], mean_best_reward: --\n",
      " 18602/100000: episode: 1003, duration: 0.013s, episode steps: 10, steps per second: 747, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.618, 2.598], mean_best_reward: --\n",
      " 18614/100000: episode: 1004, duration: 0.012s, episode steps: 12, steps per second: 1011, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.122 [-2.491, 1.523], mean_best_reward: --\n",
      " 18623/100000: episode: 1005, duration: 0.009s, episode steps: 9, steps per second: 966, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.137 [-1.172, 1.965], mean_best_reward: --\n",
      " 18647/100000: episode: 1006, duration: 0.021s, episode steps: 24, steps per second: 1154, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.041 [-1.592, 2.530], mean_best_reward: --\n",
      " 18660/100000: episode: 1007, duration: 0.012s, episode steps: 13, steps per second: 1049, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.081 [-1.373, 2.209], mean_best_reward: --\n",
      " 18670/100000: episode: 1008, duration: 0.010s, episode steps: 10, steps per second: 977, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.147 [-1.531, 2.552], mean_best_reward: --\n",
      " 18683/100000: episode: 1009, duration: 0.013s, episode steps: 13, steps per second: 1031, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.085 [-1.008, 1.534], mean_best_reward: --\n",
      " 18695/100000: episode: 1010, duration: 0.011s, episode steps: 12, steps per second: 1054, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.099 [-1.999, 1.199], mean_best_reward: --\n",
      " 18721/100000: episode: 1011, duration: 0.022s, episode steps: 26, steps per second: 1163, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.068 [-1.107, 0.640], mean_best_reward: --\n",
      " 18732/100000: episode: 1012, duration: 0.011s, episode steps: 11, steps per second: 978, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.144 [-2.795, 1.740], mean_best_reward: --\n",
      " 18744/100000: episode: 1013, duration: 0.012s, episode steps: 12, steps per second: 1017, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.119 [-2.187, 1.337], mean_best_reward: --\n",
      " 18759/100000: episode: 1014, duration: 0.014s, episode steps: 15, steps per second: 1075, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.079 [-1.837, 1.202], mean_best_reward: --\n",
      " 18767/100000: episode: 1015, duration: 0.009s, episode steps: 8, steps per second: 920, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.595, 1.595], mean_best_reward: --\n",
      " 18786/100000: episode: 1016, duration: 0.018s, episode steps: 19, steps per second: 1072, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.055 [-2.594, 1.741], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18812/100000: episode: 1017, duration: 0.033s, episode steps: 26, steps per second: 792, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.046 [-1.493, 0.813], mean_best_reward: --\n",
      " 18823/100000: episode: 1018, duration: 0.011s, episode steps: 11, steps per second: 983, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.115 [-1.813, 1.160], mean_best_reward: --\n",
      " 18856/100000: episode: 1019, duration: 0.027s, episode steps: 33, steps per second: 1214, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.125 [-0.617, 1.905], mean_best_reward: --\n",
      " 18873/100000: episode: 1020, duration: 0.015s, episode steps: 17, steps per second: 1097, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.062 [-2.290, 1.406], mean_best_reward: --\n",
      " 18889/100000: episode: 1021, duration: 0.014s, episode steps: 16, steps per second: 1110, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.102 [-2.072, 1.160], mean_best_reward: --\n",
      " 18917/100000: episode: 1022, duration: 0.024s, episode steps: 28, steps per second: 1184, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.607 [0.000, 1.000], mean observation: -0.073 [-2.112, 1.150], mean_best_reward: --\n",
      " 18933/100000: episode: 1023, duration: 0.015s, episode steps: 16, steps per second: 1089, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.078 [-1.994, 1.182], mean_best_reward: --\n",
      " 18945/100000: episode: 1024, duration: 0.012s, episode steps: 12, steps per second: 1016, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.093 [-1.551, 0.991], mean_best_reward: --\n",
      " 18958/100000: episode: 1025, duration: 0.012s, episode steps: 13, steps per second: 1041, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.094 [-1.761, 2.812], mean_best_reward: --\n",
      " 19009/100000: episode: 1026, duration: 0.041s, episode steps: 51, steps per second: 1252, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.137 [-0.948, 1.473], mean_best_reward: --\n",
      " 19037/100000: episode: 1027, duration: 0.024s, episode steps: 28, steps per second: 1176, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.084 [-0.569, 1.350], mean_best_reward: --\n",
      " 19092/100000: episode: 1028, duration: 0.053s, episode steps: 55, steps per second: 1047, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.098 [-1.396, 0.517], mean_best_reward: --\n",
      " 19103/100000: episode: 1029, duration: 0.011s, episode steps: 11, steps per second: 1004, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.105 [-1.536, 2.400], mean_best_reward: --\n",
      " 19115/100000: episode: 1030, duration: 0.012s, episode steps: 12, steps per second: 995, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.118 [-2.561, 1.614], mean_best_reward: --\n",
      " 19130/100000: episode: 1031, duration: 0.014s, episode steps: 15, steps per second: 1082, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.103 [-2.853, 1.759], mean_best_reward: --\n",
      " 19143/100000: episode: 1032, duration: 0.012s, episode steps: 13, steps per second: 1046, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.097 [-1.896, 1.205], mean_best_reward: --\n",
      " 19159/100000: episode: 1033, duration: 0.014s, episode steps: 16, steps per second: 1107, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.092 [-1.768, 0.968], mean_best_reward: --\n",
      " 19170/100000: episode: 1034, duration: 0.011s, episode steps: 11, steps per second: 984, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.143 [-1.351, 2.401], mean_best_reward: --\n",
      " 19180/100000: episode: 1035, duration: 0.010s, episode steps: 10, steps per second: 992, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.058, 1.918], mean_best_reward: --\n",
      " 19199/100000: episode: 1036, duration: 0.017s, episode steps: 19, steps per second: 1149, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.078 [-1.313, 0.759], mean_best_reward: --\n",
      " 19222/100000: episode: 1037, duration: 0.020s, episode steps: 23, steps per second: 1142, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.696 [0.000, 1.000], mean observation: -0.057 [-2.676, 1.742], mean_best_reward: --\n",
      " 19237/100000: episode: 1038, duration: 0.014s, episode steps: 15, steps per second: 1061, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.080 [-1.777, 1.001], mean_best_reward: --\n",
      " 19255/100000: episode: 1039, duration: 0.020s, episode steps: 18, steps per second: 883, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.081 [-1.958, 1.167], mean_best_reward: --\n",
      " 19265/100000: episode: 1040, duration: 0.012s, episode steps: 10, steps per second: 841, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.122 [-2.086, 1.212], mean_best_reward: --\n",
      " 19275/100000: episode: 1041, duration: 0.010s, episode steps: 10, steps per second: 998, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.109 [-2.497, 1.594], mean_best_reward: --\n",
      " 19286/100000: episode: 1042, duration: 0.011s, episode steps: 11, steps per second: 1011, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.115 [-2.797, 1.788], mean_best_reward: --\n",
      " 19329/100000: episode: 1043, duration: 0.033s, episode steps: 43, steps per second: 1298, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.077 [-1.805, 1.870], mean_best_reward: --\n",
      " 19342/100000: episode: 1044, duration: 0.012s, episode steps: 13, steps per second: 1052, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.077 [-1.434, 1.004], mean_best_reward: --\n",
      " 19361/100000: episode: 1045, duration: 0.017s, episode steps: 19, steps per second: 1136, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.119 [-0.609, 0.985], mean_best_reward: --\n",
      " 19371/100000: episode: 1046, duration: 0.010s, episode steps: 10, steps per second: 978, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.115 [-1.011, 1.693], mean_best_reward: --\n",
      " 19384/100000: episode: 1047, duration: 0.012s, episode steps: 13, steps per second: 1052, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.134 [-1.261, 0.757], mean_best_reward: --\n",
      " 19407/100000: episode: 1048, duration: 0.020s, episode steps: 23, steps per second: 1144, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.017 [-2.284, 1.588], mean_best_reward: --\n",
      " 19418/100000: episode: 1049, duration: 0.011s, episode steps: 11, steps per second: 995, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.130 [-2.468, 1.526], mean_best_reward: --\n",
      " 19444/100000: episode: 1050, duration: 0.022s, episode steps: 26, steps per second: 1171, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: -0.002 [-1.341, 1.823], mean_best_reward: --\n",
      " 19457/100000: episode: 1051, duration: 0.013s, episode steps: 13, steps per second: 1011, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.082 [-1.359, 2.012], mean_best_reward: 42.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19471/100000: episode: 1052, duration: 0.015s, episode steps: 14, steps per second: 926, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.106 [-1.564, 2.608], mean_best_reward: --\n",
      " 19481/100000: episode: 1053, duration: 0.015s, episode steps: 10, steps per second: 679, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.526, 2.516], mean_best_reward: --\n",
      " 19495/100000: episode: 1054, duration: 0.013s, episode steps: 14, steps per second: 1057, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.095 [-1.566, 2.457], mean_best_reward: --\n",
      " 19508/100000: episode: 1055, duration: 0.013s, episode steps: 13, steps per second: 1016, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.081 [-1.220, 1.910], mean_best_reward: --\n",
      " 19521/100000: episode: 1056, duration: 0.012s, episode steps: 13, steps per second: 1058, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.095 [-1.356, 2.214], mean_best_reward: --\n",
      " 19537/100000: episode: 1057, duration: 0.014s, episode steps: 16, steps per second: 1109, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.127 [-0.750, 1.259], mean_best_reward: --\n",
      " 19548/100000: episode: 1058, duration: 0.011s, episode steps: 11, steps per second: 1032, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.158 [-1.714, 2.910], mean_best_reward: --\n",
      " 19568/100000: episode: 1059, duration: 0.018s, episode steps: 20, steps per second: 1121, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.033 [-1.569, 2.234], mean_best_reward: --\n",
      " 19579/100000: episode: 1060, duration: 0.011s, episode steps: 11, steps per second: 986, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.114 [-0.966, 1.722], mean_best_reward: --\n",
      " 19599/100000: episode: 1061, duration: 0.018s, episode steps: 20, steps per second: 1135, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.089 [-0.772, 1.568], mean_best_reward: --\n",
      " 19614/100000: episode: 1062, duration: 0.014s, episode steps: 15, steps per second: 1083, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.083 [-0.978, 1.489], mean_best_reward: --\n",
      " 19628/100000: episode: 1063, duration: 0.013s, episode steps: 14, steps per second: 1078, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.080 [-1.018, 1.480], mean_best_reward: --\n",
      " 19645/100000: episode: 1064, duration: 0.015s, episode steps: 17, steps per second: 1104, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.053 [-1.802, 2.663], mean_best_reward: --\n",
      " 19680/100000: episode: 1065, duration: 0.030s, episode steps: 35, steps per second: 1155, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.132 [-0.939, 0.544], mean_best_reward: --\n",
      " 19696/100000: episode: 1066, duration: 0.019s, episode steps: 16, steps per second: 831, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.089 [-1.593, 2.648], mean_best_reward: --\n",
      " 19710/100000: episode: 1067, duration: 0.013s, episode steps: 14, steps per second: 1085, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.088 [-1.950, 2.993], mean_best_reward: --\n",
      " 19728/100000: episode: 1068, duration: 0.016s, episode steps: 18, steps per second: 1102, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.066 [-1.577, 2.464], mean_best_reward: --\n",
      " 19738/100000: episode: 1069, duration: 0.011s, episode steps: 10, steps per second: 949, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.151 [-1.384, 2.290], mean_best_reward: --\n",
      " 19748/100000: episode: 1070, duration: 0.010s, episode steps: 10, steps per second: 966, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.123 [-1.604, 2.544], mean_best_reward: --\n",
      " 19799/100000: episode: 1071, duration: 0.041s, episode steps: 51, steps per second: 1248, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.392 [0.000, 1.000], mean observation: -0.181 [-2.041, 2.029], mean_best_reward: --\n",
      " 19813/100000: episode: 1072, duration: 0.014s, episode steps: 14, steps per second: 1009, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.103 [-1.528, 2.556], mean_best_reward: --\n",
      " 19831/100000: episode: 1073, duration: 0.017s, episode steps: 18, steps per second: 1060, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.061 [-1.406, 2.190], mean_best_reward: --\n",
      " 19841/100000: episode: 1074, duration: 0.010s, episode steps: 10, steps per second: 962, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.140 [-0.974, 1.722], mean_best_reward: --\n",
      " 19852/100000: episode: 1075, duration: 0.011s, episode steps: 11, steps per second: 1014, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.126 [-0.962, 1.748], mean_best_reward: --\n",
      " 19870/100000: episode: 1076, duration: 0.016s, episode steps: 18, steps per second: 1105, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.075 [-1.153, 1.775], mean_best_reward: --\n",
      " 19880/100000: episode: 1077, duration: 0.010s, episode steps: 10, steps per second: 987, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.148 [-1.542, 2.619], mean_best_reward: --\n",
      " 19909/100000: episode: 1078, duration: 0.033s, episode steps: 29, steps per second: 889, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.066 [-1.755, 1.010], mean_best_reward: --\n",
      " 19921/100000: episode: 1079, duration: 0.014s, episode steps: 12, steps per second: 849, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.107 [-1.354, 2.177], mean_best_reward: --\n",
      " 19956/100000: episode: 1080, duration: 0.034s, episode steps: 35, steps per second: 1034, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.062 [-0.625, 1.379], mean_best_reward: --\n",
      " 19966/100000: episode: 1081, duration: 0.010s, episode steps: 10, steps per second: 976, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.124 [-1.551, 2.562], mean_best_reward: --\n",
      " 19975/100000: episode: 1082, duration: 0.009s, episode steps: 9, steps per second: 993, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.136 [-1.379, 2.207], mean_best_reward: --\n",
      " 19987/100000: episode: 1083, duration: 0.014s, episode steps: 12, steps per second: 867, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.121 [-1.936, 3.023], mean_best_reward: --\n",
      " 20001/100000: episode: 1084, duration: 0.014s, episode steps: 14, steps per second: 1030, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.119 [-1.521, 2.545], mean_best_reward: --\n",
      " 20016/100000: episode: 1085, duration: 0.017s, episode steps: 15, steps per second: 882, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.090 [-0.777, 1.465], mean_best_reward: --\n",
      " 20028/100000: episode: 1086, duration: 0.012s, episode steps: 12, steps per second: 1011, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.096 [-1.226, 2.059], mean_best_reward: --\n",
      " 20043/100000: episode: 1087, duration: 0.015s, episode steps: 15, steps per second: 1032, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.123 [-1.336, 2.347], mean_best_reward: --\n",
      " 20059/100000: episode: 1088, duration: 0.014s, episode steps: 16, steps per second: 1106, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.063 [-1.035, 1.697], mean_best_reward: --\n",
      " 20071/100000: episode: 1089, duration: 0.013s, episode steps: 12, steps per second: 959, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.115 [-1.915, 3.017], mean_best_reward: --\n",
      " 20081/100000: episode: 1090, duration: 0.011s, episode steps: 10, steps per second: 944, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.144 [-2.549, 1.589], mean_best_reward: --\n",
      " 20093/100000: episode: 1091, duration: 0.012s, episode steps: 12, steps per second: 1032, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.127 [-1.137, 1.949], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20107/100000: episode: 1092, duration: 0.016s, episode steps: 14, steps per second: 851, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.105 [-1.589, 2.693], mean_best_reward: --\n",
      " 20118/100000: episode: 1093, duration: 0.014s, episode steps: 11, steps per second: 780, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.130 [-1.586, 2.511], mean_best_reward: --\n",
      " 20129/100000: episode: 1094, duration: 0.011s, episode steps: 11, steps per second: 1020, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.115 [-1.167, 1.873], mean_best_reward: --\n",
      " 20139/100000: episode: 1095, duration: 0.010s, episode steps: 10, steps per second: 994, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.980, 3.062], mean_best_reward: --\n",
      " 20151/100000: episode: 1096, duration: 0.012s, episode steps: 12, steps per second: 1036, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.106 [-1.174, 1.991], mean_best_reward: --\n",
      " 20164/100000: episode: 1097, duration: 0.013s, episode steps: 13, steps per second: 1015, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.089 [-1.368, 2.200], mean_best_reward: --\n",
      " 20178/100000: episode: 1098, duration: 0.013s, episode steps: 14, steps per second: 1083, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.097 [-1.926, 3.013], mean_best_reward: --\n",
      " 20189/100000: episode: 1099, duration: 0.011s, episode steps: 11, steps per second: 1024, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.122 [-1.778, 2.776], mean_best_reward: --\n",
      " 20199/100000: episode: 1100, duration: 0.011s, episode steps: 10, steps per second: 917, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.951, 3.085], mean_best_reward: --\n",
      " 20227/100000: episode: 1101, duration: 0.025s, episode steps: 28, steps per second: 1106, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.124 [-1.484, 0.958], mean_best_reward: 36.000000\n",
      " 20245/100000: episode: 1102, duration: 0.019s, episode steps: 18, steps per second: 954, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.081 [-1.368, 2.280], mean_best_reward: --\n",
      " 20260/100000: episode: 1103, duration: 0.016s, episode steps: 15, steps per second: 957, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.098 [-1.906, 1.167], mean_best_reward: --\n",
      " 20277/100000: episode: 1104, duration: 0.018s, episode steps: 17, steps per second: 925, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.081 [-1.418, 2.424], mean_best_reward: --\n",
      " 20291/100000: episode: 1105, duration: 0.014s, episode steps: 14, steps per second: 972, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.100 [-2.237, 1.335], mean_best_reward: --\n",
      " 20310/100000: episode: 1106, duration: 0.027s, episode steps: 19, steps per second: 710, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.073 [-2.824, 1.772], mean_best_reward: --\n",
      " 20320/100000: episode: 1107, duration: 0.011s, episode steps: 10, steps per second: 951, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-3.052, 1.999], mean_best_reward: --\n",
      " 20336/100000: episode: 1108, duration: 0.018s, episode steps: 16, steps per second: 903, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.812 [0.000, 1.000], mean observation: -0.071 [-2.995, 1.955], mean_best_reward: --\n",
      " 20348/100000: episode: 1109, duration: 0.012s, episode steps: 12, steps per second: 1005, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.137 [-1.150, 2.115], mean_best_reward: --\n",
      " 20368/100000: episode: 1110, duration: 0.017s, episode steps: 20, steps per second: 1147, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.758, 1.344], mean_best_reward: --\n",
      " 20381/100000: episode: 1111, duration: 0.012s, episode steps: 13, steps per second: 1048, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.119 [-0.804, 1.348], mean_best_reward: --\n",
      " 20393/100000: episode: 1112, duration: 0.012s, episode steps: 12, steps per second: 1009, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.116 [-1.192, 2.025], mean_best_reward: --\n",
      " 20427/100000: episode: 1113, duration: 0.028s, episode steps: 34, steps per second: 1197, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.034 [-1.019, 1.602], mean_best_reward: --\n",
      " 20442/100000: episode: 1114, duration: 0.014s, episode steps: 15, steps per second: 1079, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.088 [-0.943, 1.564], mean_best_reward: --\n",
      " 20460/100000: episode: 1115, duration: 0.017s, episode steps: 18, steps per second: 1057, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.056 [-1.410, 2.230], mean_best_reward: --\n",
      " 20471/100000: episode: 1116, duration: 0.011s, episode steps: 11, steps per second: 995, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.131 [-2.133, 1.393], mean_best_reward: --\n",
      " 20483/100000: episode: 1117, duration: 0.012s, episode steps: 12, steps per second: 1032, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.125 [-2.080, 1.167], mean_best_reward: --\n",
      " 20494/100000: episode: 1118, duration: 0.011s, episode steps: 11, steps per second: 979, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.119 [-2.324, 1.349], mean_best_reward: --\n",
      " 20506/100000: episode: 1119, duration: 0.012s, episode steps: 12, steps per second: 992, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.114 [-2.032, 1.317], mean_best_reward: --\n",
      " 20521/100000: episode: 1120, duration: 0.019s, episode steps: 15, steps per second: 799, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.113 [-1.804, 0.967], mean_best_reward: --\n",
      " 20561/100000: episode: 1121, duration: 0.039s, episode steps: 40, steps per second: 1037, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.111 [-1.220, 0.560], mean_best_reward: --\n",
      " 20574/100000: episode: 1122, duration: 0.014s, episode steps: 13, steps per second: 912, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.135 [-2.951, 1.764], mean_best_reward: --\n",
      " 20585/100000: episode: 1123, duration: 0.012s, episode steps: 11, steps per second: 883, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.125 [-2.474, 1.538], mean_best_reward: --\n",
      " 20593/100000: episode: 1124, duration: 0.010s, episode steps: 8, steps per second: 808, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.150 [-1.160, 2.013], mean_best_reward: --\n",
      " 20601/100000: episode: 1125, duration: 0.011s, episode steps: 8, steps per second: 747, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.138 [-1.915, 1.211], mean_best_reward: --\n",
      " 20643/100000: episode: 1126, duration: 0.035s, episode steps: 42, steps per second: 1190, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.057 [-1.742, 1.519], mean_best_reward: --\n",
      " 20657/100000: episode: 1127, duration: 0.013s, episode steps: 14, steps per second: 1065, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.111 [-1.557, 2.572], mean_best_reward: --\n",
      " 20669/100000: episode: 1128, duration: 0.012s, episode steps: 12, steps per second: 987, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.103 [-2.138, 1.224], mean_best_reward: --\n",
      " 20683/100000: episode: 1129, duration: 0.015s, episode steps: 14, steps per second: 935, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.107 [-1.576, 2.687], mean_best_reward: --\n",
      " 20698/100000: episode: 1130, duration: 0.014s, episode steps: 15, steps per second: 1074, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.106 [-1.462, 0.795], mean_best_reward: --\n",
      " 20710/100000: episode: 1131, duration: 0.012s, episode steps: 12, steps per second: 989, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.123 [-0.983, 1.565], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20731/100000: episode: 1132, duration: 0.024s, episode steps: 21, steps per second: 867, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.033 [-2.842, 1.978], mean_best_reward: --\n",
      " 20765/100000: episode: 1133, duration: 0.030s, episode steps: 34, steps per second: 1150, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.124 [-1.384, 0.612], mean_best_reward: --\n",
      " 20776/100000: episode: 1134, duration: 0.011s, episode steps: 11, steps per second: 982, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.114 [-1.418, 2.279], mean_best_reward: --\n",
      " 20788/100000: episode: 1135, duration: 0.012s, episode steps: 12, steps per second: 1037, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.130 [-1.956, 3.055], mean_best_reward: --\n",
      " 20813/100000: episode: 1136, duration: 0.021s, episode steps: 25, steps per second: 1169, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.079 [-1.567, 0.786], mean_best_reward: --\n",
      " 20821/100000: episode: 1137, duration: 0.009s, episode steps: 8, steps per second: 905, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.142 [-1.355, 2.221], mean_best_reward: --\n",
      " 20842/100000: episode: 1138, duration: 0.018s, episode steps: 21, steps per second: 1142, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.057 [-2.258, 1.362], mean_best_reward: --\n",
      " 20857/100000: episode: 1139, duration: 0.014s, episode steps: 15, steps per second: 1062, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.096 [-0.995, 1.777], mean_best_reward: --\n",
      " 20879/100000: episode: 1140, duration: 0.019s, episode steps: 22, steps per second: 1160, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.032 [-1.956, 2.923], mean_best_reward: --\n",
      " 20895/100000: episode: 1141, duration: 0.015s, episode steps: 16, steps per second: 1093, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.073 [-2.427, 1.585], mean_best_reward: --\n",
      " 20905/100000: episode: 1142, duration: 0.010s, episode steps: 10, steps per second: 983, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.157 [-1.538, 2.624], mean_best_reward: --\n",
      " 20914/100000: episode: 1143, duration: 0.010s, episode steps: 9, steps per second: 901, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.146 [-1.566, 2.454], mean_best_reward: --\n",
      " 20935/100000: episode: 1144, duration: 0.020s, episode steps: 21, steps per second: 1035, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.063 [-1.153, 1.864], mean_best_reward: --\n",
      " 20947/100000: episode: 1145, duration: 0.017s, episode steps: 12, steps per second: 706, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.125 [-1.161, 1.977], mean_best_reward: --\n",
      " 20995/100000: episode: 1146, duration: 0.044s, episode steps: 48, steps per second: 1093, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.136 [-1.741, 1.499], mean_best_reward: --\n",
      " 21008/100000: episode: 1147, duration: 0.013s, episode steps: 13, steps per second: 990, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.093 [-1.429, 0.777], mean_best_reward: --\n",
      " 21028/100000: episode: 1148, duration: 0.021s, episode steps: 20, steps per second: 974, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.068 [-2.153, 1.358], mean_best_reward: --\n",
      " 21048/100000: episode: 1149, duration: 0.018s, episode steps: 20, steps per second: 1096, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.108 [-1.188, 0.583], mean_best_reward: --\n",
      " 21059/100000: episode: 1150, duration: 0.011s, episode steps: 11, steps per second: 1004, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.107 [-1.412, 2.284], mean_best_reward: --\n",
      " 21070/100000: episode: 1151, duration: 0.012s, episode steps: 11, steps per second: 956, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.116 [-1.135, 1.850], mean_best_reward: 100.500000\n",
      " 21115/100000: episode: 1152, duration: 0.036s, episode steps: 45, steps per second: 1242, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.041 [-0.607, 1.111], mean_best_reward: --\n",
      " 21159/100000: episode: 1153, duration: 0.036s, episode steps: 44, steps per second: 1236, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.149 [-1.177, 0.288], mean_best_reward: --\n",
      " 21176/100000: episode: 1154, duration: 0.019s, episode steps: 17, steps per second: 889, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.091 [-0.649, 1.262], mean_best_reward: --\n",
      " 21190/100000: episode: 1155, duration: 0.016s, episode steps: 14, steps per second: 892, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.098 [-1.800, 1.186], mean_best_reward: --\n",
      " 21198/100000: episode: 1156, duration: 0.009s, episode steps: 8, steps per second: 916, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.151 [-1.981, 1.130], mean_best_reward: --\n",
      " 21276/100000: episode: 1157, duration: 0.061s, episode steps: 78, steps per second: 1279, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.035 [-1.068, 0.596], mean_best_reward: --\n",
      " 21349/100000: episode: 1158, duration: 0.065s, episode steps: 73, steps per second: 1123, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.141 [-1.283, 0.531], mean_best_reward: --\n",
      " 21386/100000: episode: 1159, duration: 0.031s, episode steps: 37, steps per second: 1188, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.041 [-1.391, 0.757], mean_best_reward: --\n",
      " 21409/100000: episode: 1160, duration: 0.025s, episode steps: 23, steps per second: 913, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.061 [-2.011, 1.193], mean_best_reward: --\n",
      " 21441/100000: episode: 1161, duration: 0.030s, episode steps: 32, steps per second: 1052, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.656 [0.000, 1.000], mean observation: 0.005 [-2.906, 2.175], mean_best_reward: --\n",
      " 21494/100000: episode: 1162, duration: 0.042s, episode steps: 53, steps per second: 1264, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.110 [-1.100, 0.767], mean_best_reward: --\n",
      " 21504/100000: episode: 1163, duration: 0.010s, episode steps: 10, steps per second: 963, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.136 [-2.065, 1.193], mean_best_reward: --\n",
      " 21520/100000: episode: 1164, duration: 0.015s, episode steps: 16, steps per second: 1086, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.116 [-1.777, 0.953], mean_best_reward: --\n",
      " 21533/100000: episode: 1165, duration: 0.013s, episode steps: 13, steps per second: 1039, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.112 [-1.876, 1.034], mean_best_reward: --\n",
      " 21568/100000: episode: 1166, duration: 0.029s, episode steps: 35, steps per second: 1216, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.135 [-0.442, 1.114], mean_best_reward: --\n",
      " 21591/100000: episode: 1167, duration: 0.020s, episode steps: 23, steps per second: 1133, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.101 [-1.042, 0.412], mean_best_reward: --\n",
      " 21605/100000: episode: 1168, duration: 0.013s, episode steps: 14, steps per second: 1043, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.082 [-2.257, 1.400], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21643/100000: episode: 1169, duration: 0.036s, episode steps: 38, steps per second: 1062, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: -0.022 [-1.558, 0.959], mean_best_reward: --\n",
      " 21654/100000: episode: 1170, duration: 0.014s, episode steps: 11, steps per second: 809, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.140 [-2.269, 1.327], mean_best_reward: --\n",
      " 21693/100000: episode: 1171, duration: 0.035s, episode steps: 39, steps per second: 1121, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.094 [-0.395, 1.377], mean_best_reward: --\n",
      " 21743/100000: episode: 1172, duration: 0.043s, episode steps: 50, steps per second: 1152, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.145 [-0.973, 0.821], mean_best_reward: --\n",
      " 21756/100000: episode: 1173, duration: 0.017s, episode steps: 13, steps per second: 769, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.117 [-2.830, 1.752], mean_best_reward: --\n",
      " 21840/100000: episode: 1174, duration: 0.067s, episode steps: 84, steps per second: 1255, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.029 [-1.963, 1.384], mean_best_reward: --\n",
      " 21849/100000: episode: 1175, duration: 0.010s, episode steps: 9, steps per second: 872, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.752, 1.725], mean_best_reward: --\n",
      " 21873/100000: episode: 1176, duration: 0.028s, episode steps: 24, steps per second: 865, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-0.741, 1.110], mean_best_reward: --\n",
      " 21907/100000: episode: 1177, duration: 0.030s, episode steps: 34, steps per second: 1144, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.144 [-0.547, 0.994], mean_best_reward: --\n",
      " 21919/100000: episode: 1178, duration: 0.012s, episode steps: 12, steps per second: 1015, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.088 [-2.114, 1.411], mean_best_reward: --\n",
      " 21939/100000: episode: 1179, duration: 0.018s, episode steps: 20, steps per second: 1129, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.098 [-1.669, 0.776], mean_best_reward: --\n",
      " 21955/100000: episode: 1180, duration: 0.015s, episode steps: 16, steps per second: 1054, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.084 [-2.170, 1.334], mean_best_reward: --\n",
      " 21996/100000: episode: 1181, duration: 0.033s, episode steps: 41, steps per second: 1239, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.067 [-0.884, 0.557], mean_best_reward: --\n",
      " 22006/100000: episode: 1182, duration: 0.011s, episode steps: 10, steps per second: 904, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.127 [-2.980, 1.907], mean_best_reward: --\n",
      " 22014/100000: episode: 1183, duration: 0.008s, episode steps: 8, steps per second: 944, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.135 [-2.199, 1.365], mean_best_reward: --\n",
      " 22026/100000: episode: 1184, duration: 0.013s, episode steps: 12, steps per second: 942, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.121 [-3.083, 1.969], mean_best_reward: --\n",
      " 22114/100000: episode: 1185, duration: 0.086s, episode steps: 88, steps per second: 1024, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-0.547, 1.381], mean_best_reward: --\n",
      " 22145/100000: episode: 1186, duration: 0.026s, episode steps: 31, steps per second: 1180, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.110 [-0.398, 1.028], mean_best_reward: --\n",
      " 22160/100000: episode: 1187, duration: 0.016s, episode steps: 15, steps per second: 959, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.105 [-2.265, 1.333], mean_best_reward: --\n",
      " 22175/100000: episode: 1188, duration: 0.014s, episode steps: 15, steps per second: 1083, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.099 [-1.941, 1.034], mean_best_reward: --\n",
      " 22208/100000: episode: 1189, duration: 0.028s, episode steps: 33, steps per second: 1190, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.052 [-1.390, 0.624], mean_best_reward: --\n",
      " 22249/100000: episode: 1190, duration: 0.033s, episode steps: 41, steps per second: 1229, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.076 [-0.380, 0.743], mean_best_reward: --\n",
      " 22267/100000: episode: 1191, duration: 0.016s, episode steps: 18, steps per second: 1116, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-1.080, 0.642], mean_best_reward: --\n",
      " 22284/100000: episode: 1192, duration: 0.016s, episode steps: 17, steps per second: 1064, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.095 [-1.852, 1.035], mean_best_reward: --\n",
      " 22305/100000: episode: 1193, duration: 0.018s, episode steps: 21, steps per second: 1147, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.088 [-1.310, 0.583], mean_best_reward: --\n",
      " 22318/100000: episode: 1194, duration: 0.013s, episode steps: 13, steps per second: 1021, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.087 [-1.891, 1.194], mean_best_reward: --\n",
      " 22372/100000: episode: 1195, duration: 0.051s, episode steps: 54, steps per second: 1066, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.574 [0.000, 1.000], mean observation: 0.254 [-0.629, 1.654], mean_best_reward: --\n",
      " 22392/100000: episode: 1196, duration: 0.018s, episode steps: 20, steps per second: 1097, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.060 [-1.634, 1.169], mean_best_reward: --\n",
      " 22405/100000: episode: 1197, duration: 0.013s, episode steps: 13, steps per second: 996, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.092 [-1.725, 0.997], mean_best_reward: --\n",
      " 22516/100000: episode: 1198, duration: 0.099s, episode steps: 111, steps per second: 1119, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.204 [-1.555, 0.870], mean_best_reward: --\n",
      " 22541/100000: episode: 1199, duration: 0.024s, episode steps: 25, steps per second: 1045, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.062 [-1.020, 0.636], mean_best_reward: --\n",
      " 22553/100000: episode: 1200, duration: 0.012s, episode steps: 12, steps per second: 1005, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.103 [-2.123, 1.323], mean_best_reward: --\n",
      " 22639/100000: episode: 1201, duration: 0.076s, episode steps: 86, steps per second: 1135, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.001 [-1.616, 1.156], mean_best_reward: 53.500000\n",
      " 22650/100000: episode: 1202, duration: 0.012s, episode steps: 11, steps per second: 908, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.128 [-1.130, 1.941], mean_best_reward: --\n",
      " 22661/100000: episode: 1203, duration: 0.011s, episode steps: 11, steps per second: 1008, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.101 [-2.121, 1.400], mean_best_reward: --\n",
      " 22674/100000: episode: 1204, duration: 0.013s, episode steps: 13, steps per second: 1038, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.122 [-0.740, 1.282], mean_best_reward: --\n",
      " 22684/100000: episode: 1205, duration: 0.010s, episode steps: 10, steps per second: 1003, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.156 [-2.604, 1.567], mean_best_reward: --\n",
      " 22712/100000: episode: 1206, duration: 0.023s, episode steps: 28, steps per second: 1193, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.393 [0.000, 1.000], mean observation: 0.037 [-1.233, 2.168], mean_best_reward: --\n",
      " 22728/100000: episode: 1207, duration: 0.015s, episode steps: 16, steps per second: 1092, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.110 [-1.339, 2.267], mean_best_reward: --\n",
      " 22742/100000: episode: 1208, duration: 0.014s, episode steps: 14, steps per second: 995, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.105 [-2.069, 1.207], mean_best_reward: --\n",
      " 22752/100000: episode: 1209, duration: 0.010s, episode steps: 10, steps per second: 971, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-3.048, 1.975], mean_best_reward: --\n",
      " 22763/100000: episode: 1210, duration: 0.016s, episode steps: 11, steps per second: 705, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.122 [-1.021, 1.783], mean_best_reward: --\n",
      " 22778/100000: episode: 1211, duration: 0.016s, episode steps: 15, steps per second: 941, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.098 [-2.307, 1.393], mean_best_reward: --\n",
      " 22791/100000: episode: 1212, duration: 0.015s, episode steps: 13, steps per second: 881, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.114 [-2.335, 1.394], mean_best_reward: --\n",
      " 22813/100000: episode: 1213, duration: 0.023s, episode steps: 22, steps per second: 945, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-1.108, 0.548], mean_best_reward: --\n",
      " 22823/100000: episode: 1214, duration: 0.010s, episode steps: 10, steps per second: 977, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.121 [-2.490, 1.586], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22838/100000: episode: 1215, duration: 0.018s, episode steps: 15, steps per second: 812, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.091 [-1.139, 1.724], mean_best_reward: --\n",
      " 22847/100000: episode: 1216, duration: 0.015s, episode steps: 9, steps per second: 606, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.793, 1.719], mean_best_reward: --\n",
      " 22862/100000: episode: 1217, duration: 0.017s, episode steps: 15, steps per second: 861, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.096 [-1.382, 2.390], mean_best_reward: --\n",
      " 22874/100000: episode: 1218, duration: 0.013s, episode steps: 12, steps per second: 937, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.125 [-2.057, 1.175], mean_best_reward: --\n",
      " 22884/100000: episode: 1219, duration: 0.011s, episode steps: 10, steps per second: 911, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.099 [-1.890, 1.198], mean_best_reward: --\n",
      " 22937/100000: episode: 1220, duration: 0.042s, episode steps: 53, steps per second: 1257, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.077 [-1.019, 0.482], mean_best_reward: --\n",
      " 22951/100000: episode: 1221, duration: 0.013s, episode steps: 14, steps per second: 1038, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.091 [-1.737, 0.999], mean_best_reward: --\n",
      " 22963/100000: episode: 1222, duration: 0.012s, episode steps: 12, steps per second: 1022, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.121 [-1.594, 0.982], mean_best_reward: --\n",
      " 22973/100000: episode: 1223, duration: 0.010s, episode steps: 10, steps per second: 997, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.113 [-1.206, 1.953], mean_best_reward: --\n",
      " 22992/100000: episode: 1224, duration: 0.017s, episode steps: 19, steps per second: 1112, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.073 [-1.796, 1.014], mean_best_reward: --\n",
      " 23002/100000: episode: 1225, duration: 0.010s, episode steps: 10, steps per second: 964, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.133 [-1.356, 2.099], mean_best_reward: --\n",
      " 23015/100000: episode: 1226, duration: 0.012s, episode steps: 13, steps per second: 1047, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.120 [-1.334, 0.594], mean_best_reward: --\n",
      " 23029/100000: episode: 1227, duration: 0.014s, episode steps: 14, steps per second: 1030, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.085 [-2.580, 1.614], mean_best_reward: --\n",
      " 23044/100000: episode: 1228, duration: 0.019s, episode steps: 15, steps per second: 792, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.096 [-1.154, 1.722], mean_best_reward: --\n",
      " 23064/100000: episode: 1229, duration: 0.023s, episode steps: 20, steps per second: 884, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.052 [-2.174, 1.378], mean_best_reward: --\n",
      " 23076/100000: episode: 1230, duration: 0.012s, episode steps: 12, steps per second: 973, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.101 [-1.694, 1.025], mean_best_reward: --\n",
      " 23091/100000: episode: 1231, duration: 0.018s, episode steps: 15, steps per second: 844, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.076 [-1.834, 1.206], mean_best_reward: --\n",
      " 23102/100000: episode: 1232, duration: 0.013s, episode steps: 11, steps per second: 846, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.102 [-2.189, 1.409], mean_best_reward: --\n",
      " 23125/100000: episode: 1233, duration: 0.025s, episode steps: 23, steps per second: 911, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.114 [-1.664, 0.574], mean_best_reward: --\n",
      " 23140/100000: episode: 1234, duration: 0.014s, episode steps: 15, steps per second: 1076, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.082 [-1.792, 1.021], mean_best_reward: --\n",
      " 23158/100000: episode: 1235, duration: 0.016s, episode steps: 18, steps per second: 1093, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.047 [-2.908, 1.987], mean_best_reward: --\n",
      " 23180/100000: episode: 1236, duration: 0.019s, episode steps: 22, steps per second: 1164, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.068 [-1.174, 2.158], mean_best_reward: --\n",
      " 23200/100000: episode: 1237, duration: 0.020s, episode steps: 20, steps per second: 993, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.105 [-1.506, 1.005], mean_best_reward: --\n",
      " 23218/100000: episode: 1238, duration: 0.016s, episode steps: 18, steps per second: 1100, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.093 [-1.236, 0.567], mean_best_reward: --\n",
      " 23232/100000: episode: 1239, duration: 0.013s, episode steps: 14, steps per second: 1059, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.077 [-2.085, 1.392], mean_best_reward: --\n",
      " 23241/100000: episode: 1240, duration: 0.013s, episode steps: 9, steps per second: 673, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.138 [-1.375, 2.198], mean_best_reward: --\n",
      " 23255/100000: episode: 1241, duration: 0.017s, episode steps: 14, steps per second: 814, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.083 [-1.514, 1.004], mean_best_reward: --\n",
      " 23274/100000: episode: 1242, duration: 0.018s, episode steps: 19, steps per second: 1061, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.073 [-1.184, 0.638], mean_best_reward: --\n",
      " 23289/100000: episode: 1243, duration: 0.014s, episode steps: 15, steps per second: 1069, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.068 [-2.138, 1.365], mean_best_reward: --\n",
      " 23300/100000: episode: 1244, duration: 0.011s, episode steps: 11, steps per second: 1008, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.130 [-1.558, 2.475], mean_best_reward: --\n",
      " 23338/100000: episode: 1245, duration: 0.031s, episode steps: 38, steps per second: 1226, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.416, 1.340], mean_best_reward: --\n",
      " 23348/100000: episode: 1246, duration: 0.010s, episode steps: 10, steps per second: 978, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.123 [-1.960, 1.152], mean_best_reward: --\n",
      " 23375/100000: episode: 1247, duration: 0.023s, episode steps: 27, steps per second: 1183, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.080 [-1.121, 0.573], mean_best_reward: --\n",
      " 23385/100000: episode: 1248, duration: 0.011s, episode steps: 10, steps per second: 943, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.155 [-2.069, 1.159], mean_best_reward: --\n",
      " 23402/100000: episode: 1249, duration: 0.016s, episode steps: 17, steps per second: 1091, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.073 [-1.549, 2.440], mean_best_reward: --\n",
      " 23412/100000: episode: 1250, duration: 0.010s, episode steps: 10, steps per second: 976, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.595, 1.594], mean_best_reward: --\n",
      " 23426/100000: episode: 1251, duration: 0.015s, episode steps: 14, steps per second: 946, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.088 [-2.207, 1.411], mean_best_reward: 74.000000\n",
      " 23438/100000: episode: 1252, duration: 0.013s, episode steps: 12, steps per second: 898, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.129 [-2.536, 1.559], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23451/100000: episode: 1253, duration: 0.017s, episode steps: 13, steps per second: 744, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.118 [-2.969, 1.922], mean_best_reward: --\n",
      " 23467/100000: episode: 1254, duration: 0.019s, episode steps: 16, steps per second: 858, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.092 [-1.983, 1.134], mean_best_reward: --\n",
      " 23478/100000: episode: 1255, duration: 0.011s, episode steps: 11, steps per second: 977, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.110 [-2.353, 1.418], mean_best_reward: --\n",
      " 23497/100000: episode: 1256, duration: 0.017s, episode steps: 19, steps per second: 1112, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.108 [-0.770, 1.660], mean_best_reward: --\n",
      " 23526/100000: episode: 1257, duration: 0.025s, episode steps: 29, steps per second: 1154, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.063 [-1.126, 0.397], mean_best_reward: --\n",
      " 23553/100000: episode: 1258, duration: 0.025s, episode steps: 27, steps per second: 1099, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.089 [-0.945, 0.577], mean_best_reward: --\n",
      " 23563/100000: episode: 1259, duration: 0.010s, episode steps: 10, steps per second: 961, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-3.061, 1.922], mean_best_reward: --\n",
      " 23634/100000: episode: 1260, duration: 0.056s, episode steps: 71, steps per second: 1265, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.023 [-0.887, 0.613], mean_best_reward: --\n",
      " 23651/100000: episode: 1261, duration: 0.015s, episode steps: 17, steps per second: 1112, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.060 [-2.800, 1.808], mean_best_reward: --\n",
      " 23663/100000: episode: 1262, duration: 0.012s, episode steps: 12, steps per second: 981, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.114 [-2.526, 1.532], mean_best_reward: --\n",
      " 23675/100000: episode: 1263, duration: 0.015s, episode steps: 12, steps per second: 825, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.079 [-1.625, 1.019], mean_best_reward: --\n",
      " 23691/100000: episode: 1264, duration: 0.019s, episode steps: 16, steps per second: 822, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.129 [-1.386, 0.574], mean_best_reward: --\n",
      " 23705/100000: episode: 1265, duration: 0.014s, episode steps: 14, steps per second: 1034, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.095 [-2.235, 1.357], mean_best_reward: --\n",
      " 23721/100000: episode: 1266, duration: 0.015s, episode steps: 16, steps per second: 1064, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.504, 0.982], mean_best_reward: --\n",
      " 23732/100000: episode: 1267, duration: 0.011s, episode steps: 11, steps per second: 995, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.104 [-2.384, 1.562], mean_best_reward: --\n",
      " 23749/100000: episode: 1268, duration: 0.016s, episode steps: 17, steps per second: 1078, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.078 [-1.725, 0.950], mean_best_reward: --\n",
      " 23762/100000: episode: 1269, duration: 0.013s, episode steps: 13, steps per second: 1002, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.119 [-2.834, 1.747], mean_best_reward: --\n",
      " 23773/100000: episode: 1270, duration: 0.012s, episode steps: 11, steps per second: 900, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.117 [-2.239, 1.357], mean_best_reward: --\n",
      " 23789/100000: episode: 1271, duration: 0.020s, episode steps: 16, steps per second: 818, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.101 [-1.755, 0.973], mean_best_reward: --\n",
      " 23799/100000: episode: 1272, duration: 0.011s, episode steps: 10, steps per second: 913, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.131 [-1.707, 1.014], mean_best_reward: --\n",
      " 23823/100000: episode: 1273, duration: 0.022s, episode steps: 24, steps per second: 1074, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-0.921, 0.429], mean_best_reward: --\n",
      " 23843/100000: episode: 1274, duration: 0.018s, episode steps: 20, steps per second: 1126, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.078 [-1.188, 0.638], mean_best_reward: --\n",
      " 23893/100000: episode: 1275, duration: 0.048s, episode steps: 50, steps per second: 1037, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.109 [-0.349, 0.756], mean_best_reward: --\n",
      " 23930/100000: episode: 1276, duration: 0.031s, episode steps: 37, steps per second: 1207, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.048 [-0.786, 1.021], mean_best_reward: --\n",
      " 23968/100000: episode: 1277, duration: 0.031s, episode steps: 38, steps per second: 1233, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.036 [-1.462, 0.829], mean_best_reward: --\n",
      " 23981/100000: episode: 1278, duration: 0.013s, episode steps: 13, steps per second: 1030, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.108 [-0.810, 1.330], mean_best_reward: --\n",
      " 24004/100000: episode: 1279, duration: 0.020s, episode steps: 23, steps per second: 1149, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.123 [-0.372, 1.263], mean_best_reward: --\n",
      " 24043/100000: episode: 1280, duration: 0.032s, episode steps: 39, steps per second: 1223, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.013 [-1.466, 1.121], mean_best_reward: --\n",
      " 24060/100000: episode: 1281, duration: 0.015s, episode steps: 17, steps per second: 1120, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.118 [-1.172, 0.564], mean_best_reward: --\n",
      " 24069/100000: episode: 1282, duration: 0.009s, episode steps: 9, steps per second: 963, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.864, 1.730], mean_best_reward: --\n",
      " 24105/100000: episode: 1283, duration: 0.030s, episode steps: 36, steps per second: 1188, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.074 [-0.926, 0.590], mean_best_reward: --\n",
      " 24114/100000: episode: 1284, duration: 0.010s, episode steps: 9, steps per second: 898, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.140 [-2.255, 1.363], mean_best_reward: --\n",
      " 24159/100000: episode: 1285, duration: 0.050s, episode steps: 45, steps per second: 906, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.065 [-1.089, 0.820], mean_best_reward: --\n",
      " 24200/100000: episode: 1286, duration: 0.036s, episode steps: 41, steps per second: 1143, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.585 [0.000, 1.000], mean observation: -0.001 [-1.907, 1.326], mean_best_reward: --\n",
      " 24237/100000: episode: 1287, duration: 0.030s, episode steps: 37, steps per second: 1232, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.095 [-0.987, 0.551], mean_best_reward: --\n",
      " 24248/100000: episode: 1288, duration: 0.012s, episode steps: 11, steps per second: 892, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.133 [-2.361, 1.376], mean_best_reward: --\n",
      " 24263/100000: episode: 1289, duration: 0.015s, episode steps: 15, steps per second: 984, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.116 [-1.947, 1.127], mean_best_reward: --\n",
      " 24276/100000: episode: 1290, duration: 0.018s, episode steps: 13, steps per second: 718, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.104 [-2.112, 1.346], mean_best_reward: --\n",
      " 24301/100000: episode: 1291, duration: 0.022s, episode steps: 25, steps per second: 1161, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.117 [-0.564, 0.992], mean_best_reward: --\n",
      " 24314/100000: episode: 1292, duration: 0.012s, episode steps: 13, steps per second: 1051, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.091 [-2.224, 1.341], mean_best_reward: --\n",
      " 24325/100000: episode: 1293, duration: 0.011s, episode steps: 11, steps per second: 1002, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.123 [-2.337, 1.399], mean_best_reward: --\n",
      " 24345/100000: episode: 1294, duration: 0.018s, episode steps: 20, steps per second: 1125, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.062 [-1.876, 1.131], mean_best_reward: --\n",
      " 24359/100000: episode: 1295, duration: 0.013s, episode steps: 14, steps per second: 1066, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.093 [-2.934, 1.909], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24370/100000: episode: 1296, duration: 0.013s, episode steps: 11, steps per second: 852, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.107 [-2.298, 1.414], mean_best_reward: --\n",
      " 24387/100000: episode: 1297, duration: 0.020s, episode steps: 17, steps per second: 869, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.069 [-2.753, 1.766], mean_best_reward: --\n",
      " 24400/100000: episode: 1298, duration: 0.012s, episode steps: 13, steps per second: 1050, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.093 [-1.779, 1.037], mean_best_reward: --\n",
      " 24410/100000: episode: 1299, duration: 0.010s, episode steps: 10, steps per second: 983, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.519, 1.600], mean_best_reward: --\n",
      " 24468/100000: episode: 1300, duration: 0.046s, episode steps: 58, steps per second: 1260, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: 0.039 [-0.813, 2.088], mean_best_reward: --\n",
      " 24521/100000: episode: 1301, duration: 0.042s, episode steps: 53, steps per second: 1256, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.009 [-0.810, 1.521], mean_best_reward: 45.500000\n",
      " 24530/100000: episode: 1302, duration: 0.010s, episode steps: 9, steps per second: 941, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.147 [-2.268, 1.344], mean_best_reward: --\n",
      " 24557/100000: episode: 1303, duration: 0.023s, episode steps: 27, steps per second: 1191, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.031 [-1.039, 1.808], mean_best_reward: --\n",
      " 24603/100000: episode: 1304, duration: 0.039s, episode steps: 46, steps per second: 1185, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.040 [-0.752, 1.388], mean_best_reward: --\n",
      " 24630/100000: episode: 1305, duration: 0.027s, episode steps: 27, steps per second: 1007, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.087 [-1.033, 0.363], mean_best_reward: --\n",
      " 24645/100000: episode: 1306, duration: 0.014s, episode steps: 15, steps per second: 1079, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.093 [-0.795, 1.506], mean_best_reward: --\n",
      " 24667/100000: episode: 1307, duration: 0.019s, episode steps: 22, steps per second: 1130, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.057 [-1.393, 0.976], mean_best_reward: --\n",
      " 24681/100000: episode: 1308, duration: 0.013s, episode steps: 14, steps per second: 1059, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.097 [-2.249, 1.354], mean_best_reward: --\n",
      " 24701/100000: episode: 1309, duration: 0.018s, episode steps: 20, steps per second: 1137, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.615, 1.240], mean_best_reward: --\n",
      " 24714/100000: episode: 1310, duration: 0.012s, episode steps: 13, steps per second: 1056, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.117 [-1.737, 2.750], mean_best_reward: --\n",
      " 24772/100000: episode: 1311, duration: 0.046s, episode steps: 58, steps per second: 1258, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.130 [-0.971, 0.468], mean_best_reward: --\n",
      " 24790/100000: episode: 1312, duration: 0.016s, episode steps: 18, steps per second: 1115, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.099 [-0.766, 1.565], mean_best_reward: --\n",
      " 24813/100000: episode: 1313, duration: 0.020s, episode steps: 23, steps per second: 1154, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.093 [-0.352, 0.858], mean_best_reward: --\n",
      " 24826/100000: episode: 1314, duration: 0.014s, episode steps: 13, steps per second: 948, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.088 [-1.415, 2.186], mean_best_reward: --\n",
      " 24846/100000: episode: 1315, duration: 0.022s, episode steps: 20, steps per second: 911, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.404, 0.908], mean_best_reward: --\n",
      " 24876/100000: episode: 1316, duration: 0.025s, episode steps: 30, steps per second: 1208, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.068 [-1.503, 0.598], mean_best_reward: --\n",
      " 24897/100000: episode: 1317, duration: 0.019s, episode steps: 21, steps per second: 1128, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.075 [-1.014, 1.740], mean_best_reward: --\n",
      " 24940/100000: episode: 1318, duration: 0.036s, episode steps: 43, steps per second: 1206, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.605 [0.000, 1.000], mean observation: 0.103 [-2.088, 1.763], mean_best_reward: --\n",
      " 24956/100000: episode: 1319, duration: 0.015s, episode steps: 16, steps per second: 1078, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.094 [-2.175, 1.346], mean_best_reward: --\n",
      " 24976/100000: episode: 1320, duration: 0.018s, episode steps: 20, steps per second: 1123, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.084 [-2.198, 1.343], mean_best_reward: --\n",
      " 24992/100000: episode: 1321, duration: 0.015s, episode steps: 16, steps per second: 1069, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.105 [-0.614, 1.109], mean_best_reward: --\n",
      " 25003/100000: episode: 1322, duration: 0.011s, episode steps: 11, steps per second: 1006, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.129 [-1.323, 2.069], mean_best_reward: --\n",
      " 25014/100000: episode: 1323, duration: 0.011s, episode steps: 11, steps per second: 1003, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.120 [-1.799, 2.797], mean_best_reward: --\n",
      " 25028/100000: episode: 1324, duration: 0.013s, episode steps: 14, steps per second: 1041, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.098 [-2.408, 1.580], mean_best_reward: --\n",
      " 25046/100000: episode: 1325, duration: 0.017s, episode steps: 18, steps per second: 1030, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.096 [-1.218, 2.213], mean_best_reward: --\n",
      " 25063/100000: episode: 1326, duration: 0.019s, episode steps: 17, steps per second: 907, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.089 [-0.799, 1.202], mean_best_reward: --\n",
      " 25110/100000: episode: 1327, duration: 0.038s, episode steps: 47, steps per second: 1253, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.175 [-0.404, 0.915], mean_best_reward: --\n",
      " 25131/100000: episode: 1328, duration: 0.018s, episode steps: 21, steps per second: 1142, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.091 [-0.531, 1.015], mean_best_reward: --\n",
      " 25147/100000: episode: 1329, duration: 0.015s, episode steps: 16, steps per second: 1095, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.081 [-1.463, 0.959], mean_best_reward: --\n",
      " 25160/100000: episode: 1330, duration: 0.013s, episode steps: 13, steps per second: 988, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.130 [-2.492, 1.542], mean_best_reward: --\n",
      " 25169/100000: episode: 1331, duration: 0.010s, episode steps: 9, steps per second: 930, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.158 [-2.273, 1.330], mean_best_reward: --\n",
      " 25187/100000: episode: 1332, duration: 0.016s, episode steps: 18, steps per second: 1102, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.066 [-2.591, 1.590], mean_best_reward: --\n",
      " 25210/100000: episode: 1333, duration: 0.020s, episode steps: 23, steps per second: 1126, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.054 [-0.626, 1.065], mean_best_reward: --\n",
      " 25221/100000: episode: 1334, duration: 0.011s, episode steps: 11, steps per second: 1011, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.129 [-2.820, 1.755], mean_best_reward: --\n",
      " 25237/100000: episode: 1335, duration: 0.015s, episode steps: 16, steps per second: 1094, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.086 [-0.943, 1.414], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25261/100000: episode: 1336, duration: 0.021s, episode steps: 24, steps per second: 1157, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.073 [-0.749, 1.455], mean_best_reward: --\n",
      " 25299/100000: episode: 1337, duration: 0.037s, episode steps: 38, steps per second: 1028, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.605 [0.000, 1.000], mean observation: 0.105 [-1.842, 1.758], mean_best_reward: --\n",
      " 25326/100000: episode: 1338, duration: 0.022s, episode steps: 27, steps per second: 1225, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.111 [-0.374, 1.288], mean_best_reward: --\n",
      " 25353/100000: episode: 1339, duration: 0.023s, episode steps: 27, steps per second: 1175, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.119 [-0.603, 0.970], mean_best_reward: --\n",
      " 25364/100000: episode: 1340, duration: 0.011s, episode steps: 11, steps per second: 1004, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.120 [-1.756, 0.950], mean_best_reward: --\n",
      " 25383/100000: episode: 1341, duration: 0.017s, episode steps: 19, steps per second: 1117, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.099 [-1.946, 1.031], mean_best_reward: --\n",
      " 25502/100000: episode: 1342, duration: 0.093s, episode steps: 119, steps per second: 1277, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.103 [-1.278, 0.727], mean_best_reward: --\n",
      " 25523/100000: episode: 1343, duration: 0.022s, episode steps: 21, steps per second: 940, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.762 [0.000, 1.000], mean observation: -0.050 [-3.243, 2.183], mean_best_reward: --\n",
      " 25534/100000: episode: 1344, duration: 0.011s, episode steps: 11, steps per second: 1002, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.128 [-2.252, 1.369], mean_best_reward: --\n",
      " 25545/100000: episode: 1345, duration: 0.012s, episode steps: 11, steps per second: 938, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.105 [-1.175, 1.794], mean_best_reward: --\n",
      " 25556/100000: episode: 1346, duration: 0.011s, episode steps: 11, steps per second: 971, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.122 [-2.307, 1.333], mean_best_reward: --\n",
      " 25584/100000: episode: 1347, duration: 0.023s, episode steps: 28, steps per second: 1201, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.137 [-0.357, 0.766], mean_best_reward: --\n",
      " 25605/100000: episode: 1348, duration: 0.018s, episode steps: 21, steps per second: 1147, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.099 [-0.567, 1.475], mean_best_reward: --\n",
      " 25634/100000: episode: 1349, duration: 0.024s, episode steps: 29, steps per second: 1192, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.063 [-0.824, 1.580], mean_best_reward: --\n",
      " 25645/100000: episode: 1350, duration: 0.011s, episode steps: 11, steps per second: 969, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.114 [-2.805, 1.791], mean_best_reward: --\n",
      " 25665/100000: episode: 1351, duration: 0.018s, episode steps: 20, steps per second: 1100, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.074 [-0.962, 1.399], mean_best_reward: 72.000000\n",
      " 25677/100000: episode: 1352, duration: 0.012s, episode steps: 12, steps per second: 1034, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.119 [-1.620, 2.627], mean_best_reward: --\n",
      " 25687/100000: episode: 1353, duration: 0.010s, episode steps: 10, steps per second: 986, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.147 [-1.369, 2.282], mean_best_reward: --\n",
      " 25704/100000: episode: 1354, duration: 0.015s, episode steps: 17, steps per second: 1120, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.091 [-1.529, 2.460], mean_best_reward: --\n",
      " 25716/100000: episode: 1355, duration: 0.015s, episode steps: 12, steps per second: 809, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.097 [-2.143, 1.342], mean_best_reward: --\n",
      " 25769/100000: episode: 1356, duration: 0.048s, episode steps: 53, steps per second: 1113, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.125 [-1.217, 0.715], mean_best_reward: --\n",
      " 25865/100000: episode: 1357, duration: 0.074s, episode steps: 96, steps per second: 1292, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.151 [-0.933, 1.189], mean_best_reward: --\n",
      " 25881/100000: episode: 1358, duration: 0.015s, episode steps: 16, steps per second: 1090, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.095 [-0.986, 1.779], mean_best_reward: --\n",
      " 25900/100000: episode: 1359, duration: 0.017s, episode steps: 19, steps per second: 1115, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.090 [-0.959, 1.465], mean_best_reward: --\n",
      " 25911/100000: episode: 1360, duration: 0.011s, episode steps: 11, steps per second: 986, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.138 [-1.347, 2.284], mean_best_reward: --\n",
      " 25924/100000: episode: 1361, duration: 0.013s, episode steps: 13, steps per second: 982, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.102 [-1.759, 0.954], mean_best_reward: --\n",
      " 26057/100000: episode: 1362, duration: 0.112s, episode steps: 133, steps per second: 1182, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.026 [-1.271, 1.112], mean_best_reward: --\n",
      " 26089/100000: episode: 1363, duration: 0.028s, episode steps: 32, steps per second: 1150, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.068 [-0.637, 1.458], mean_best_reward: --\n",
      " 26108/100000: episode: 1364, duration: 0.017s, episode steps: 19, steps per second: 1120, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.059 [-2.702, 1.783], mean_best_reward: --\n",
      " 26118/100000: episode: 1365, duration: 0.010s, episode steps: 10, steps per second: 972, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.136 [-2.565, 1.556], mean_best_reward: --\n",
      " 26135/100000: episode: 1366, duration: 0.016s, episode steps: 17, steps per second: 1091, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.111 [-0.789, 1.215], mean_best_reward: --\n",
      " 26149/100000: episode: 1367, duration: 0.013s, episode steps: 14, steps per second: 1052, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.093 [-1.140, 1.971], mean_best_reward: --\n",
      " 26219/100000: episode: 1368, duration: 0.055s, episode steps: 70, steps per second: 1273, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.081 [-0.737, 1.193], mean_best_reward: --\n",
      " 26260/100000: episode: 1369, duration: 0.033s, episode steps: 41, steps per second: 1236, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.080 [-0.945, 1.313], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26297/100000: episode: 1370, duration: 0.034s, episode steps: 37, steps per second: 1080, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.123 [-1.114, 1.146], mean_best_reward: --\n",
      " 26317/100000: episode: 1371, duration: 0.022s, episode steps: 20, steps per second: 927, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.040 [-2.502, 1.596], mean_best_reward: --\n",
      " 26349/100000: episode: 1372, duration: 0.027s, episode steps: 32, steps per second: 1205, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.680, 1.245], mean_best_reward: --\n",
      " 26385/100000: episode: 1373, duration: 0.030s, episode steps: 36, steps per second: 1217, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.038 [-0.608, 0.927], mean_best_reward: --\n",
      " 26396/100000: episode: 1374, duration: 0.011s, episode steps: 11, steps per second: 994, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.136 [-1.405, 2.327], mean_best_reward: --\n",
      " 26408/100000: episode: 1375, duration: 0.012s, episode steps: 12, steps per second: 1033, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.113 [-1.583, 2.592], mean_best_reward: --\n",
      " 26429/100000: episode: 1376, duration: 0.018s, episode steps: 21, steps per second: 1138, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.081 [-0.618, 0.959], mean_best_reward: --\n",
      " 26444/100000: episode: 1377, duration: 0.014s, episode steps: 15, steps per second: 1064, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.109 [-1.068, 0.617], mean_best_reward: --\n",
      " 26466/100000: episode: 1378, duration: 0.019s, episode steps: 22, steps per second: 1136, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.050 [-2.037, 1.350], mean_best_reward: --\n",
      " 26484/100000: episode: 1379, duration: 0.016s, episode steps: 18, steps per second: 1103, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.103 [-0.565, 1.263], mean_best_reward: --\n",
      " 26495/100000: episode: 1380, duration: 0.011s, episode steps: 11, steps per second: 1007, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.136 [-1.782, 0.948], mean_best_reward: --\n",
      " 26510/100000: episode: 1381, duration: 0.014s, episode steps: 15, steps per second: 1067, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.076 [-1.393, 2.232], mean_best_reward: --\n",
      " 26527/100000: episode: 1382, duration: 0.022s, episode steps: 17, steps per second: 767, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.824 [0.000, 1.000], mean observation: -0.061 [-3.159, 2.117], mean_best_reward: --\n",
      " 26559/100000: episode: 1383, duration: 0.027s, episode steps: 32, steps per second: 1207, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.061 [-0.630, 1.337], mean_best_reward: --\n",
      " 26589/100000: episode: 1384, duration: 0.025s, episode steps: 30, steps per second: 1221, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.042 [-1.699, 0.982], mean_best_reward: --\n",
      " 26600/100000: episode: 1385, duration: 0.011s, episode steps: 11, steps per second: 1002, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.119 [-1.767, 0.964], mean_best_reward: --\n",
      " 26646/100000: episode: 1386, duration: 0.038s, episode steps: 46, steps per second: 1222, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.100 [-0.875, 0.620], mean_best_reward: --\n",
      " 26667/100000: episode: 1387, duration: 0.018s, episode steps: 21, steps per second: 1144, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.052 [-1.553, 2.372], mean_best_reward: --\n",
      " 26698/100000: episode: 1388, duration: 0.026s, episode steps: 31, steps per second: 1205, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.043 [-0.611, 1.189], mean_best_reward: --\n",
      " 26715/100000: episode: 1389, duration: 0.016s, episode steps: 17, steps per second: 1086, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.062 [-1.704, 2.637], mean_best_reward: --\n",
      " 26733/100000: episode: 1390, duration: 0.016s, episode steps: 18, steps per second: 1115, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.095 [-1.159, 2.002], mean_best_reward: --\n",
      " 26780/100000: episode: 1391, duration: 0.043s, episode steps: 47, steps per second: 1084, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.097 [-0.445, 0.963], mean_best_reward: --\n",
      " 26802/100000: episode: 1392, duration: 0.019s, episode steps: 22, steps per second: 1150, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.077 [-0.814, 1.485], mean_best_reward: --\n",
      " 26817/100000: episode: 1393, duration: 0.014s, episode steps: 15, steps per second: 1077, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.079 [-1.401, 2.342], mean_best_reward: --\n",
      " 26849/100000: episode: 1394, duration: 0.027s, episode steps: 32, steps per second: 1204, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.023 [-0.935, 1.476], mean_best_reward: --\n",
      " 26876/100000: episode: 1395, duration: 0.023s, episode steps: 27, steps per second: 1194, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.071 [-0.572, 0.988], mean_best_reward: --\n",
      " 26890/100000: episode: 1396, duration: 0.013s, episode steps: 14, steps per second: 1070, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.088 [-1.919, 2.991], mean_best_reward: --\n",
      " 26901/100000: episode: 1397, duration: 0.011s, episode steps: 11, steps per second: 1018, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.143 [-1.168, 2.001], mean_best_reward: --\n",
      " 26924/100000: episode: 1398, duration: 0.020s, episode steps: 23, steps per second: 1154, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.095 [-0.795, 1.646], mean_best_reward: --\n",
      " 26937/100000: episode: 1399, duration: 0.013s, episode steps: 13, steps per second: 1037, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.114 [-0.772, 1.403], mean_best_reward: --\n",
      " 26960/100000: episode: 1400, duration: 0.020s, episode steps: 23, steps per second: 1165, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.022 [-2.175, 1.420], mean_best_reward: --\n",
      " 26969/100000: episode: 1401, duration: 0.010s, episode steps: 9, steps per second: 894, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.799, 1.807], mean_best_reward: 61.500000\n",
      " 26979/100000: episode: 1402, duration: 0.010s, episode steps: 10, steps per second: 958, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.139 [-1.186, 2.046], mean_best_reward: --\n",
      " 27043/100000: episode: 1403, duration: 0.061s, episode steps: 64, steps per second: 1056, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.093 [-0.906, 0.463], mean_best_reward: --\n",
      " 27060/100000: episode: 1404, duration: 0.015s, episode steps: 17, steps per second: 1108, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.106 [-0.947, 1.922], mean_best_reward: --\n",
      " 27079/100000: episode: 1405, duration: 0.017s, episode steps: 19, steps per second: 1094, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.097 [-0.442, 1.112], mean_best_reward: --\n",
      " 27093/100000: episode: 1406, duration: 0.014s, episode steps: 14, steps per second: 1037, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.100 [-0.948, 1.757], mean_best_reward: --\n",
      " 27136/100000: episode: 1407, duration: 0.035s, episode steps: 43, steps per second: 1238, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.128 [-1.187, 0.585], mean_best_reward: --\n",
      " 27147/100000: episode: 1408, duration: 0.011s, episode steps: 11, steps per second: 990, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.093 [-0.827, 1.455], mean_best_reward: --\n",
      " 27210/100000: episode: 1409, duration: 0.049s, episode steps: 63, steps per second: 1277, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.101 [-1.016, 0.574], mean_best_reward: --\n",
      " 27236/100000: episode: 1410, duration: 0.022s, episode steps: 26, steps per second: 1164, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.019 [-0.814, 1.181], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27283/100000: episode: 1411, duration: 0.039s, episode steps: 47, steps per second: 1203, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: -0.038 [-1.706, 0.976], mean_best_reward: --\n",
      " 27299/100000: episode: 1412, duration: 0.018s, episode steps: 16, steps per second: 883, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.078 [-0.966, 1.686], mean_best_reward: --\n",
      " 27322/100000: episode: 1413, duration: 0.020s, episode steps: 23, steps per second: 1159, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.110 [-0.433, 1.094], mean_best_reward: --\n",
      " 27337/100000: episode: 1414, duration: 0.014s, episode steps: 15, steps per second: 1090, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.111 [-1.302, 0.571], mean_best_reward: --\n",
      " 27367/100000: episode: 1415, duration: 0.026s, episode steps: 30, steps per second: 1171, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.086 [-0.563, 1.451], mean_best_reward: --\n",
      " 27378/100000: episode: 1416, duration: 0.011s, episode steps: 11, steps per second: 992, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.119 [-1.377, 2.197], mean_best_reward: --\n",
      " 27397/100000: episode: 1417, duration: 0.017s, episode steps: 19, steps per second: 1087, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.081 [-0.784, 1.393], mean_best_reward: --\n",
      " 27412/100000: episode: 1418, duration: 0.014s, episode steps: 15, steps per second: 1109, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.059 [-1.185, 1.784], mean_best_reward: --\n",
      " 27468/100000: episode: 1419, duration: 0.044s, episode steps: 56, steps per second: 1283, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.075 [-0.668, 0.912], mean_best_reward: --\n",
      " 27524/100000: episode: 1420, duration: 0.052s, episode steps: 56, steps per second: 1070, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.164 [-1.019, 0.725], mean_best_reward: --\n",
      " 27582/100000: episode: 1421, duration: 0.048s, episode steps: 58, steps per second: 1221, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.061 [-0.765, 1.467], mean_best_reward: --\n",
      " 27598/100000: episode: 1422, duration: 0.014s, episode steps: 16, steps per second: 1112, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.097 [-0.616, 1.101], mean_best_reward: --\n",
      " 27609/100000: episode: 1423, duration: 0.011s, episode steps: 11, steps per second: 1024, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.123 [-0.973, 1.791], mean_best_reward: --\n",
      " 27629/100000: episode: 1424, duration: 0.018s, episode steps: 20, steps per second: 1141, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.112 [-0.376, 0.904], mean_best_reward: --\n",
      " 27646/100000: episode: 1425, duration: 0.015s, episode steps: 17, steps per second: 1119, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.089 [-0.961, 1.825], mean_best_reward: --\n",
      " 27661/100000: episode: 1426, duration: 0.014s, episode steps: 15, steps per second: 1088, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.074 [-0.816, 1.303], mean_best_reward: --\n",
      " 27684/100000: episode: 1427, duration: 0.019s, episode steps: 23, steps per second: 1182, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.130 [-0.360, 0.975], mean_best_reward: --\n",
      " 27746/100000: episode: 1428, duration: 0.048s, episode steps: 62, steps per second: 1284, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.018 [-0.920, 0.794], mean_best_reward: --\n",
      " 27767/100000: episode: 1429, duration: 0.022s, episode steps: 21, steps per second: 941, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.100 [-0.399, 0.920], mean_best_reward: --\n",
      " 27780/100000: episode: 1430, duration: 0.017s, episode steps: 13, steps per second: 759, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.120 [-0.576, 1.234], mean_best_reward: --\n",
      " 27810/100000: episode: 1431, duration: 0.025s, episode steps: 30, steps per second: 1206, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.564, 1.224], mean_best_reward: --\n",
      " 27822/100000: episode: 1432, duration: 0.012s, episode steps: 12, steps per second: 1018, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.109 [-0.984, 1.756], mean_best_reward: --\n",
      " 27839/100000: episode: 1433, duration: 0.015s, episode steps: 17, steps per second: 1110, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.116 [-0.547, 0.947], mean_best_reward: --\n",
      " 27858/100000: episode: 1434, duration: 0.017s, episode steps: 19, steps per second: 1134, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.065 [-0.616, 1.102], mean_best_reward: --\n",
      " 27870/100000: episode: 1435, duration: 0.011s, episode steps: 12, steps per second: 1066, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.109 [-0.749, 1.497], mean_best_reward: --\n",
      " 27898/100000: episode: 1436, duration: 0.023s, episode steps: 28, steps per second: 1205, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.126 [-1.232, 0.601], mean_best_reward: --\n",
      " 27945/100000: episode: 1437, duration: 0.038s, episode steps: 47, steps per second: 1250, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.089 [-0.601, 1.088], mean_best_reward: --\n",
      " 27971/100000: episode: 1438, duration: 0.022s, episode steps: 26, steps per second: 1192, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.087 [-0.630, 1.444], mean_best_reward: --\n",
      " 27985/100000: episode: 1439, duration: 0.013s, episode steps: 14, steps per second: 1075, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.108 [-0.613, 1.231], mean_best_reward: --\n",
      " 27997/100000: episode: 1440, duration: 0.016s, episode steps: 12, steps per second: 742, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.106 [-1.018, 1.626], mean_best_reward: --\n",
      " 28007/100000: episode: 1441, duration: 0.012s, episode steps: 10, steps per second: 835, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.126 [-1.172, 1.888], mean_best_reward: --\n",
      " 28057/100000: episode: 1442, duration: 0.040s, episode steps: 50, steps per second: 1246, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.003 [-0.616, 1.081], mean_best_reward: --\n",
      " 28075/100000: episode: 1443, duration: 0.016s, episode steps: 18, steps per second: 1159, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.085 [-1.572, 0.843], mean_best_reward: --\n",
      " 28087/100000: episode: 1444, duration: 0.012s, episode steps: 12, steps per second: 1025, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.811, 1.209], mean_best_reward: --\n",
      " 28099/100000: episode: 1445, duration: 0.012s, episode steps: 12, steps per second: 1006, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.100 [-2.178, 1.347], mean_best_reward: --\n",
      " 28110/100000: episode: 1446, duration: 0.011s, episode steps: 11, steps per second: 976, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.101 [-0.783, 1.328], mean_best_reward: --\n",
      " 28140/100000: episode: 1447, duration: 0.026s, episode steps: 30, steps per second: 1148, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.002 [-1.160, 1.510], mean_best_reward: --\n",
      " 28167/100000: episode: 1448, duration: 0.022s, episode steps: 27, steps per second: 1203, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.048 [-0.814, 1.265], mean_best_reward: --\n",
      " 28203/100000: episode: 1449, duration: 0.031s, episode steps: 36, steps per second: 1147, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.045 [-0.876, 1.432], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28223/100000: episode: 1450, duration: 0.019s, episode steps: 20, steps per second: 1040, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.831, 1.228], mean_best_reward: --\n",
      " 28248/100000: episode: 1451, duration: 0.026s, episode steps: 25, steps per second: 976, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.095 [-0.432, 1.275], mean_best_reward: 69.500000\n",
      " 28287/100000: episode: 1452, duration: 0.031s, episode steps: 39, steps per second: 1244, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.410 [0.000, 1.000], mean observation: -0.063 [-1.294, 1.564], mean_best_reward: --\n",
      " 28323/100000: episode: 1453, duration: 0.029s, episode steps: 36, steps per second: 1233, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.639 [0.000, 1.000], mean observation: 0.020 [-2.804, 2.174], mean_best_reward: --\n",
      " 28344/100000: episode: 1454, duration: 0.018s, episode steps: 21, steps per second: 1150, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.071 [-0.813, 1.493], mean_best_reward: --\n",
      " 28373/100000: episode: 1455, duration: 0.024s, episode steps: 29, steps per second: 1202, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.049 [-1.020, 1.823], mean_best_reward: --\n",
      " 28386/100000: episode: 1456, duration: 0.012s, episode steps: 13, steps per second: 1062, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.105 [-1.956, 1.181], mean_best_reward: --\n",
      " 28400/100000: episode: 1457, duration: 0.013s, episode steps: 14, steps per second: 1062, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.100 [-1.324, 2.099], mean_best_reward: --\n",
      " 28412/100000: episode: 1458, duration: 0.011s, episode steps: 12, steps per second: 1045, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.106 [-1.605, 2.532], mean_best_reward: --\n",
      " 28421/100000: episode: 1459, duration: 0.009s, episode steps: 9, steps per second: 952, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.139 [-1.354, 2.247], mean_best_reward: --\n",
      " 28452/100000: episode: 1460, duration: 0.028s, episode steps: 31, steps per second: 1093, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.065 [-0.757, 1.087], mean_best_reward: --\n",
      " 28472/100000: episode: 1461, duration: 0.022s, episode steps: 20, steps per second: 894, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.080 [-1.649, 0.815], mean_best_reward: --\n",
      " 28485/100000: episode: 1462, duration: 0.012s, episode steps: 13, steps per second: 1069, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.107 [-0.814, 1.464], mean_best_reward: --\n",
      " 28495/100000: episode: 1463, duration: 0.010s, episode steps: 10, steps per second: 1012, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-2.005, 3.078], mean_best_reward: --\n",
      " 28510/100000: episode: 1464, duration: 0.014s, episode steps: 15, steps per second: 1097, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.084 [-1.341, 2.227], mean_best_reward: --\n",
      " 28521/100000: episode: 1465, duration: 0.011s, episode steps: 11, steps per second: 1019, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.102 [-1.377, 2.208], mean_best_reward: --\n",
      " 28539/100000: episode: 1466, duration: 0.016s, episode steps: 18, steps per second: 1115, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.077 [-0.794, 1.528], mean_best_reward: --\n",
      " 28548/100000: episode: 1467, duration: 0.010s, episode steps: 9, steps per second: 942, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.156 [-1.349, 2.288], mean_best_reward: --\n",
      " 28558/100000: episode: 1468, duration: 0.010s, episode steps: 10, steps per second: 992, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.098 [-1.026, 1.658], mean_best_reward: --\n",
      " 28575/100000: episode: 1469, duration: 0.015s, episode steps: 17, steps per second: 1130, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.057 [-1.204, 1.773], mean_best_reward: --\n",
      " 28590/100000: episode: 1470, duration: 0.014s, episode steps: 15, steps per second: 1111, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.102 [-1.624, 0.840], mean_best_reward: --\n",
      " 28618/100000: episode: 1471, duration: 0.023s, episode steps: 28, steps per second: 1218, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.036 [-1.764, 2.052], mean_best_reward: --\n",
      " 28634/100000: episode: 1472, duration: 0.014s, episode steps: 16, steps per second: 1123, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.103 [-1.649, 0.792], mean_best_reward: --\n",
      " 28652/100000: episode: 1473, duration: 0.016s, episode steps: 18, steps per second: 1102, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.080 [-2.453, 1.521], mean_best_reward: --\n",
      " 28670/100000: episode: 1474, duration: 0.019s, episode steps: 18, steps per second: 934, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.078 [-1.195, 1.998], mean_best_reward: --\n",
      " 28681/100000: episode: 1475, duration: 0.013s, episode steps: 11, steps per second: 853, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.122 [-1.779, 2.771], mean_best_reward: --\n",
      " 28691/100000: episode: 1476, duration: 0.010s, episode steps: 10, steps per second: 1018, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.144 [-1.978, 1.149], mean_best_reward: --\n",
      " 28699/100000: episode: 1477, duration: 0.009s, episode steps: 8, steps per second: 918, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [-1.561, 2.576], mean_best_reward: --\n",
      " 28757/100000: episode: 1478, duration: 0.045s, episode steps: 58, steps per second: 1282, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.066 [-0.573, 0.954], mean_best_reward: --\n",
      " 28777/100000: episode: 1479, duration: 0.017s, episode steps: 20, steps per second: 1164, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.064 [-1.749, 2.659], mean_best_reward: --\n",
      " 28837/100000: episode: 1480, duration: 0.047s, episode steps: 60, steps per second: 1283, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.106 [-0.835, 0.934], mean_best_reward: --\n",
      " 28859/100000: episode: 1481, duration: 0.019s, episode steps: 22, steps per second: 1166, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-0.971, 0.577], mean_best_reward: --\n",
      " 28878/100000: episode: 1482, duration: 0.017s, episode steps: 19, steps per second: 1144, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.080 [-0.618, 1.364], mean_best_reward: --\n",
      " 28889/100000: episode: 1483, duration: 0.028s, episode steps: 11, steps per second: 392, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.125 [-1.743, 2.728], mean_best_reward: --\n",
      " 28911/100000: episode: 1484, duration: 0.030s, episode steps: 22, steps per second: 739, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.134 [-1.038, 0.358], mean_best_reward: --\n",
      " 28927/100000: episode: 1485, duration: 0.019s, episode steps: 16, steps per second: 853, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.084 [-1.608, 2.515], mean_best_reward: --\n",
      " 28948/100000: episode: 1486, duration: 0.035s, episode steps: 21, steps per second: 602, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.088 [-1.008, 2.018], mean_best_reward: --\n",
      " 28958/100000: episode: 1487, duration: 0.020s, episode steps: 10, steps per second: 509, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.939, 3.070], mean_best_reward: --\n",
      " 28972/100000: episode: 1488, duration: 0.019s, episode steps: 14, steps per second: 724, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.092 [-1.137, 1.928], mean_best_reward: --\n",
      " 28983/100000: episode: 1489, duration: 0.018s, episode steps: 11, steps per second: 618, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.095 [-1.393, 2.210], mean_best_reward: --\n",
      " 28996/100000: episode: 1490, duration: 0.017s, episode steps: 13, steps per second: 761, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.116 [-1.486, 0.799], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29016/100000: episode: 1491, duration: 0.057s, episode steps: 20, steps per second: 348, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.088 [-0.813, 1.279], mean_best_reward: --\n",
      " 29036/100000: episode: 1492, duration: 0.040s, episode steps: 20, steps per second: 500, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.086 [-1.144, 2.027], mean_best_reward: --\n",
      " 29048/100000: episode: 1493, duration: 0.024s, episode steps: 12, steps per second: 500, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.102 [-1.987, 3.023], mean_best_reward: --\n",
      " 29061/100000: episode: 1494, duration: 0.023s, episode steps: 13, steps per second: 568, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.094 [-1.755, 2.681], mean_best_reward: --\n",
      " 29076/100000: episode: 1495, duration: 0.020s, episode steps: 15, steps per second: 754, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.084 [-1.417, 0.797], mean_best_reward: --\n",
      " 29086/100000: episode: 1496, duration: 0.012s, episode steps: 10, steps per second: 823, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.571, 2.499], mean_best_reward: --\n",
      " 29099/100000: episode: 1497, duration: 0.013s, episode steps: 13, steps per second: 975, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.081 [-1.786, 2.750], mean_best_reward: --\n",
      " 29112/100000: episode: 1498, duration: 0.012s, episode steps: 13, steps per second: 1065, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.105 [-1.396, 2.221], mean_best_reward: --\n",
      " 29122/100000: episode: 1499, duration: 0.010s, episode steps: 10, steps per second: 1007, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.140 [-1.150, 2.080], mean_best_reward: --\n",
      " 29134/100000: episode: 1500, duration: 0.011s, episode steps: 12, steps per second: 1078, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.104 [-1.138, 1.953], mean_best_reward: --\n",
      " 29148/100000: episode: 1501, duration: 0.013s, episode steps: 14, steps per second: 1046, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.109 [-0.954, 1.643], mean_best_reward: 79.000000\n",
      " 29160/100000: episode: 1502, duration: 0.012s, episode steps: 12, steps per second: 1031, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.094 [-0.826, 1.421], mean_best_reward: --\n",
      " 29173/100000: episode: 1503, duration: 0.016s, episode steps: 13, steps per second: 803, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.126 [-1.344, 2.408], mean_best_reward: --\n",
      " 29187/100000: episode: 1504, duration: 0.016s, episode steps: 14, steps per second: 887, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.115 [-0.550, 1.190], mean_best_reward: --\n",
      " 29198/100000: episode: 1505, duration: 0.011s, episode steps: 11, steps per second: 970, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.124 [-1.589, 2.449], mean_best_reward: --\n",
      " 29210/100000: episode: 1506, duration: 0.012s, episode steps: 12, steps per second: 1002, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.122 [-1.134, 1.967], mean_best_reward: --\n",
      " 29225/100000: episode: 1507, duration: 0.015s, episode steps: 15, steps per second: 980, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.107 [-0.985, 1.686], mean_best_reward: --\n",
      " 29250/100000: episode: 1508, duration: 0.022s, episode steps: 25, steps per second: 1118, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.092 [-0.413, 1.132], mean_best_reward: --\n",
      " 29280/100000: episode: 1509, duration: 0.027s, episode steps: 30, steps per second: 1091, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.045 [-0.793, 1.350], mean_best_reward: --\n",
      " 29358/100000: episode: 1510, duration: 0.068s, episode steps: 78, steps per second: 1140, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.168 [-1.084, 0.556], mean_best_reward: --\n",
      " 29373/100000: episode: 1511, duration: 0.015s, episode steps: 15, steps per second: 998, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.080 [-1.332, 2.123], mean_best_reward: --\n",
      " 29388/100000: episode: 1512, duration: 0.018s, episode steps: 15, steps per second: 843, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.111 [-0.757, 1.505], mean_best_reward: --\n",
      " 29397/100000: episode: 1513, duration: 0.016s, episode steps: 9, steps per second: 566, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.142 [-1.389, 2.285], mean_best_reward: --\n",
      " 29469/100000: episode: 1514, duration: 0.058s, episode steps: 72, steps per second: 1252, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.039 [-0.575, 1.001], mean_best_reward: --\n",
      " 29480/100000: episode: 1515, duration: 0.011s, episode steps: 11, steps per second: 1001, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.133 [-1.540, 2.482], mean_best_reward: --\n",
      " 29491/100000: episode: 1516, duration: 0.011s, episode steps: 11, steps per second: 995, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.109 [-0.972, 1.667], mean_best_reward: --\n",
      " 29528/100000: episode: 1517, duration: 0.031s, episode steps: 37, steps per second: 1203, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.059 [-0.634, 1.600], mean_best_reward: --\n",
      " 29597/100000: episode: 1518, duration: 0.054s, episode steps: 69, steps per second: 1283, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.085 [-1.492, 1.486], mean_best_reward: --\n",
      " 29634/100000: episode: 1519, duration: 0.035s, episode steps: 37, steps per second: 1045, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.081 [-0.651, 1.684], mean_best_reward: --\n",
      " 29646/100000: episode: 1520, duration: 0.012s, episode steps: 12, steps per second: 1018, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.116 [-0.769, 1.480], mean_best_reward: --\n",
      " 29665/100000: episode: 1521, duration: 0.016s, episode steps: 19, steps per second: 1155, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.044 [-1.192, 1.873], mean_best_reward: --\n",
      " 29704/100000: episode: 1522, duration: 0.032s, episode steps: 39, steps per second: 1235, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.067 [-0.432, 0.631], mean_best_reward: --\n",
      " 29715/100000: episode: 1523, duration: 0.011s, episode steps: 11, steps per second: 996, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.111 [-1.760, 2.821], mean_best_reward: --\n",
      " 29749/100000: episode: 1524, duration: 0.030s, episode steps: 34, steps per second: 1133, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.082 [-0.477, 1.532], mean_best_reward: --\n",
      " 29823/100000: episode: 1525, duration: 0.056s, episode steps: 74, steps per second: 1322, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.041 [-1.149, 0.808], mean_best_reward: --\n",
      " 29847/100000: episode: 1526, duration: 0.021s, episode steps: 24, steps per second: 1146, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.070 [-0.770, 1.269], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29869/100000: episode: 1527, duration: 0.020s, episode steps: 22, steps per second: 1106, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.769, 1.351], mean_best_reward: --\n",
      " 29918/100000: episode: 1528, duration: 0.048s, episode steps: 49, steps per second: 1027, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.050 [-0.593, 1.567], mean_best_reward: --\n",
      " 29942/100000: episode: 1529, duration: 0.021s, episode steps: 24, steps per second: 1164, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.067 [-0.785, 1.560], mean_best_reward: --\n",
      " 29983/100000: episode: 1530, duration: 0.034s, episode steps: 41, steps per second: 1197, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.112 [-0.373, 0.900], mean_best_reward: --\n",
      " 30031/100000: episode: 1531, duration: 0.041s, episode steps: 48, steps per second: 1182, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.106 [-1.125, 0.504], mean_best_reward: --\n",
      " 30051/100000: episode: 1532, duration: 0.018s, episode steps: 20, steps per second: 1109, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.073 [-0.779, 1.319], mean_best_reward: --\n",
      " 30069/100000: episode: 1533, duration: 0.016s, episode steps: 18, steps per second: 1092, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.081 [-0.603, 1.284], mean_best_reward: --\n",
      " 30096/100000: episode: 1534, duration: 0.026s, episode steps: 27, steps per second: 1040, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.091 [-0.768, 1.149], mean_best_reward: --\n",
      " 30111/100000: episode: 1535, duration: 0.017s, episode steps: 15, steps per second: 894, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.083 [-1.559, 2.410], mean_best_reward: --\n",
      " 30122/100000: episode: 1536, duration: 0.012s, episode steps: 11, steps per second: 916, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.111 [-1.145, 1.777], mean_best_reward: --\n",
      " 30140/100000: episode: 1537, duration: 0.018s, episode steps: 18, steps per second: 1026, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.107 [-0.899, 0.432], mean_best_reward: --\n",
      " 30232/100000: episode: 1538, duration: 0.070s, episode steps: 92, steps per second: 1306, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.134 [-1.010, 0.837], mean_best_reward: --\n",
      " 30248/100000: episode: 1539, duration: 0.015s, episode steps: 16, steps per second: 1081, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.101 [-0.771, 1.323], mean_best_reward: --\n",
      " 30260/100000: episode: 1540, duration: 0.012s, episode steps: 12, steps per second: 1004, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.113 [-1.203, 0.799], mean_best_reward: --\n",
      " 30312/100000: episode: 1541, duration: 0.041s, episode steps: 52, steps per second: 1274, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.179 [-1.288, 0.722], mean_best_reward: --\n",
      " 30336/100000: episode: 1542, duration: 0.024s, episode steps: 24, steps per second: 993, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.030 [-1.229, 1.991], mean_best_reward: --\n",
      " 30347/100000: episode: 1543, duration: 0.012s, episode steps: 11, steps per second: 938, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.114 [-1.190, 1.940], mean_best_reward: --\n",
      " 30370/100000: episode: 1544, duration: 0.022s, episode steps: 23, steps per second: 1066, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.066 [-0.582, 1.352], mean_best_reward: --\n",
      " 30395/100000: episode: 1545, duration: 0.021s, episode steps: 25, steps per second: 1169, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.094 [-0.459, 1.360], mean_best_reward: --\n",
      " 30412/100000: episode: 1546, duration: 0.016s, episode steps: 17, steps per second: 1093, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.101 [-0.743, 1.319], mean_best_reward: --\n",
      " 30445/100000: episode: 1547, duration: 0.028s, episode steps: 33, steps per second: 1158, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.086 [-0.613, 0.970], mean_best_reward: --\n",
      " 30457/100000: episode: 1548, duration: 0.012s, episode steps: 12, steps per second: 1034, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.106 [-1.394, 2.290], mean_best_reward: --\n",
      " 30465/100000: episode: 1549, duration: 0.009s, episode steps: 8, steps per second: 894, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.127 [-1.398, 2.198], mean_best_reward: --\n",
      " 30515/100000: episode: 1550, duration: 0.040s, episode steps: 50, steps per second: 1248, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: 0.102 [-3.042, 2.841], mean_best_reward: --\n",
      " 30541/100000: episode: 1551, duration: 0.022s, episode steps: 26, steps per second: 1157, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-1.242, 0.606], mean_best_reward: 94.000000\n",
      " 30574/100000: episode: 1552, duration: 0.034s, episode steps: 33, steps per second: 983, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.115 [-0.429, 0.938], mean_best_reward: --\n",
      " 30589/100000: episode: 1553, duration: 0.017s, episode steps: 15, steps per second: 909, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.091 [-1.849, 1.015], mean_best_reward: --\n",
      " 30619/100000: episode: 1554, duration: 0.025s, episode steps: 30, steps per second: 1201, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.115 [-0.427, 1.512], mean_best_reward: --\n",
      " 30629/100000: episode: 1555, duration: 0.010s, episode steps: 10, steps per second: 996, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.115 [-2.075, 1.208], mean_best_reward: --\n",
      " 30656/100000: episode: 1556, duration: 0.023s, episode steps: 27, steps per second: 1194, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.038 [-0.629, 1.056], mean_best_reward: --\n",
      " 30685/100000: episode: 1557, duration: 0.024s, episode steps: 29, steps per second: 1203, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.127 [-1.249, 0.352], mean_best_reward: --\n",
      " 30696/100000: episode: 1558, duration: 0.011s, episode steps: 11, steps per second: 1011, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.108 [-2.499, 1.610], mean_best_reward: --\n",
      " 30778/100000: episode: 1559, duration: 0.114s, episode steps: 82, steps per second: 717, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.561 [0.000, 1.000], mean observation: 0.216 [-1.673, 2.032], mean_best_reward: --\n",
      " 30788/100000: episode: 1560, duration: 0.011s, episode steps: 10, steps per second: 883, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.145 [-1.134, 2.001], mean_best_reward: --\n",
      " 30826/100000: episode: 1561, duration: 0.035s, episode steps: 38, steps per second: 1081, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.107 [-0.363, 1.127], mean_best_reward: --\n",
      " 30854/100000: episode: 1562, duration: 0.026s, episode steps: 28, steps per second: 1097, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.091 [-0.600, 1.263], mean_best_reward: --\n",
      " 30864/100000: episode: 1563, duration: 0.012s, episode steps: 10, steps per second: 827, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.118 [-2.538, 1.558], mean_best_reward: --\n",
      " 30892/100000: episode: 1564, duration: 0.024s, episode steps: 28, steps per second: 1151, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.117 [-0.356, 0.794], mean_best_reward: --\n",
      " 30919/100000: episode: 1565, duration: 0.025s, episode steps: 27, steps per second: 1063, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.095 [-0.603, 1.013], mean_best_reward: --\n",
      " 30935/100000: episode: 1566, duration: 0.017s, episode steps: 16, steps per second: 942, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.634, 1.119], mean_best_reward: --\n",
      " 30945/100000: episode: 1567, duration: 0.012s, episode steps: 10, steps per second: 840, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.122 [-1.572, 2.542], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30992/100000: episode: 1568, duration: 0.046s, episode steps: 47, steps per second: 1022, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.158 [-0.894, 0.411], mean_best_reward: --\n",
      " 31039/100000: episode: 1569, duration: 0.054s, episode steps: 47, steps per second: 873, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.099 [-0.581, 1.169], mean_best_reward: --\n",
      " 31067/100000: episode: 1570, duration: 0.024s, episode steps: 28, steps per second: 1173, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.101 [-0.927, 0.368], mean_best_reward: --\n",
      " 31093/100000: episode: 1571, duration: 0.022s, episode steps: 26, steps per second: 1163, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.031 [-0.794, 1.347], mean_best_reward: --\n",
      " 31167/100000: episode: 1572, duration: 0.059s, episode steps: 74, steps per second: 1244, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.116 [-1.395, 0.632], mean_best_reward: --\n",
      " 31210/100000: episode: 1573, duration: 0.035s, episode steps: 43, steps per second: 1222, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.100 [-0.524, 1.028], mean_best_reward: --\n",
      " 31222/100000: episode: 1574, duration: 0.015s, episode steps: 12, steps per second: 814, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.118 [-0.800, 1.518], mean_best_reward: --\n",
      " 31249/100000: episode: 1575, duration: 0.023s, episode steps: 27, steps per second: 1177, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.071 [-1.008, 0.609], mean_best_reward: --\n",
      " 31306/100000: episode: 1576, duration: 0.045s, episode steps: 57, steps per second: 1266, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.131 [-1.657, 0.917], mean_best_reward: --\n",
      " 31319/100000: episode: 1577, duration: 0.012s, episode steps: 13, steps per second: 1048, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.089 [-1.744, 1.133], mean_best_reward: --\n",
      " 31361/100000: episode: 1578, duration: 0.035s, episode steps: 42, steps per second: 1207, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.150 [-1.000, 0.576], mean_best_reward: --\n",
      " 31376/100000: episode: 1579, duration: 0.014s, episode steps: 15, steps per second: 1050, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.068 [-1.396, 1.023], mean_best_reward: --\n",
      " 31428/100000: episode: 1580, duration: 0.083s, episode steps: 52, steps per second: 625, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.076, 0.398], mean_best_reward: --\n",
      " 31441/100000: episode: 1581, duration: 0.022s, episode steps: 13, steps per second: 581, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.092 [-1.377, 2.127], mean_best_reward: --\n",
      " 31467/100000: episode: 1582, duration: 0.031s, episode steps: 26, steps per second: 845, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.097 [-0.586, 1.063], mean_best_reward: --\n",
      " 31496/100000: episode: 1583, duration: 0.051s, episode steps: 29, steps per second: 569, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.064 [-0.583, 1.125], mean_best_reward: --\n",
      " 31517/100000: episode: 1584, duration: 0.022s, episode steps: 21, steps per second: 950, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.063 [-0.776, 1.196], mean_best_reward: --\n",
      " 31554/100000: episode: 1585, duration: 0.031s, episode steps: 37, steps per second: 1204, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.074 [-0.632, 0.920], mean_best_reward: --\n",
      " 31576/100000: episode: 1586, duration: 0.027s, episode steps: 22, steps per second: 821, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.075 [-1.165, 0.767], mean_best_reward: --\n",
      " 31605/100000: episode: 1587, duration: 0.070s, episode steps: 29, steps per second: 415, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.068 [-0.551, 0.937], mean_best_reward: --\n",
      " 31619/100000: episode: 1588, duration: 0.028s, episode steps: 14, steps per second: 509, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.108 [-1.295, 0.770], mean_best_reward: --\n",
      " 31651/100000: episode: 1589, duration: 0.043s, episode steps: 32, steps per second: 746, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.063 [-1.740, 0.809], mean_best_reward: --\n",
      " 31735/100000: episode: 1590, duration: 0.102s, episode steps: 84, steps per second: 823, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.215 [-1.557, 1.268], mean_best_reward: --\n",
      " 31747/100000: episode: 1591, duration: 0.013s, episode steps: 12, steps per second: 904, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.106 [-1.604, 2.603], mean_best_reward: --\n",
      " 31760/100000: episode: 1592, duration: 0.012s, episode steps: 13, steps per second: 1067, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.097 [-1.300, 0.781], mean_best_reward: --\n",
      " 31778/100000: episode: 1593, duration: 0.019s, episode steps: 18, steps per second: 937, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.093 [-0.588, 1.143], mean_best_reward: --\n",
      " 31797/100000: episode: 1594, duration: 0.024s, episode steps: 19, steps per second: 797, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.081 [-1.394, 2.373], mean_best_reward: --\n",
      " 31824/100000: episode: 1595, duration: 0.026s, episode steps: 27, steps per second: 1050, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.127 [-0.407, 1.219], mean_best_reward: --\n",
      " 31880/100000: episode: 1596, duration: 0.045s, episode steps: 56, steps per second: 1257, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.556, 1.182], mean_best_reward: --\n",
      " 31924/100000: episode: 1597, duration: 0.038s, episode steps: 44, steps per second: 1156, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.069 [-0.560, 1.089], mean_best_reward: --\n",
      " 31933/100000: episode: 1598, duration: 0.010s, episode steps: 9, steps per second: 931, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.159 [-1.139, 1.972], mean_best_reward: --\n",
      " 31968/100000: episode: 1599, duration: 0.031s, episode steps: 35, steps per second: 1132, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.146 [-0.945, 0.557], mean_best_reward: --\n",
      " 32021/100000: episode: 1600, duration: 0.064s, episode steps: 53, steps per second: 832, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.069 [-0.889, 0.594], mean_best_reward: --\n",
      " 32041/100000: episode: 1601, duration: 0.035s, episode steps: 20, steps per second: 564, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.104 [-0.570, 1.315], mean_best_reward: 98.500000\n",
      " 32087/100000: episode: 1602, duration: 0.063s, episode steps: 46, steps per second: 728, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.070 [-0.952, 0.346], mean_best_reward: --\n",
      " 32109/100000: episode: 1603, duration: 0.047s, episode steps: 22, steps per second: 470, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.046 [-1.348, 0.963], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32177/100000: episode: 1604, duration: 0.100s, episode steps: 68, steps per second: 683, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.030 [-0.570, 1.428], mean_best_reward: --\n",
      " 32193/100000: episode: 1605, duration: 0.023s, episode steps: 16, steps per second: 695, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.065 [-1.030, 1.566], mean_best_reward: --\n",
      " 32208/100000: episode: 1606, duration: 0.032s, episode steps: 15, steps per second: 472, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.109 [-1.882, 0.958], mean_best_reward: --\n",
      " 32222/100000: episode: 1607, duration: 0.028s, episode steps: 14, steps per second: 504, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.119 [-0.802, 1.584], mean_best_reward: --\n",
      " 32249/100000: episode: 1608, duration: 0.039s, episode steps: 27, steps per second: 685, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.072 [-1.210, 0.811], mean_best_reward: --\n",
      " 32271/100000: episode: 1609, duration: 0.042s, episode steps: 22, steps per second: 525, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.068 [-1.603, 0.955], mean_best_reward: --\n",
      " 32408/100000: episode: 1610, duration: 0.123s, episode steps: 137, steps per second: 1112, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.382 [-1.079, 1.974], mean_best_reward: --\n",
      " 32420/100000: episode: 1611, duration: 0.012s, episode steps: 12, steps per second: 1008, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.113 [-0.961, 1.609], mean_best_reward: --\n",
      " 32457/100000: episode: 1612, duration: 0.032s, episode steps: 37, steps per second: 1160, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.118 [-0.633, 0.999], mean_best_reward: --\n",
      " 32494/100000: episode: 1613, duration: 0.031s, episode steps: 37, steps per second: 1188, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.160 [-1.132, 0.348], mean_best_reward: --\n",
      " 32565/100000: episode: 1614, duration: 0.061s, episode steps: 71, steps per second: 1163, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.042 [-1.824, 1.340], mean_best_reward: --\n",
      " 32593/100000: episode: 1615, duration: 0.025s, episode steps: 28, steps per second: 1115, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.012 [-1.538, 1.024], mean_best_reward: --\n",
      " 32675/100000: episode: 1616, duration: 0.080s, episode steps: 82, steps per second: 1030, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.149 [-0.949, 0.531], mean_best_reward: --\n",
      " 32688/100000: episode: 1617, duration: 0.015s, episode steps: 13, steps per second: 896, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.097 [-1.815, 1.197], mean_best_reward: --\n",
      " 32782/100000: episode: 1618, duration: 0.077s, episode steps: 94, steps per second: 1228, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-0.706, 1.136], mean_best_reward: --\n",
      " 32812/100000: episode: 1619, duration: 0.025s, episode steps: 30, steps per second: 1182, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.117 [-0.876, 0.408], mean_best_reward: --\n",
      " 32873/100000: episode: 1620, duration: 0.048s, episode steps: 61, steps per second: 1261, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.003 [-1.217, 0.782], mean_best_reward: --\n",
      " 33013/100000: episode: 1621, duration: 0.124s, episode steps: 140, steps per second: 1127, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.002 [-1.044, 1.021], mean_best_reward: --\n",
      " 33053/100000: episode: 1622, duration: 0.041s, episode steps: 40, steps per second: 984, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.198 [-1.521, 0.841], mean_best_reward: --\n",
      " 33084/100000: episode: 1623, duration: 0.030s, episode steps: 31, steps per second: 1019, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.105 [-0.871, 0.623], mean_best_reward: --\n",
      " 33141/100000: episode: 1624, duration: 0.051s, episode steps: 57, steps per second: 1126, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.237 [-1.483, 0.511], mean_best_reward: --\n",
      " 33185/100000: episode: 1625, duration: 0.046s, episode steps: 44, steps per second: 958, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.095 [-0.988, 0.583], mean_best_reward: --\n",
      " 33226/100000: episode: 1626, duration: 0.054s, episode steps: 41, steps per second: 758, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.022 [-0.806, 1.441], mean_best_reward: --\n",
      " 33274/100000: episode: 1627, duration: 0.051s, episode steps: 48, steps per second: 945, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.109 [-0.399, 0.813], mean_best_reward: --\n",
      " 33313/100000: episode: 1628, duration: 0.034s, episode steps: 39, steps per second: 1141, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.085 [-1.704, 0.644], mean_best_reward: --\n",
      " 33338/100000: episode: 1629, duration: 0.027s, episode steps: 25, steps per second: 930, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.019 [-1.148, 1.606], mean_best_reward: --\n",
      " 33406/100000: episode: 1630, duration: 0.058s, episode steps: 68, steps per second: 1177, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.063 [-1.043, 0.481], mean_best_reward: --\n",
      " 33446/100000: episode: 1631, duration: 0.053s, episode steps: 40, steps per second: 756, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.068 [-1.613, 0.660], mean_best_reward: --\n",
      " 33463/100000: episode: 1632, duration: 0.017s, episode steps: 17, steps per second: 1001, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.073 [-1.377, 0.833], mean_best_reward: --\n",
      " 33500/100000: episode: 1633, duration: 0.030s, episode steps: 37, steps per second: 1235, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.101 [-0.543, 0.948], mean_best_reward: --\n",
      " 33526/100000: episode: 1634, duration: 0.022s, episode steps: 26, steps per second: 1199, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.077 [-0.997, 0.639], mean_best_reward: --\n",
      " 33560/100000: episode: 1635, duration: 0.030s, episode steps: 34, steps per second: 1137, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.136 [-0.595, 0.923], mean_best_reward: --\n",
      " 33581/100000: episode: 1636, duration: 0.020s, episode steps: 21, steps per second: 1052, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.066 [-1.919, 1.039], mean_best_reward: --\n",
      " 33705/100000: episode: 1637, duration: 0.117s, episode steps: 124, steps per second: 1063, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.299 [-1.026, 1.361], mean_best_reward: --\n",
      " 33741/100000: episode: 1638, duration: 0.032s, episode steps: 36, steps per second: 1129, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.013 [-1.997, 1.204], mean_best_reward: --\n",
      " 33751/100000: episode: 1639, duration: 0.010s, episode steps: 10, steps per second: 962, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.149 [-2.268, 1.354], mean_best_reward: --\n",
      " 33785/100000: episode: 1640, duration: 0.027s, episode steps: 34, steps per second: 1278, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.110 [-0.824, 1.219], mean_best_reward: --\n",
      " 33833/100000: episode: 1641, duration: 0.039s, episode steps: 48, steps per second: 1240, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.092 [-1.380, 0.835], mean_best_reward: --\n",
      " 33873/100000: episode: 1642, duration: 0.033s, episode steps: 40, steps per second: 1225, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.114 [-0.940, 0.548], mean_best_reward: --\n",
      " 33895/100000: episode: 1643, duration: 0.024s, episode steps: 22, steps per second: 929, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.039 [-0.834, 1.336], mean_best_reward: --\n",
      " 33910/100000: episode: 1644, duration: 0.015s, episode steps: 15, steps per second: 1025, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.106 [-0.605, 1.183], mean_best_reward: --\n",
      " 33923/100000: episode: 1645, duration: 0.013s, episode steps: 13, steps per second: 1027, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.101 [-0.771, 1.189], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34010/100000: episode: 1646, duration: 0.090s, episode steps: 87, steps per second: 969, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.084 [-1.098, 0.803], mean_best_reward: --\n",
      " 34093/100000: episode: 1647, duration: 0.076s, episode steps: 83, steps per second: 1094, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.340 [-1.902, 0.891], mean_best_reward: --\n",
      " 34104/100000: episode: 1648, duration: 0.013s, episode steps: 11, steps per second: 831, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.108 [-0.810, 1.300], mean_best_reward: --\n",
      " 34122/100000: episode: 1649, duration: 0.027s, episode steps: 18, steps per second: 678, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.071 [-1.171, 0.635], mean_best_reward: --\n",
      " 34242/100000: episode: 1650, duration: 0.116s, episode steps: 120, steps per second: 1031, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.073 [-1.063, 0.803], mean_best_reward: --\n",
      " 34254/100000: episode: 1651, duration: 0.015s, episode steps: 12, steps per second: 788, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.120 [-0.963, 1.683], mean_best_reward: 86.500000\n",
      " 34277/100000: episode: 1652, duration: 0.023s, episode steps: 23, steps per second: 996, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.124 [-0.544, 1.256], mean_best_reward: --\n",
      " 34317/100000: episode: 1653, duration: 0.036s, episode steps: 40, steps per second: 1103, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.025 [-1.264, 0.803], mean_best_reward: --\n",
      " 34339/100000: episode: 1654, duration: 0.021s, episode steps: 22, steps per second: 1037, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.063 [-1.705, 1.140], mean_best_reward: --\n",
      " 34352/100000: episode: 1655, duration: 0.014s, episode steps: 13, steps per second: 925, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.102 [-2.291, 1.368], mean_best_reward: --\n",
      " 34367/100000: episode: 1656, duration: 0.015s, episode steps: 15, steps per second: 975, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.067 [-1.032, 1.744], mean_best_reward: --\n",
      " 34412/100000: episode: 1657, duration: 0.044s, episode steps: 45, steps per second: 1034, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.136 [-1.078, 0.386], mean_best_reward: --\n",
      " 34435/100000: episode: 1658, duration: 0.024s, episode steps: 23, steps per second: 952, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.048 [-0.821, 1.307], mean_best_reward: --\n",
      " 34460/100000: episode: 1659, duration: 0.032s, episode steps: 25, steps per second: 771, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.057 [-0.404, 1.053], mean_best_reward: --\n",
      " 34472/100000: episode: 1660, duration: 0.015s, episode steps: 12, steps per second: 789, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.095 [-2.028, 1.221], mean_best_reward: --\n",
      " 34499/100000: episode: 1661, duration: 0.024s, episode steps: 27, steps per second: 1122, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.084 [-0.403, 1.135], mean_best_reward: --\n",
      " 34520/100000: episode: 1662, duration: 0.019s, episode steps: 21, steps per second: 1099, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.085 [-0.593, 1.034], mean_best_reward: --\n",
      " 34540/100000: episode: 1663, duration: 0.018s, episode steps: 20, steps per second: 1099, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.082 [-0.661, 1.470], mean_best_reward: --\n",
      " 34632/100000: episode: 1664, duration: 0.076s, episode steps: 92, steps per second: 1204, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-1.152, 1.526], mean_best_reward: --\n",
      " 34645/100000: episode: 1665, duration: 0.013s, episode steps: 13, steps per second: 968, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.101 [-2.290, 1.402], mean_best_reward: --\n",
      " 34671/100000: episode: 1666, duration: 0.029s, episode steps: 26, steps per second: 899, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.089 [-1.084, 0.605], mean_best_reward: --\n",
      " 34682/100000: episode: 1667, duration: 0.015s, episode steps: 11, steps per second: 719, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.132 [-0.946, 1.663], mean_best_reward: --\n",
      " 34761/100000: episode: 1668, duration: 0.068s, episode steps: 79, steps per second: 1157, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.050 [-0.774, 1.215], mean_best_reward: --\n",
      " 34780/100000: episode: 1669, duration: 0.019s, episode steps: 19, steps per second: 1006, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.081 [-0.626, 1.245], mean_best_reward: --\n",
      " 34809/100000: episode: 1670, duration: 0.025s, episode steps: 29, steps per second: 1145, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.119 [-0.601, 1.103], mean_best_reward: --\n",
      " 34838/100000: episode: 1671, duration: 0.026s, episode steps: 29, steps per second: 1134, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.097 [-0.251, 1.066], mean_best_reward: --\n",
      " 34853/100000: episode: 1672, duration: 0.014s, episode steps: 15, steps per second: 1059, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.072 [-1.748, 1.199], mean_best_reward: --\n",
      " 34939/100000: episode: 1673, duration: 0.077s, episode steps: 86, steps per second: 1116, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.193 [-1.800, 0.752], mean_best_reward: --\n",
      " 34977/100000: episode: 1674, duration: 0.033s, episode steps: 38, steps per second: 1163, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.054 [-0.822, 1.653], mean_best_reward: --\n",
      " 34997/100000: episode: 1675, duration: 0.017s, episode steps: 20, steps per second: 1149, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.085 [-1.596, 0.771], mean_best_reward: --\n",
      " 35027/100000: episode: 1676, duration: 0.026s, episode steps: 30, steps per second: 1147, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.144 [-0.584, 0.940], mean_best_reward: --\n",
      " 35069/100000: episode: 1677, duration: 0.043s, episode steps: 42, steps per second: 985, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.018 [-1.345, 0.651], mean_best_reward: --\n",
      " 35095/100000: episode: 1678, duration: 0.023s, episode steps: 26, steps per second: 1123, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.375, 1.042], mean_best_reward: --\n",
      " 35122/100000: episode: 1679, duration: 0.025s, episode steps: 27, steps per second: 1087, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.072 [-0.583, 1.112], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35184/100000: episode: 1680, duration: 0.052s, episode steps: 62, steps per second: 1186, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.105 [-0.835, 0.690], mean_best_reward: --\n",
      " 35277/100000: episode: 1681, duration: 0.078s, episode steps: 93, steps per second: 1196, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.033 [-0.619, 1.278], mean_best_reward: --\n",
      " 35311/100000: episode: 1682, duration: 0.029s, episode steps: 34, steps per second: 1182, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.140 [-1.113, 0.204], mean_best_reward: --\n",
      " 35327/100000: episode: 1683, duration: 0.015s, episode steps: 16, steps per second: 1085, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.071 [-0.812, 1.324], mean_best_reward: --\n",
      " 35374/100000: episode: 1684, duration: 0.038s, episode steps: 47, steps per second: 1248, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.101 [-0.792, 0.480], mean_best_reward: --\n",
      " 35389/100000: episode: 1685, duration: 0.014s, episode steps: 15, steps per second: 1067, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.088 [-1.014, 1.665], mean_best_reward: --\n",
      " 35441/100000: episode: 1686, duration: 0.051s, episode steps: 52, steps per second: 1016, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.034 [-0.638, 1.392], mean_best_reward: --\n",
      " 35459/100000: episode: 1687, duration: 0.016s, episode steps: 18, steps per second: 1101, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.085 [-2.170, 1.198], mean_best_reward: --\n",
      " 35474/100000: episode: 1688, duration: 0.015s, episode steps: 15, steps per second: 999, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.098 [-1.129, 1.791], mean_best_reward: --\n",
      " 35485/100000: episode: 1689, duration: 0.011s, episode steps: 11, steps per second: 1000, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.121 [-1.205, 1.910], mean_best_reward: --\n",
      " 35501/100000: episode: 1690, duration: 0.015s, episode steps: 16, steps per second: 1086, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.087 [-0.988, 1.764], mean_best_reward: --\n",
      " 35526/100000: episode: 1691, duration: 0.022s, episode steps: 25, steps per second: 1130, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.055 [-1.802, 1.010], mean_best_reward: --\n",
      " 35554/100000: episode: 1692, duration: 0.023s, episode steps: 28, steps per second: 1209, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.147 [-0.974, 0.558], mean_best_reward: --\n",
      " 35576/100000: episode: 1693, duration: 0.019s, episode steps: 22, steps per second: 1156, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.623, 0.986], mean_best_reward: --\n",
      " 35587/100000: episode: 1694, duration: 0.011s, episode steps: 11, steps per second: 979, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.114 [-0.750, 1.261], mean_best_reward: --\n",
      " 35624/100000: episode: 1695, duration: 0.030s, episode steps: 37, steps per second: 1233, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.051 [-0.559, 0.831], mean_best_reward: --\n",
      " 35635/100000: episode: 1696, duration: 0.011s, episode steps: 11, steps per second: 994, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.109 [-2.493, 1.584], mean_best_reward: --\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "\n",
    "memory = EpisodeParameterMemory(limit=1000, window_length=1)\n",
    "\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()\n",
    "\n",
    "cem.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
    "\n",
    "# After training is done, we save the best weights.\n",
    "cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 11.000, steps: 11\n",
      "Episode 2: reward: 10.000, steps: 10\n",
      "Episode 3: reward: 10.000, steps: 10\n",
      "Episode 4: reward: 11.000, steps: 11\n",
      "Episode 5: reward: 9.000, steps: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa55c7267b8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "cem.load_weights('cem_{}_params.h5f'.format(ENV_NAME))\n",
    "cem.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
